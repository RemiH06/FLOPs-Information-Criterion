{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2c0505",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{0. Libs}}$\n",
    "\n",
    "MNIST Classification: Comparaci√≥n de 3 enfoques diferentes\n",
    "1. MLP (Multi-Layer Perceptron) - Baseline denso\n",
    "2. CNN (Convolutional Neural Network) - LeNet-like con BatchNorm\n",
    "3. HOG + SVM (Cl√°sico) - Histograms of Oriented Gradients + Linear SVM\n",
    "\n",
    "Objetivo: Entrenar cada modelo y luego evaluar con AIC, BIC y FIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259b7b6",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{1. Configuraci√≥n de par√°metros generales}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MNIST: COMPARACI√ìN DE 3 ENFOQUES DE CLASIFICACI√ìN\n",
      "================================================================================\n",
      "\n",
      "Configuraci√≥n:\n",
      "  Device: cpu\n",
      "  Batch size: 128\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "res=\"Resultados:\"\n",
    "\n",
    "print(f\"\\nConfiguraci√≥n:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf66358",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{2. Carga de datos}}$\n",
    "\n",
    "Desde OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c945fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CARGANDO DATOS MNIST\n",
      "====================================================================================================\n",
      "Descargando MNIST desde OpenML...\n",
      "\n",
      "Datasets cargados:\n",
      "  Train: 60000 im√°genes\n",
      "  Test:  10000 im√°genes\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "X = mnist.data.to_numpy().astype(np.float32)\n",
    "y = mnist.target.to_numpy().astype(np.int64)\n",
    "\n",
    "# Normalizar\n",
    "X = X / 255.0\n",
    "\n",
    "spli=60000\n",
    "# Split train/test\n",
    "X_train = X[:spli]\n",
    "y_train = y[:spli]\n",
    "X_test = X[spli:]\n",
    "y_test = y[spli:]\n",
    "\n",
    "# Normalizaci√≥n est√°ndar\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(f\"\\nDatasets cargados:\")\n",
    "print(f\"  Train: {X_train.shape[0]} im√°genes\")\n",
    "print(f\"  Test:  {X_test.shape[0]} im√°genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a tensores PyTorch\n",
    "X_train_torch = torch.FloatTensor(X_train).view(-1, 1, 28, 28)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test).view(-1, 1, 28, 28)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed824f",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{3. Funciones auxiliares}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Cuenta par√°metros entrenables\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_pytorch_model(model, train_loader, epochs, lr):\n",
    "    \"\"\"Entrena un modelo de PyTorch\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    \"\"\"Eval√∫a un modelo de PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e83ea",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{4. MNIST}}$\n",
    "\n",
    "Resuelto 10 veces, cada una por un m√©todo distinto para comparar cu√°l es el mejor de acuerdo a un FIC que parametrizamos para economizar en recursos de c√≥mputo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cd4f1",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.1. Regresi√≥n Log√≠stica}}$\n",
    "\n",
    "Caracter√≠sticas: Lineal, sin capas ocultas, baseline cl√°sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 1: REGRESI√ìN LOG√çSTICA\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Lineal, sin capas ocultas, baseline cl√°sico\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 92.50%\n",
      "  Par√°metros: 7,850\n",
      "  Training Time: 14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hecto\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_start = time.time()\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=100, random_state=42, verbose=0)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "logreg_train_time = time.time() - logreg_start\n",
    "logreg_predictions = logreg_model.predict(X_test)\n",
    "logreg_accuracy = 100. * np.mean(logreg_predictions == y_test)\n",
    "\n",
    "logreg_params = logreg_model.coef_.size + logreg_model.intercept_.size\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {logreg_accuracy:.2f}%\")\n",
    "print(f\"  Par√°metros: {logreg_params:,}\")\n",
    "print(f\"  Training Time: {logreg_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05139c71",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.2. K-Nearest Neighbors}}$\n",
    "\n",
    "Caracter√≠sticas: No param√©trico, basado en distancia\n",
    "\n",
    "k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 2: K-NEAREST NEIGHBORS (k=5)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: No param√©trico, basado en distancia\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 94.42%\n",
      "  Par√°metros: 0 (no param√©trico)\n",
      "  Training Time: 0.1s\n",
      "  Nota: Entrenado con subset de 10000 muestras\n"
     ]
    }
   ],
   "source": [
    "knn_start = time.time()\n",
    "\n",
    "# Usar subset para acelerar\n",
    "subset_size = 10000\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn_model.fit(X_train[:subset_size], y_train[:subset_size])\n",
    "\n",
    "knn_train_time = time.time() - knn_start\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = 100. * np.mean(knn_predictions == y_test)\n",
    "\n",
    "knn_params = 0  # KNN no tiene par√°metros tradicionales\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {knn_accuracy:.2f}%\")\n",
    "print(f\"  Par√°metros: {knn_params} (no param√©trico)\")\n",
    "print(f\"  Training Time: {knn_train_time:.1f}s\")\n",
    "print(f\"  Nota: Entrenado con subset de {subset_size} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a0ed1",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.3. Random Forest}}$\n",
    "\n",
    "Caracter√≠sticas: Ensemble, robusto, interpretable\n",
    "\n",
    "100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a255ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 3: RANDOM FOREST (100 √°rboles)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Ensemble, robusto, interpretable\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 96.80%\n",
      "  Par√°metros: 958,830 (nodos totales)\n",
      "  Training Time: 28.1s\n"
     ]
    }
   ],
   "source": [
    "rf_start = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_train_time = time.time() - rf_start\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_accuracy = 100. * np.mean(rf_predictions == y_test)\n",
    "\n",
    "# Estimaci√≥n de par√°metros: nodos √ó √°rboles\n",
    "rf_params = sum(tree.tree_.node_count for tree in rf_model.estimators_)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {rf_accuracy:.2f}%\")\n",
    "print(f\"  Par√°metros: {rf_params:,} (nodos totales)\")\n",
    "print(f\"  Training Time: {rf_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ef933",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.4. PCA + SVM}}$\n",
    "\n",
    "Caracter√≠sticas: Reducci√≥n dimensionalidad + clasificador lineal\n",
    "\n",
    "PCA (150 comps) + LINEAR SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 4: PCA (150 componentes) + LINEAR SVM\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Reducci√≥n dimensionalidad + clasificador lineal\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 91.55%\n",
      "  Par√°metros: 1,510\n",
      "  Training Time: 21.3s\n",
      "  Varianza explicada: 94.8%\n"
     ]
    }
   ],
   "source": [
    "pca_svm_start = time.time()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "svm_model = LinearSVC(C=1.0, max_iter=1000, random_state=42)\n",
    "svm_model.fit(X_train_pca_scaled, y_train)\n",
    "\n",
    "pca_svm_train_time = time.time() - pca_svm_start\n",
    "pca_svm_predictions = svm_model.predict(X_test_pca_scaled)\n",
    "pca_svm_accuracy = 100. * np.mean(pca_svm_predictions == y_test)\n",
    "\n",
    "pca_svm_params = svm_model.coef_.size + svm_model.intercept_.size\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {pca_svm_accuracy:.2f}%\")\n",
    "print(f\"  Par√°metros: {pca_svm_params:,}\")\n",
    "print(f\"  Training Time: {pca_svm_train_time:.1f}s\")\n",
    "print(f\"  Varianza explicada: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e2d2a",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.5. MLP Tiny (muy muy peque√±o :3)}}$\n",
    "\n",
    "Caracter√≠sticas: Red m√≠nima, ultra eficiente\n",
    "\n",
    "MLP TINY (784 -> 64 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 5: MLP TINY (784 -> 64 -> 10)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Red m√≠nima, ultra eficiente\n",
      "\n",
      "Par√°metros: 50,890\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.34%\n",
      "  Training Time: 64.7s\n"
     ]
    }
   ],
   "source": [
    "class MLPTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPTiny, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "mlp_tiny_model = MLPTiny().to(DEVICE)\n",
    "mlp_tiny_params = count_parameters(mlp_tiny_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {mlp_tiny_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_tiny_start = time.time()\n",
    "train_pytorch_model(mlp_tiny_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_tiny_train_time = time.time() - mlp_tiny_start\n",
    "\n",
    "mlp_tiny_accuracy = evaluate_pytorch_model(mlp_tiny_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_tiny_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_tiny_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d0de3",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.6. MLP Medium (balanceado)}}$\n",
    "\n",
    "Caracter√≠sticas: Balance entre tama√±o y capacidad\n",
    "\n",
    "MLP MEDIUM (784 -> 256 -> 128 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 6: MLP MEDIUM (784 -> 256 -> 128 -> 10)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Balance entre tama√±o y capacidad\n",
      "\n",
      "Par√°metros: 235,146\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 98.28%\n",
      "  Training Time: 73.1s\n"
     ]
    }
   ],
   "source": [
    "class MLPMedium(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPMedium, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp_medium_model = MLPMedium().to(DEVICE)\n",
    "mlp_medium_params = count_parameters(mlp_medium_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {mlp_medium_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_medium_start = time.time()\n",
    "train_pytorch_model(mlp_medium_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_medium_train_time = time.time() - mlp_medium_start\n",
    "\n",
    "mlp_medium_accuracy = evaluate_pytorch_model(mlp_medium_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_medium_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_medium_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9815c",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.7. MLP Large (sobredimensionado)}}$\n",
    "\n",
    "Caracter√≠sticas: Muy grande, posible overfitting\n",
    "\n",
    "MLP LARGE (784 -> 512 -> 512 -> 256 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cacac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 7: MLP LARGE (784 -> 512 -> 512 -> 256 -> 10)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Muy grande, posible overfitting\n",
      "\n",
      "Par√°metros: 798,474\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.93%\n",
      "  Training Time: 128.5s\n"
     ]
    }
   ],
   "source": [
    "class MLPLarge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPLarge, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "mlp_large_model = MLPLarge().to(DEVICE)\n",
    "mlp_large_params = count_parameters(mlp_large_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {mlp_large_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_large_start = time.time()\n",
    "train_pytorch_model(mlp_large_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_large_train_time = time.time() - mlp_large_start\n",
    "\n",
    "mlp_large_accuracy = evaluate_pytorch_model(mlp_large_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_large_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_large_train_time:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd91e44",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.8. CNN Tiny (m√≠nima convolucional)}}$\n",
    "\n",
    "Caracter√≠sticas: Convolucional m√≠nima, muy eficiente\n",
    "\n",
    "CNN TINY (SOLO UNA capa conv :0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 8: CNN TINY (1 capa conv)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: Convolucional m√≠nima, muy eficiente\n",
      "\n",
      "Par√°metros: 31,530\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.98%\n",
      "  Training Time: 155.8s\n"
     ]
    }
   ],
   "source": [
    "class CNNTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTiny, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_tiny_model = CNNTiny().to(DEVICE)\n",
    "cnn_tiny_params = count_parameters(cnn_tiny_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {cnn_tiny_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_tiny_start = time.time()\n",
    "train_pytorch_model(cnn_tiny_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_tiny_train_time = time.time() - cnn_tiny_start\n",
    "\n",
    "cnn_tiny_accuracy = evaluate_pytorch_model(cnn_tiny_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_tiny_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_tiny_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6931bc3",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.9. CNN Medium (LeNet-like)}}$\n",
    "\n",
    "Caracter√≠sticas: LeNet-like moderno, buen balance\n",
    "\n",
    "CNN MEDIUM (2 capas conv con BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 9: CNN MEDIUM (2 capas conv con BatchNorm)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: LeNet-like moderno, buen balance\n",
      "\n",
      "Par√°metros: 421,834\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 99.19%\n",
      "  Training Time: 693.7s\n"
     ]
    }
   ],
   "source": [
    "class CNNMedium(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMedium, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_medium_model = CNNMedium().to(DEVICE)\n",
    "cnn_medium_params = count_parameters(cnn_medium_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {cnn_medium_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_medium_start = time.time()\n",
    "train_pytorch_model(cnn_medium_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_medium_train_time = time.time() - cnn_medium_start\n",
    "\n",
    "cnn_medium_accuracy = evaluate_pytorch_model(cnn_medium_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_medium_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_medium_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322de3a0",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.10. CNN Deep (muy profunda)}}$\n",
    "\n",
    "Caracter√≠sticas: M√°xima capacidad, posible overkill para MNIST\n",
    "\n",
    "CNN DEEP (4 capas conv, muy profunda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 10: CNN DEEP (4 capas conv, muy profunda)\n",
      "====================================================================================================\n",
      "Caracter√≠sticas: M√°xima capacidad, posible overkill para MNIST\n",
      "\n",
      "Par√°metros: 276,554\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 99.18%\n",
      "  Training Time: 1066.6s\n"
     ]
    }
   ],
   "source": [
    "class CNNDeep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDeep, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 1 * 1, 256)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_deep_model = CNNDeep().to(DEVICE)\n",
    "cnn_deep_params = count_parameters(cnn_deep_model)\n",
    "\n",
    "print(f\"\\nPar√°metros: {cnn_deep_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_deep_start = time.time()\n",
    "train_pytorch_model(cnn_deep_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_deep_train_time = time.time() - cnn_deep_start\n",
    "\n",
    "cnn_deep_accuracy = evaluate_pytorch_model(cnn_deep_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_deep_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_deep_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317860b9",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{5. AIC, BIC y FIC pa' todos los modelos}}$\n",
    "\n",
    "Favor de leer los comentarios en el c√≥digo (sobre todo en los hiperpar√°metros de la f√≥rmula) para entender mejor qu√© pasa y por qu√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Par√°metros FIC configurados:\n",
      "  Variant:        custom\n",
      "  Alpha (FLOPs):  1000\n",
      "  Beta (Params):  0.0\n",
      "  Lambda (Balance): 0.1\n",
      "\n",
      "üí° Gu√≠a de ajuste:\n",
      "  ‚Ä¢ ALPHA alto ‚Üí Penaliza fuertemente modelos con muchos FLOPs\n",
      "  ‚Ä¢ BETA alto ‚Üí Penaliza fuertemente modelos con muchos par√°metros\n",
      "  ‚Ä¢ LAMBDA cercano a 0 ‚Üí Prioriza eficiencia en FLOPs\n",
      "  ‚Ä¢ LAMBDA cercano a 1 ‚Üí Prioriza parsimonia en par√°metros\n",
      "\n",
      "Ejemplos de configuraciones:\n",
      "  Deployment (edge):   alpha=10.0, beta=1.0, lambda=0.2\n",
      "  Research:            alpha=1.0, beta=2.0, lambda=0.7\n",
      "  Balanced:            alpha=5.0, beta=0.5, lambda=0.3\n"
     ]
    }
   ],
   "source": [
    "from flop_counter import FlopInformationCriterion, count_model_flops\n",
    "\n",
    "# Par√°metros ajustables del FIC\n",
    "\n",
    "wonder_flop_priority = 3 # Va de la mano con alpha, en este caso ser√≠a la potencia de 10 que prioriza los pocos flops.\n",
    "# No s√© si deber√≠a colocarla como una variable propia del modelo pero funciona para al menos este ejemplo que lo amerita.\n",
    "# En este caso le di prioridad absoluta a los FLOPs, sin tener en cuenta los par√°metros\n",
    "# Incluso si se tiene una beta en cero, alpha deber√≠a aumentarse o reducirse de acuerdo a qu√© tanto nos importa la log-verosimilitud\n",
    "# Teniendo eso en cuenta, unos 10000 de alpha indican que la prioridad que le doy a los FLOPs es mucho mayor que la que le doy a la verosimilitud.\n",
    "# En este caso es extremo porque quiero probar mi punto, pero este notebook ejemplifica que el FIC balancear√° diferente de acuerdo a las prioridades que le pongamos.\n",
    "# Si bajamos wonder_flop_priority a, por ejemplo, 2 (10^2 = 100), entonces decimos que quiz√° nos importan los FLOPs pero la verosimilitud sigue importando bastantito.\n",
    "\n",
    "FIC_VARIANT = 'custom'  # Opciones: 'deployment', 'research', 'balanced', 'custom'\n",
    "FIC_ALPHA = 1000         # Peso del t√©rmino de FLOPs (mayor = penaliza m√°s FLOPs)\n",
    "FIC_BETA = 0.0         # Peso del t√©rmino de par√°metros (mayor = penaliza m√°s params)\n",
    "FIC_LAMBDA = 0.1        # Balance entre t√©rminos (0=solo FLOPs, 1=solo params) Este par√°metro depende mucho de la cantidad de FLOPs que se esperan\n",
    "\n",
    "print(f\"\\nPar√°metros FIC configurados:\")\n",
    "print(f\"  Variant:        {FIC_VARIANT}\")\n",
    "print(f\"  Alpha (FLOPs):  {FIC_ALPHA}\")\n",
    "print(f\"  Beta (Params):  {FIC_BETA}\")\n",
    "print(f\"  Lambda (Balance): {FIC_LAMBDA}\")\n",
    "\n",
    "print(\"\\nüí° Gu√≠a de ajuste:\")\n",
    "print(\"  ‚Ä¢ ALPHA alto ‚Üí Penaliza fuertemente modelos con muchos FLOPs\")\n",
    "print(\"  ‚Ä¢ BETA alto ‚Üí Penaliza fuertemente modelos con muchos par√°metros\")\n",
    "print(\"  ‚Ä¢ LAMBDA cercano a 0 ‚Üí Prioriza eficiencia en FLOPs\")\n",
    "print(\"  ‚Ä¢ LAMBDA cercano a 1 ‚Üí Prioriza parsimonia en par√°metros\")\n",
    "print(\"\\nEjemplos de configuraciones:\")\n",
    "print(\"  Deployment (edge):   alpha=10.0, beta=1.0, lambda=0.2\")\n",
    "print(\"  Research:            alpha=1.0, beta=2.0, lambda=0.7\")\n",
    "print(\"  Balanced:            alpha=5.0, beta=0.5, lambda=0.3\")\n",
    "\n",
    "# Configurar FIC con par√°metros personalizados\n",
    "fic_calculator = FlopInformationCriterion(\n",
    "    variant=FIC_VARIANT,\n",
    "    alpha=FIC_ALPHA,\n",
    "    beta=FIC_BETA,\n",
    "    lambda_balance=FIC_LAMBDA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7f736",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.1. Funciones de c√°lculo}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "74fb598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aic(log_likelihood, k):\n",
    "    \"\"\"AIC = 2k - 2ln(L) = log_likelihood + 2k\"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "def calculate_bic(log_likelihood, k, n):\n",
    "    \"\"\"BIC = k*ln(n) - 2ln(L) = log_likelihood + k*ln(n)\"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "def calculate_log_likelihood_pytorch(model, data_loader, device):\n",
    "    \"\"\"Calcula log-likelihood negativa para modelos PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def calculate_log_likelihood_sklearn(predictions, y_true, n_classes=10):\n",
    "    \"\"\"Aproximaci√≥n de log-likelihood para modelos sklearn\"\"\"\n",
    "    correct = (predictions == y_true).astype(float)\n",
    "    # Probabilidades: alta si correcto, baja si incorrecto\n",
    "    probs = np.where(correct == 1, 0.99, 0.01/(n_classes-1))\n",
    "    return -2 * np.sum(np.log(probs + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847c24b",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.2. SKLearn eval}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "99aa5d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[sklearn] Logistic Regression...\n",
      "  Log-likelihood: 10,390\n",
      "  Params: 7,850\n",
      "  FLOPs: 15,680\n",
      "  AIC: 26,090\n",
      "  BIC: 82,691\n",
      "  FIC: 11,370\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[sklearn] KNN (k=5)...\n",
      "  Log-likelihood: 7,781\n",
      "  Params: 100\n",
      "  FLOPs: 39,200,000\n",
      "  AIC: 7,981\n",
      "  BIC: 8,702\n",
      "  FIC: 44,810\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[sklearn] Random Forest...\n",
      "  Log-likelihood: 4,548\n",
      "  Params: 958,830\n",
      "  FLOPs: 9,588,300\n",
      "  AIC: 1,922,208\n",
      "  BIC: 8,835,699\n",
      "  FIC: 14,785\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[sklearn] PCA+SVM...\n",
      "  Log-likelihood: 11,680\n",
      "  Params: 1,510\n",
      "  FLOPs: 120,600\n",
      "  AIC: 14,700\n",
      "  BIC: 25,588\n",
      "  FIC: 12,959\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "sklearn_models = [\n",
    "    (\"Logistic Regression\", logreg_predictions, logreg_params, logreg_accuracy, logreg_train_time),\n",
    "    (\"KNN (k=5)\", knn_predictions, knn_params if knn_params > 0 else 100, knn_accuracy, knn_train_time),\n",
    "    (\"Random Forest\", rf_predictions, rf_params, rf_accuracy, rf_train_time),\n",
    "    (\"PCA+SVM\", pca_svm_predictions, pca_svm_params, pca_svm_accuracy, pca_svm_train_time),\n",
    "]\n",
    "\n",
    "for name, predictions, n_params, accuracy, train_time in sklearn_models:\n",
    "    print(f\"\\n[sklearn] {name}...\")\n",
    "    \n",
    "    # Calcular log-likelihood\n",
    "    log_lik = calculate_log_likelihood_sklearn(predictions, y_test)\n",
    "    \n",
    "    # Calcular AIC y BIC\n",
    "    aic = calculate_aic(log_lik, n_params)\n",
    "    bic = calculate_bic(log_lik, n_params, len(y_test))\n",
    "    \n",
    "    # Estimar FLOPs seg√∫n el tipo de modelo\n",
    "    if \"Logistic\" in name:\n",
    "        flops = 784 * 10 * 2  # matmul input->output\n",
    "    elif \"KNN\" in name:\n",
    "        flops = 784 * len(y_test) * 5  # distance calculations\n",
    "    elif \"Forest\" in name:\n",
    "        flops = rf_params * 10  # traversals aproximados\n",
    "    elif \"SVM\" in name:\n",
    "        flops = 150 * 10 * 2 + 784 * 150  # PCA transform + SVM\n",
    "    else:\n",
    "        flops = 0\n",
    "    \n",
    "    # Calcular FIC con par√°metros configurados\n",
    "    fic_result = fic_calculator.calculate_fic(\n",
    "        log_likelihood=log_lik,\n",
    "        flops=flops,\n",
    "        n_params=n_params,\n",
    "        n_samples=len(y_test)\n",
    "    )\n",
    "    \n",
    "    # Mostrar desglose del FIC\n",
    "    print(f\"  Log-likelihood: {log_lik:,.0f}\")\n",
    "    print(f\"  Params: {n_params:,}\")\n",
    "    print(f\"  FLOPs: {flops:,}\")\n",
    "    print(f\"  AIC: {aic:,.0f}\")\n",
    "    print(f\"  BIC: {bic:,.0f}\")\n",
    "    print(f\"  FIC: {fic_result['fic']:,.0f}\")\n",
    "    print(f\"    ‚îî‚îÄ FLOPs term: {fic_result.get('flops_term', 0):,.0f}\")\n",
    "    print(f\"    ‚îî‚îÄ Params term: {fic_result.get('params_term', 0):,.0f}\")\n",
    "    \n",
    "    all_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'params': n_params,\n",
    "        'flops': flops,\n",
    "        'train_time': train_time,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'fic': fic_result['fic'],\n",
    "        'log_likelihood': log_lik,\n",
    "        'flops_term': fic_result.get('flops_term', 0),\n",
    "        'params_term': fic_result.get('params_term', 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158313d",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.3. PyTorch eval}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b06801b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PyTorch] MLP Tiny...\n",
      "  Log-likelihood: 925\n",
      "  Params: 50,890\n",
      "  FLOPs: 1,237,568\n",
      "  AIC: 102,705\n",
      "  BIC: 469,640\n",
      "  FIC: 3,442\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[PyTorch] MLP Medium...\n",
      "  Log-likelihood: 609\n",
      "  Params: 235,146\n",
      "  FLOPs: 1,393,536\n",
      "  AIC: 470,901\n",
      "  BIC: 2,166,384\n",
      "  FIC: 3,278\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[PyTorch] MLP Large...\n",
      "  Log-likelihood: 702\n",
      "  Params: 798,474\n",
      "  FLOPs: 2,410,240\n",
      "  AIC: 1,597,650\n",
      "  BIC: 7,354,920\n",
      "  FIC: 4,341\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Tiny...\n",
      "  Log-likelihood: 644\n",
      "  Params: 31,530\n",
      "  FLOPs: 19,894,784\n",
      "  AIC: 63,704\n",
      "  BIC: 291,046\n",
      "  FIC: 20,229\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Medium...\n",
      "  Log-likelihood: 306\n",
      "  Params: 421,834\n",
      "  FLOPs: 27,378,816\n",
      "  AIC: 843,974\n",
      "  BIC: 3,885,541\n",
      "  FIC: 26,660\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Deep...\n",
      "  Log-likelihood: 319\n",
      "  Params: 276,554\n",
      "  FLOPs: 17,720,576\n",
      "  AIC: 553,427\n",
      "  BIC: 2,547,476\n",
      "  FIC: 17,937\n",
      "    ‚îî‚îÄ FLOPs term: 0\n",
      "    ‚îî‚îÄ Params term: 0\n"
     ]
    }
   ],
   "source": [
    "pytorch_models = [\n",
    "    (\"MLP Tiny\", mlp_tiny_model, mlp_tiny_params, mlp_tiny_accuracy, mlp_tiny_train_time),\n",
    "    (\"MLP Medium\", mlp_medium_model, mlp_medium_params, mlp_medium_accuracy, mlp_medium_train_time),\n",
    "    (\"MLP Large\", mlp_large_model, mlp_large_params, mlp_large_accuracy, mlp_large_train_time),\n",
    "    (\"CNN Tiny\", cnn_tiny_model, cnn_tiny_params, cnn_tiny_accuracy, cnn_tiny_train_time),\n",
    "    (\"CNN Medium\", cnn_medium_model, cnn_medium_params, cnn_medium_accuracy, cnn_medium_train_time),\n",
    "    (\"CNN Deep\", cnn_deep_model, cnn_deep_params, cnn_deep_accuracy, cnn_deep_train_time),\n",
    "]\n",
    "\n",
    "for name, model, n_params, accuracy, train_time in pytorch_models:\n",
    "    print(f\"\\n[PyTorch] {name}...\")\n",
    "    \n",
    "    # Calcular log-likelihood\n",
    "    log_lik = calculate_log_likelihood_pytorch(model, test_loader, DEVICE)\n",
    "    \n",
    "    # Calcular AIC y BIC\n",
    "    aic = calculate_aic(log_lik, n_params)\n",
    "    bic = calculate_bic(log_lik, n_params, len(y_test))\n",
    "    \n",
    "    # Contar FLOPs reales\n",
    "    sample_input = X_test_torch[:1].to(DEVICE)\n",
    "    try:\n",
    "        flops_result = count_model_flops(\n",
    "            model=model,\n",
    "            input_data=sample_input,\n",
    "            framework='torch',\n",
    "            verbose=False\n",
    "        )\n",
    "        flops = flops_result['total_flops']\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Warning: No se pudieron contar FLOPs: {e}\")\n",
    "        flops = 0\n",
    "    \n",
    "    # Calcular FIC con par√°metros configurados\n",
    "    fic_result = fic_calculator.calculate_fic(\n",
    "        log_likelihood=log_lik,\n",
    "        flops=flops,\n",
    "        n_params=n_params,\n",
    "        n_samples=len(y_test)\n",
    "    )\n",
    "    \n",
    "    # Mostrar desglose del FIC\n",
    "    print(f\"  Log-likelihood: {log_lik:,.0f}\")\n",
    "    print(f\"  Params: {n_params:,}\")\n",
    "    print(f\"  FLOPs: {flops:,}\")\n",
    "    print(f\"  AIC: {aic:,.0f}\")\n",
    "    print(f\"  BIC: {bic:,.0f}\")\n",
    "    print(f\"  FIC: {fic_result['fic']:,.0f}\")\n",
    "    print(f\"    ‚îî‚îÄ FLOPs term: {fic_result.get('flops_term', 0):,.0f}\")\n",
    "    print(f\"    ‚îî‚îÄ Params term: {fic_result.get('params_term', 0):,.0f}\")\n",
    "    \n",
    "    all_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'params': n_params,\n",
    "        'flops': flops,\n",
    "        'train_time': train_time,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'fic': fic_result['fic'],\n",
    "        'log_likelihood': log_lik,\n",
    "        'flops_term': fic_result.get('flops_term', 0),\n",
    "        'params_term': fic_result.get('params_term', 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf08d3",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{6. Resultados}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "764f646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLA COMPARATIVA COMPLETA: 10 ENFOQUES\n",
      "====================================================================================================\n",
      "\n",
      "Modelo                    Acc%     Params       FLOPs           Time(s)    AIC          BIC          FIC         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLP Medium                98.28    235,146      1,393,536       73.1       470901       2166384      3278        \n",
      "MLP Tiny                  97.34    50,890       1,237,568       64.7       102705       469640       3442        \n",
      "MLP Large                 97.93    798,474      2,410,240       128.5      1597650      7354920      4341        \n",
      "Logistic Regression       92.50    7,850        15,680          14.2       26090        82691        11370       \n",
      "PCA+SVM                   91.55    1,510        120,600         21.3       14700        25588        12959       \n",
      "Random Forest             96.80    958,830      9,588,300       28.1       1922208      8835699      14785       \n",
      "CNN Deep                  99.18    276,554      17,720,576      1066.6     553427       2547476      17937       \n",
      "CNN Tiny                  97.98    31,530       19,894,784      155.8      63704        291046       20229       \n",
      "CNN Medium                99.19    421,834      27,378,816      693.7      843974       3885541      26660       \n",
      "KNN (k=5)                 94.42    100          39,200,000      0.1        7981         8702         44810       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLA COMPARATIVA COMPLETA: 10 ENFOQUES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n{'Modelo':<25} {'Acc%':<8} {'Params':<12} {'FLOPs':<15} {'Time(s)':<10} {'AIC':<12} {'BIC':<12} {'FIC':<12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for name in sorted(all_results.keys(), key=lambda x: all_results[x]['fic']):\n",
    "    r = all_results[name]\n",
    "    print(f\"{name:<25} {r['accuracy']:<8.2f} {r['params']:<12,} {r['flops']:<15,} \"\n",
    "          f\"{r['train_time']:<10.1f} {r['aic']:<12.0f} {r['bic']:<12.0f} {r['fic']:<12.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8358ae",
   "metadata": {},
   "source": [
    "\n",
    "## $\\color{#dda}{\\text{7. Conclusiones}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba24f7",
   "metadata": {},
   "source": [
    "La variable que dice wonder es la potencia de 10 que aumentar√° a alpha lo suficiente para dar prioridad a los FLOPs pero sin opacar la verosimilitud. Con estos par√°metros se priorizan los FLOPs pero sin dejar de lado la eficiencia del modelo y su log-likelihood. Para este caso de demostraci√≥n, digamos que dejamos la beta en cero y la lambda en 0.1 porque priorizamos el balance entre FLOPs y verosimilitud.\n",
    "\n",
    "```python\n",
    "wonder_flop_priority = 3\n",
    "\n",
    "FIC_VARIANT = 'custom' \n",
    "FIC_ALPHA = 1000\n",
    "FIC_BETA = 0.0\n",
    "FIC_LAMBDA = 0.1 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
