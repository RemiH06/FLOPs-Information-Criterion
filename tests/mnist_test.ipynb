{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d258d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MNIST Classification: Comparación de 3 enfoques diferentes\n",
    "1. MLP (Multi-Layer Perceptron) - Baseline denso\n",
    "2. CNN (Convolutional Neural Network) - LeNet-like con BatchNorm\n",
    "3. HOG + SVM (Clásico) - Histograms of Oriented Gradients + Linear SVM\n",
    "\n",
    "Objetivo: Entrenar cada modelo y luego evaluar con AIC, BIC y FIC\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0c392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MNIST: COMPARACIÓN DE 3 ENFOQUES DE CLASIFICACIÓN\n",
      "================================================================================\n",
      "\n",
      "Configuración:\n",
      "  Device: cpu\n",
      "  Batch size: 128\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MNIST: COMPARACIÓN DE 3 ENFOQUES DE CLASIFICACIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN\n",
    "# ============================================================================\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"\\nConfiguración:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c945fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CARGANDO DATOS MNIST\n",
      "================================================================================\n",
      "Descargando MNIST desde OpenML...\n",
      "\n",
      "Datasets cargados:\n",
      "  Train: 60000 imágenes\n",
      "  Test:  10000 imágenes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CARGAR DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CARGANDO DATOS MNIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cargar MNIST usando sklearn (evita problema de torchvision)\n",
    "print(\"Descargando MNIST desde OpenML...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "X = mnist.data.to_numpy().astype(np.float32)\n",
    "y = mnist.target.to_numpy().astype(np.int64)\n",
    "\n",
    "# Normalizar\n",
    "X = X / 255.0\n",
    "\n",
    "# Split train/test (primeros 60000 train, resto test)\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "# Normalización estándar\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(f\"\\nDatasets cargados:\")\n",
    "print(f\"  Train: {X_train.shape[0]} imágenes\")\n",
    "print(f\"  Test:  {X_test.shape[0]} imágenes\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_train_torch = torch.FloatTensor(X_train).view(-1, 1, 28, 28)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test).view(-1, 1, 28, 28)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bd6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 1: MLP (Multi-Layer Perceptron)\n",
      "================================================================================\n",
      "\n",
      "Arquitectura: 784 -> 512 -> 256 -> 10\n",
      "Características: Denso, Dropout 0.2, ReLU\n",
      "\n",
      "Parámetros entrenables: 535,818\n",
      "\n",
      "Entrenando MLP...\n",
      "  Epoch 1/10 - Loss: 0.2541, Acc: 92.35%\n",
      "  Epoch 2/10 - Loss: 0.1104, Acc: 96.62%\n",
      "  Epoch 3/10 - Loss: 0.0832, Acc: 97.40%\n",
      "  Epoch 4/10 - Loss: 0.0650, Acc: 97.97%\n",
      "  Epoch 5/10 - Loss: 0.0545, Acc: 98.27%\n",
      "  Epoch 6/10 - Loss: 0.0484, Acc: 98.46%\n",
      "  Epoch 7/10 - Loss: 0.0413, Acc: 98.66%\n",
      "  Epoch 8/10 - Loss: 0.0423, Acc: 98.61%\n",
      "  Epoch 9/10 - Loss: 0.0356, Acc: 98.86%\n",
      "  Epoch 10/10 - Loss: 0.0342, Acc: 98.89%\n",
      "\n",
      "Evaluando MLP...\n",
      "\n",
      "Resultados MLP:\n",
      "  Test Accuracy: 97.95%\n",
      "  Test Loss: 0.0785\n",
      "  Training Time: 68.7s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 1: MLP (Multi-Layer Perceptron)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 1: MLP (Multi-Layer Perceptron)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nArquitectura: 784 -> 512 -> 256 -> 10\")\n",
    "print(\"Características: Denso, Dropout 0.2, ReLU\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Cuenta parámetros entrenables\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Inicializar MLP\n",
    "mlp_model = MLP().to(DEVICE)\n",
    "mlp_params = count_parameters(mlp_model)\n",
    "\n",
    "print(f\"\\nParámetros entrenables: {mlp_params:,}\")\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\nEntrenando MLP...\")\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=LEARNING_RATE)\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mlp_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        mlp_optimizer.zero_grad()\n",
    "        output = mlp_model(data)\n",
    "        loss = mlp_criterion(output, target)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    train_acc = 100. * correct / total\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "\n",
    "mlp_train_time = time.time() - mlp_start_time\n",
    "\n",
    "# Evaluación\n",
    "print(\"\\nEvaluando MLP...\")\n",
    "mlp_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = mlp_model(data)\n",
    "        test_loss += mlp_criterion(output, target).item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "mlp_test_acc = 100. * correct / total\n",
    "mlp_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nResultados MLP:\")\n",
    "print(f\"  Test Accuracy: {mlp_test_acc:.2f}%\")\n",
    "print(f\"  Test Loss: {mlp_test_loss:.4f}\")\n",
    "print(f\"  Training Time: {mlp_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1f125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 2: CNN (LeNet-like con BatchNorm)\n",
      "================================================================================\n",
      "\n",
      "Arquitectura:\n",
      "  Conv1: 1->32 (3x3) -> BN -> ReLU -> MaxPool(2x2)\n",
      "  Conv2: 32->64 (3x3) -> BN -> ReLU -> MaxPool(2x2)\n",
      "  FC: 1600 -> 128 -> 10\n",
      "\n",
      "Parámetros entrenables: 421,834\n",
      "\n",
      "Entrenando CNN...\n",
      "  Epoch 1/10 - Loss: 0.1870, Acc: 94.31%\n",
      "  Epoch 2/10 - Loss: 0.0671, Acc: 97.98%\n",
      "  Epoch 3/10 - Loss: 0.0513, Acc: 98.41%\n",
      "  Epoch 4/10 - Loss: 0.0417, Acc: 98.75%\n",
      "  Epoch 5/10 - Loss: 0.0350, Acc: 98.94%\n",
      "  Epoch 6/10 - Loss: 0.0324, Acc: 98.96%\n",
      "  Epoch 7/10 - Loss: 0.0283, Acc: 99.08%\n",
      "  Epoch 8/10 - Loss: 0.0266, Acc: 99.16%\n",
      "  Epoch 9/10 - Loss: 0.0231, Acc: 99.24%\n",
      "  Epoch 10/10 - Loss: 0.0200, Acc: 99.33%\n",
      "\n",
      "Evaluando CNN...\n",
      "\n",
      "Resultados CNN:\n",
      "  Test Accuracy: 99.10%\n",
      "  Test Loss: 0.0300\n",
      "  Training Time: 559.3s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 2: CNN (Convolutional Neural Network)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 2: CNN (LeNet-like con BatchNorm)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nArquitectura:\")\n",
    "print(\"  Conv1: 1->32 (3x3) -> BN -> ReLU -> MaxPool(2x2)\")\n",
    "print(\"  Conv2: 32->64 (3x3) -> BN -> ReLU -> MaxPool(2x2)\")\n",
    "print(\"  FC: 1600 -> 128 -> 10\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Bloque 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bloque 2: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Clasificador denso\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bloque 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Bloque 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Clasificador\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Inicializar CNN\n",
    "cnn_model = CNN().to(DEVICE)\n",
    "cnn_params = count_parameters(cnn_model)\n",
    "\n",
    "print(f\"\\nParámetros entrenables: {cnn_params:,}\")\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\nEntrenando CNN...\")\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cnn_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        cnn_optimizer.zero_grad()\n",
    "        output = cnn_model(data)\n",
    "        loss = cnn_criterion(output, target)\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    train_acc = 100. * correct / total\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "\n",
    "cnn_train_time = time.time() - cnn_start_time\n",
    "\n",
    "# Evaluación\n",
    "print(\"\\nEvaluando CNN...\")\n",
    "cnn_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = cnn_model(data)\n",
    "        test_loss += cnn_criterion(output, target).item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "cnn_test_acc = 100. * correct / total\n",
    "cnn_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nResultados CNN:\")\n",
    "print(f\"  Test Accuracy: {cnn_test_acc:.2f}%\")\n",
    "print(f\"  Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"  Training Time: {cnn_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a255ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 3: PCA + Linear SVM (Clásico)\n",
      "================================================================================\n",
      "\n",
      "Características:\n",
      "  Reducción dim: PCA (784 -> 150 componentes)\n",
      "  Clasificador: Linear SVM (C=1.0)\n",
      "\n",
      "Preparando datos...\n",
      "\n",
      "Aplicando PCA (784 -> 150 dimensiones)...\n",
      "Varianza explicada: 94.84%\n",
      "\n",
      "Entrenando Linear SVM...\n",
      "[LibLinear]\n",
      "Evaluando SVM...\n",
      "\n",
      "Resultados SVM:\n",
      "  Test Accuracy: 91.55%\n",
      "  Training Time: 11.7s\n",
      "  PCA componentes: 150\n",
      "  Varianza capturada: 94.8%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 3: PCA + Linear SVM (Clásico)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 3: PCA + Linear SVM (Clásico)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCaracterísticas:\")\n",
    "print(\"  Reducción dim: PCA (784 -> 150 componentes)\")\n",
    "print(\"  Clasificador: Linear SVM (C=1.0)\")\n",
    "\n",
    "# Preparar datos para SVM (píxeles crudos)\n",
    "print(\"\\nPreparando datos...\")\n",
    "\n",
    "# Usar los datos ya normalizados\n",
    "X_train_flat = X_train  # Ya está en forma (60000, 784)\n",
    "X_test_flat = X_test    # Ya está en forma (10000, 784)\n",
    "\n",
    "svm_start_time = time.time()\n",
    "\n",
    "# Aplicar PCA para reducir dimensionalidad\n",
    "print(\"\\nAplicando PCA (784 -> 150 dimensiones)...\")\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Varianza explicada: {explained_var*100:.2f}%\")\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "# Entrenar SVM\n",
    "print(\"\\nEntrenando Linear SVM...\")\n",
    "svm_model = LinearSVC(C=1.0, max_iter=1000, random_state=42, verbose=1)\n",
    "svm_model.fit(X_train_pca_scaled, y_train)\n",
    "\n",
    "svm_train_time = time.time() - svm_start_time\n",
    "\n",
    "# Evaluar\n",
    "print(\"\\nEvaluando SVM...\")\n",
    "svm_predictions = svm_model.predict(X_test_pca_scaled)\n",
    "svm_test_acc = 100. * np.mean(svm_predictions == y_test)\n",
    "\n",
    "print(f\"\\nResultados SVM:\")\n",
    "print(f\"  Test Accuracy: {svm_test_acc:.2f}%\")\n",
    "print(f\"  Training Time: {svm_train_time:.1f}s\")\n",
    "print(f\"  PCA componentes: {X_train_pca.shape[1]}\")\n",
    "print(f\"  Varianza capturada: {explained_var*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "764f646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN COMPARATIVO\n",
      "================================================================================\n",
      "\n",
      "Modelo          Parámetros      Test Acc     Train Time     \n",
      "------------------------------------------------------------\n",
      "MLP             535,818         97.95       % 68.7           s\n",
      "CNN             421,834         99.10       % 559.3          s\n",
      "PCA+SVM         150             91.55       % 11.7           s\n",
      "\n",
      "================================================================================\n",
      "CALCULANDO CRITERIOS DE INFORMACIÓN: AIC, BIC, FIC\n",
      "================================================================================\n",
      "\n",
      "Configuración del FIC:\n",
      "  α = 5.0   (peso de penalización de FLOPs - aumentado)\n",
      "  β = 0.5   (peso de penalización de parámetros - reducido)\n",
      "  λ = 0.3   (30% log, 70% lineal - sensible a diferencias absolutas)\n",
      "\n",
      "  Objetivo: Priorizar eficiencia computacional cuando accuracy es similar\n",
      "\n",
      "Calculando métricas para cada modelo...\n",
      "\n",
      "[1/3] MLP...\n",
      "  Contando FLOPs del MLP...\n",
      "  FLOPs por muestra: 1,885,440\n",
      "\n",
      "[2/3] CNN...\n",
      "  Contando FLOPs de la CNN...\n",
      "  FLOPs por muestra: 27,378,816\n",
      "\n",
      "[3/3] PCA+SVM...\n",
      "  FLOPs estimados por muestra: 119,100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RESUMEN COMPARATIVO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN COMPARATIVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Modelo':<15} {'Parámetros':<15} {'Test Acc':<12} {'Train Time':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MLP':<15} {mlp_params:<15,} {mlp_test_acc:<12.2f}% {mlp_train_time:<15.1f}s\")\n",
    "print(f\"{'CNN':<15} {cnn_params:<15,} {cnn_test_acc:<12.2f}% {cnn_train_time:<15.1f}s\")\n",
    "print(f\"{'PCA+SVM':<15} {X_train_pca.shape[1]:<15} {svm_test_acc:<12.2f}% {svm_train_time:<15.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULAR AIC, BIC Y FIC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULANDO CRITERIOS DE INFORMACIÓN: AIC, BIC, FIC\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nConfiguración del FIC:\")\n",
    "print(\"  α = 5.0   (peso de penalización de FLOPs - aumentado)\")\n",
    "print(\"  β = 0.5   (peso de penalización de parámetros - reducido)\")\n",
    "print(\"  λ = 0.3   (30% log, 70% lineal - sensible a diferencias absolutas)\")\n",
    "print(\"\\n  Objetivo: Priorizar eficiencia computacional cuando accuracy es similar\")\n",
    "\n",
    "from flop_counter import FlopInformationCriterion, count_model_flops\n",
    "\n",
    "# Configurar FIC con más peso en FLOPs\n",
    "# α=5.0: Mayor penalización de FLOPs (vs α=2.0 estándar)\n",
    "# λ=0.3: 30% logarítmico, 70% lineal (más sensible a diferencias absolutas)\n",
    "# β=0.5: Menor peso en parámetros (para que FLOPs dominen más)\n",
    "OPTIMAL_LAMBDA = 0.3\n",
    "ALPHA = 0.1\n",
    "BETA = 10\n",
    "fic_calculator = FlopInformationCriterion(\n",
    "    variant='custom',\n",
    "    alpha=ALPHA,  # Penaliza más los FLOPs\n",
    "    beta=BETA,   # Menos peso a parámetros\n",
    "    flops_scale=f'parametric_log_linear_{OPTIMAL_LAMBDA}'\n",
    ")\n",
    "\n",
    "def calculate_aic(log_likelihood, k):\n",
    "    \"\"\"AIC = -2*log(L) + 2*k\"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "def calculate_bic(log_likelihood, k, n):\n",
    "    \"\"\"BIC = -2*log(L) + k*log(n)\"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "def calculate_log_likelihood_pytorch(model, data_loader, device):\n",
    "    \"\"\"Calcula -2*log(L) para modelo PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            n_samples += len(target)\n",
    "    \n",
    "    # -2*log(L) = 2 * CrossEntropyLoss (sum)\n",
    "    return total_loss\n",
    "\n",
    "def calculate_log_likelihood_svm(model, X, y):\n",
    "    \"\"\"Calcula -2*log(L) para SVM (aproximación usando hinge loss)\"\"\"\n",
    "    # Decision function da scores sin calibrar\n",
    "    decision_scores = model.decision_function(X)\n",
    "    \n",
    "    # Convertir a probabilidades usando softmax\n",
    "    exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # Cross-entropy\n",
    "    y_int = y.astype(int)\n",
    "    log_likelihood = -2 * np.sum(np.log(probs[np.arange(len(y)), y_int] + 1e-10))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "print(\"\\nCalculando métricas para cada modelo...\")\n",
    "\n",
    "# ---------------------------\n",
    "# MLP\n",
    "# ---------------------------\n",
    "print(\"\\n[1/3] MLP...\")\n",
    "\n",
    "# Log-likelihood\n",
    "mlp_log_lik = calculate_log_likelihood_pytorch(mlp_model, test_loader, DEVICE)\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "# AIC y BIC\n",
    "mlp_aic = calculate_aic(mlp_log_lik, mlp_params)\n",
    "mlp_bic = calculate_bic(mlp_log_lik, mlp_params, n_test)\n",
    "\n",
    "# FIC - necesitamos contar FLOPs\n",
    "print(\"  Contando FLOPs del MLP...\")\n",
    "sample_input = X_test_torch[:1].to(DEVICE)  # Una muestra para profiling\n",
    "try:\n",
    "    mlp_flops_result = count_model_flops(\n",
    "        model=mlp_model,\n",
    "        input_data=sample_input,\n",
    "        framework='torch',\n",
    "        verbose=False\n",
    "    )\n",
    "    mlp_flops = mlp_flops_result['total_flops']\n",
    "    print(f\"  FLOPs por muestra: {mlp_flops:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Advertencia: No se pudieron contar FLOPs: {e}\")\n",
    "    mlp_flops = 0\n",
    "\n",
    "# Calcular FIC\n",
    "mlp_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=mlp_log_lik,\n",
    "    flops=mlp_flops,\n",
    "    n_params=mlp_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "mlp_fic = mlp_fic_result['fic']\n",
    "\n",
    "# ---------------------------\n",
    "# CNN\n",
    "# ---------------------------\n",
    "print(\"\\n[2/3] CNN...\")\n",
    "\n",
    "# Log-likelihood\n",
    "cnn_log_lik = calculate_log_likelihood_pytorch(cnn_model, test_loader, DEVICE)\n",
    "\n",
    "# AIC y BIC\n",
    "cnn_aic = calculate_aic(cnn_log_lik, cnn_params)\n",
    "cnn_bic = calculate_bic(cnn_log_lik, cnn_params, n_test)\n",
    "\n",
    "# FIC - contar FLOPs\n",
    "print(\"  Contando FLOPs de la CNN...\")\n",
    "try:\n",
    "    cnn_flops_result = count_model_flops(\n",
    "        model=cnn_model,\n",
    "        input_data=sample_input,\n",
    "        framework='torch',\n",
    "        verbose=False\n",
    "    )\n",
    "    cnn_flops = cnn_flops_result['total_flops']\n",
    "    print(f\"  FLOPs por muestra: {cnn_flops:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Advertencia: No se pudieron contar FLOPs: {e}\")\n",
    "    cnn_flops = 0\n",
    "\n",
    "# Calcular FIC\n",
    "cnn_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=cnn_log_lik,\n",
    "    flops=cnn_flops,\n",
    "    n_params=cnn_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "cnn_fic = cnn_fic_result['fic']\n",
    "\n",
    "# ---------------------------\n",
    "# PCA+SVM\n",
    "# ---------------------------\n",
    "print(\"\\n[3/3] PCA+SVM...\")\n",
    "\n",
    "# Log-likelihood (aproximación)\n",
    "svm_log_lik = calculate_log_likelihood_svm(svm_model, X_test_pca_scaled, y_test)\n",
    "\n",
    "# \"Parámetros efectivos\" del SVM: coeficientes de decisión\n",
    "# LinearSVC con 10 clases tiene 10 vectores de coeficientes\n",
    "svm_params = svm_model.coef_.size + svm_model.intercept_.size\n",
    "\n",
    "# AIC y BIC\n",
    "svm_aic = calculate_aic(svm_log_lik, svm_params)\n",
    "svm_bic = calculate_bic(svm_log_lik, svm_params, n_test)\n",
    "\n",
    "# FIC - SVM no tiene \"FLOPs\" en el sentido tradicional\n",
    "# Estimamos operaciones: PCA transform + dot products\n",
    "pca_ops = X_test_pca.shape[1] * X_test_flat.shape[1]  # 150 * 784\n",
    "svm_ops = X_test_pca.shape[1] * svm_model.coef_.shape[0]  # 150 * 10\n",
    "svm_flops = pca_ops + svm_ops  # Aproximación conservadora\n",
    "\n",
    "print(f\"  FLOPs estimados por muestra: {svm_flops:,}\")\n",
    "\n",
    "# Calcular FIC\n",
    "svm_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=svm_log_lik,\n",
    "    flops=svm_flops,\n",
    "    n_params=svm_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "svm_fic = svm_fic_result['fic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d8cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE CRITERIOS DE INFORMACIÓN\n",
      "================================================================================\n",
      "\n",
      "Modelo          Params       FLOPs           AIC             BIC             FIC            \n",
      "---------------------------------------------------------------------------------------\n",
      "MLP             535,818      1,885,440       1072429.87      4935860.03      5358974.43     \n",
      "CNN             421,834      27,378,816      843971.16       3885537.88      4218645.59     \n",
      "PCA+SVM         1,510        119,100         14973.55        25861.17        27053.91       \n",
      "\n",
      "================================================================================\n",
      "MODELOS SELECCIONADOS POR CADA CRITERIO\n",
      "================================================================================\n",
      "\n",
      "Criterio        Modelo Seleccionado  Valor           Razón                                   \n",
      "------------------------------------------------------------------------------------------\n",
      "AIC             PCA+SVM              14973.55        Minimiza params + ajuste                \n",
      "BIC             PCA+SVM              25861.17        Penaliza más los params                 \n",
      "FIC             PCA+SVM              27053.91        Considera params + FLOPs + ajuste       \n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS: ¿Por qué cada criterio prefiere diferente modelo?\n",
      "================================================================================\n",
      "\n",
      "AIC selecciona: PCA+SVM\n",
      "  → Penaliza solo 2*k (# parámetros)\n",
      "  → Ignora costo computacional (FLOPs)\n",
      "  → PCA+SVM tiene 91.55% accuracy\n",
      "  \n",
      "BIC selecciona: PCA+SVM\n",
      "  → Penaliza k*log(n) - más estricto que AIC\n",
      "  → Favorece modelos más simples\n",
      "  → También ignora FLOPs completamente\n",
      "  → PCA+SVM tiene 91.55% accuracy\n",
      "\n",
      "FIC selecciona: PCA+SVM (α=5.0, β=0.5, λ=0.3)\n",
      "  → Penaliza FUERTEMENTE los FLOPs\n",
      "  → También considera parámetros (pero con menos peso)\n",
      "  → Balance entre precisión y eficiencia computacional\n",
      "  → PCA+SVM tiene 91.55% accuracy con 119,100 FLOPs\n",
      "  \n",
      "Trade-off detectado por FIC:\n",
      "  CNN: 99.10% acc, 27,378,816 FLOPs\n",
      "  MLP: 97.95% acc, 1,885,440 FLOPs\n",
      "  → Diferencia accuracy: 1.15%\n",
      "  → Diferencia FLOPs: 14.5x más costoso\n",
      "  → ¿Vale la pena 14.5x más FLOPs por 1.15% mejor?\n",
      "  → FIC dice: NO\n",
      "\n",
      "================================================================================\n",
      "RECOMENDACIÓN SEGÚN CONTEXTO\n",
      "================================================================================\n",
      "\n",
      "📱 MÓVIL/EDGE (recursos limitados):\n",
      "   → Usar FIC para seleccionar modelo\n",
      "   → FLOPs son críticos para latencia y batería\n",
      "   → Con α=5.0, FIC penaliza fuertemente modelos costosos\n",
      "   \n",
      "🖥️  SERVIDOR (recursos abundantes):\n",
      "   → AIC/BIC suficientes si solo importa precisión\n",
      "   → FLOPs menos relevantes, priorizar accuracy\n",
      "   \n",
      "⚖️  PRODUCCIÓN BALANCEADA (nuestra configuración):\n",
      "   → FIC con α=5.0, β=0.5, λ=0.3\n",
      "   → Penaliza modelos ineficientes\n",
      "   → Prefiere MLP si CNN no ofrece mejora sustancial\n",
      "   → Ideal: CNN debe ser >2% mejor para justificar 14x FLOPs\n",
      "\n",
      "💡 AJUSTAR SENSIBILIDAD:\n",
      "   α más alto (7.0-10.0) → Penaliza AÚN MÁS los FLOPs\n",
      "   λ más bajo (0.1-0.2)  → Más sensible a diferencias absolutas\n",
      "   β más bajo (0.1-0.3)  → Ignora casi completamente los parámetros\n",
      "\n",
      "\n",
      "================================================================================\n",
      "VERIFICACIÓN Y EVALUACIÓN COMPLETADOS\n",
      "================================================================================\n",
      "\n",
      "Los 3 modelos han sido entrenados y evaluados con AIC, BIC y FIC\n",
      "\n",
      "💡 Observaciones clave:\n",
      "  - AIC/BIC ignoran FLOPs completamente\n",
      "  - FIC considera el costo computacional real\n",
      "  - Diferentes criterios pueden seleccionar diferentes modelos\n",
      "  - La elección depende del contexto de deployment\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN DE CRITERIOS DE INFORMACIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Modelo':<15} {'Params':<12} {'FLOPs':<15} {'AIC':<15} {'BIC':<15} {'FIC':<15}\")\n",
    "print(\"-\" * 87)\n",
    "print(f\"{'MLP':<15} {mlp_params:<12,} {mlp_flops:<15,} {mlp_aic:<15.2f} {mlp_bic:<15.2f} {mlp_fic:<15.2f}\")\n",
    "print(f\"{'CNN':<15} {cnn_params:<12,} {cnn_flops:<15,} {cnn_aic:<15.2f} {cnn_bic:<15.2f} {cnn_fic:<15.2f}\")\n",
    "print(f\"{'PCA+SVM':<15} {svm_params:<12,} {svm_flops:<15,} {svm_aic:<15.2f} {svm_bic:<15.2f} {svm_fic:<15.2f}\")\n",
    "\n",
    "# Determinar ganadores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELOS SELECCIONADOS POR CADA CRITERIO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {'MLP': (mlp_aic, mlp_bic, mlp_fic, mlp_test_acc, mlp_flops),\n",
    "          'CNN': (cnn_aic, cnn_bic, cnn_fic, cnn_test_acc, cnn_flops),\n",
    "          'PCA+SVM': (svm_aic, svm_bic, svm_fic, svm_test_acc, svm_flops)}\n",
    "\n",
    "aic_winner = min(models.items(), key=lambda x: x[1][0])\n",
    "bic_winner = min(models.items(), key=lambda x: x[1][1])\n",
    "fic_winner = min(models.items(), key=lambda x: x[1][2])\n",
    "\n",
    "print(f\"\\n{'Criterio':<15} {'Modelo Seleccionado':<20} {'Valor':<15} {'Razón':<40}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'AIC':<15} {aic_winner[0]:<20} {aic_winner[1][0]:<15.2f} {'Minimiza params + ajuste':<40}\")\n",
    "print(f\"{'BIC':<15} {bic_winner[0]:<20} {bic_winner[1][1]:<15.2f} {'Penaliza más los params':<40}\")\n",
    "print(f\"{'FIC':<15} {fic_winner[0]:<20} {fic_winner[1][2]:<15.2f} {'Considera params + FLOPs + ajuste':<40}\")\n",
    "\n",
    "# Análisis de diferencias\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS: ¿Por qué cada criterio prefiere diferente modelo?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "AIC selecciona: {aic_winner[0]}\n",
    "  → Penaliza solo 2*k (# parámetros)\n",
    "  → Ignora costo computacional (FLOPs)\n",
    "  → {aic_winner[0]} tiene {aic_winner[1][3]:.2f}% accuracy\n",
    "  \n",
    "BIC selecciona: {bic_winner[0]}\n",
    "  → Penaliza k*log(n) - más estricto que AIC\n",
    "  → Favorece modelos más simples\n",
    "  → También ignora FLOPs completamente\n",
    "  → {bic_winner[0]} tiene {bic_winner[1][3]:.2f}% accuracy\n",
    "\n",
    "FIC selecciona: {fic_winner[0]} (α=5.0, β=0.5, λ=0.3)\n",
    "  → Penaliza FUERTEMENTE los FLOPs\n",
    "  → También considera parámetros (pero con menos peso)\n",
    "  → Balance entre precisión y eficiencia computacional\n",
    "  → {fic_winner[0]} tiene {fic_winner[1][3]:.2f}% accuracy con {fic_winner[1][4]:,} FLOPs\n",
    "  \n",
    "Trade-off detectado por FIC:\n",
    "  CNN: {cnn_test_acc:.2f}% acc, {cnn_flops:,} FLOPs\n",
    "  MLP: {mlp_test_acc:.2f}% acc, {mlp_flops:,} FLOPs\n",
    "  → Diferencia accuracy: {cnn_test_acc - mlp_test_acc:.2f}%\n",
    "  → Diferencia FLOPs: {(cnn_flops / mlp_flops):.1f}x más costoso\n",
    "  → ¿Vale la pena {(cnn_flops / mlp_flops):.1f}x más FLOPs por {cnn_test_acc - mlp_test_acc:.2f}% mejor?\n",
    "  → FIC dice: {\"SÍ\" if fic_winner[0] == \"CNN\" else \"NO\"}\n",
    "\"\"\")\n",
    "\n",
    "# Recomendación final\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMENDACIÓN SEGÚN CONTEXTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "📱 MÓVIL/EDGE (recursos limitados):\n",
    "   → Usar FIC para seleccionar modelo\n",
    "   → FLOPs son críticos para latencia y batería\n",
    "   → Con α=5.0, FIC penaliza fuertemente modelos costosos\n",
    "   \n",
    "🖥️  SERVIDOR (recursos abundantes):\n",
    "   → AIC/BIC suficientes si solo importa precisión\n",
    "   → FLOPs menos relevantes, priorizar accuracy\n",
    "   \n",
    "⚖️  PRODUCCIÓN BALANCEADA (nuestra configuración):\n",
    "   → FIC con α=5.0, β=0.5, λ=0.3\n",
    "   → Penaliza modelos ineficientes\n",
    "   → Prefiere MLP si CNN no ofrece mejora sustancial\n",
    "   → Ideal: CNN debe ser >2% mejor para justificar 14x FLOPs\n",
    "\n",
    "💡 AJUSTAR SENSIBILIDAD:\n",
    "   α más alto (7.0-10.0) → Penaliza AÚN MÁS los FLOPs\n",
    "   λ más bajo (0.1-0.2)  → Más sensible a diferencias absolutas\n",
    "   β más bajo (0.1-0.3)  → Ignora casi completamente los parámetros\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICACIÓN Y EVALUACIÓN COMPLETADOS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLos 3 modelos han sido entrenados y evaluados con AIC, BIC y FIC\")\n",
    "print(\"\\n💡 Observaciones clave:\")\n",
    "print(\"  - AIC/BIC ignoran FLOPs completamente\")\n",
    "print(\"  - FIC considera el costo computacional real\")\n",
    "print(\"  - Diferentes criterios pueden seleccionar diferentes modelos\")\n",
    "print(\"  - La elección depende del contexto de deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
