{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d258d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MNIST Classification: Comparaci√≥n de 3 enfoques diferentes\n",
    "1. MLP (Multi-Layer Perceptron) - Baseline denso\n",
    "2. CNN (Convolutional Neural Network) - LeNet-like con BatchNorm\n",
    "3. HOG + SVM (Cl√°sico) - Histograms of Oriented Gradients + Linear SVM\n",
    "\n",
    "Objetivo: Entrenar cada modelo y luego evaluar con AIC, BIC y FIC\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0c392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MNIST: COMPARACI√ìN DE 3 ENFOQUES DE CLASIFICACI√ìN\n",
      "================================================================================\n",
      "\n",
      "Configuraci√≥n:\n",
      "  Device: cpu\n",
      "  Batch size: 128\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MNIST: COMPARACI√ìN DE 3 ENFOQUES DE CLASIFICACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"\\nConfiguraci√≥n:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c945fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CARGANDO DATOS MNIST\n",
      "================================================================================\n",
      "Descargando MNIST desde OpenML...\n",
      "\n",
      "Datasets cargados:\n",
      "  Train: 60000 im√°genes\n",
      "  Test:  10000 im√°genes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CARGAR DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CARGANDO DATOS MNIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cargar MNIST usando sklearn (evita problema de torchvision)\n",
    "print(\"Descargando MNIST desde OpenML...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "X = mnist.data.to_numpy().astype(np.float32)\n",
    "y = mnist.target.to_numpy().astype(np.int64)\n",
    "\n",
    "# Normalizar\n",
    "X = X / 255.0\n",
    "\n",
    "# Split train/test (primeros 60000 train, resto test)\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "# Normalizaci√≥n est√°ndar\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(f\"\\nDatasets cargados:\")\n",
    "print(f\"  Train: {X_train.shape[0]} im√°genes\")\n",
    "print(f\"  Test:  {X_test.shape[0]} im√°genes\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_train_torch = torch.FloatTensor(X_train).view(-1, 1, 28, 28)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test).view(-1, 1, 28, 28)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bd6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 1: MLP (Multi-Layer Perceptron)\n",
      "================================================================================\n",
      "\n",
      "Arquitectura: 784 -> 512 -> 256 -> 10\n",
      "Caracter√≠sticas: Denso, Dropout 0.2, ReLU\n",
      "\n",
      "Par√°metros entrenables: 535,818\n",
      "\n",
      "Entrenando MLP...\n",
      "  Epoch 1/10 - Loss: 0.2541, Acc: 92.35%\n",
      "  Epoch 2/10 - Loss: 0.1104, Acc: 96.62%\n",
      "  Epoch 3/10 - Loss: 0.0832, Acc: 97.40%\n",
      "  Epoch 4/10 - Loss: 0.0650, Acc: 97.97%\n",
      "  Epoch 5/10 - Loss: 0.0545, Acc: 98.27%\n",
      "  Epoch 6/10 - Loss: 0.0484, Acc: 98.46%\n",
      "  Epoch 7/10 - Loss: 0.0413, Acc: 98.66%\n",
      "  Epoch 8/10 - Loss: 0.0423, Acc: 98.61%\n",
      "  Epoch 9/10 - Loss: 0.0356, Acc: 98.86%\n",
      "  Epoch 10/10 - Loss: 0.0342, Acc: 98.89%\n",
      "\n",
      "Evaluando MLP...\n",
      "\n",
      "Resultados MLP:\n",
      "  Test Accuracy: 97.95%\n",
      "  Test Loss: 0.0785\n",
      "  Training Time: 68.7s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 1: MLP (Multi-Layer Perceptron)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 1: MLP (Multi-Layer Perceptron)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nArquitectura: 784 -> 512 -> 256 -> 10\")\n",
    "print(\"Caracter√≠sticas: Denso, Dropout 0.2, ReLU\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Cuenta par√°metros entrenables\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Inicializar MLP\n",
    "mlp_model = MLP().to(DEVICE)\n",
    "mlp_params = count_parameters(mlp_model)\n",
    "\n",
    "print(f\"\\nPar√°metros entrenables: {mlp_params:,}\")\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\nEntrenando MLP...\")\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=LEARNING_RATE)\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mlp_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        mlp_optimizer.zero_grad()\n",
    "        output = mlp_model(data)\n",
    "        loss = mlp_criterion(output, target)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    train_acc = 100. * correct / total\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "\n",
    "mlp_train_time = time.time() - mlp_start_time\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(\"\\nEvaluando MLP...\")\n",
    "mlp_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = mlp_model(data)\n",
    "        test_loss += mlp_criterion(output, target).item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "mlp_test_acc = 100. * correct / total\n",
    "mlp_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nResultados MLP:\")\n",
    "print(f\"  Test Accuracy: {mlp_test_acc:.2f}%\")\n",
    "print(f\"  Test Loss: {mlp_test_loss:.4f}\")\n",
    "print(f\"  Training Time: {mlp_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1f125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 2: CNN (LeNet-like con BatchNorm)\n",
      "================================================================================\n",
      "\n",
      "Arquitectura:\n",
      "  Conv1: 1->32 (3x3) -> BN -> ReLU -> MaxPool(2x2)\n",
      "  Conv2: 32->64 (3x3) -> BN -> ReLU -> MaxPool(2x2)\n",
      "  FC: 1600 -> 128 -> 10\n",
      "\n",
      "Par√°metros entrenables: 421,834\n",
      "\n",
      "Entrenando CNN...\n",
      "  Epoch 1/10 - Loss: 0.1870, Acc: 94.31%\n",
      "  Epoch 2/10 - Loss: 0.0671, Acc: 97.98%\n",
      "  Epoch 3/10 - Loss: 0.0513, Acc: 98.41%\n",
      "  Epoch 4/10 - Loss: 0.0417, Acc: 98.75%\n",
      "  Epoch 5/10 - Loss: 0.0350, Acc: 98.94%\n",
      "  Epoch 6/10 - Loss: 0.0324, Acc: 98.96%\n",
      "  Epoch 7/10 - Loss: 0.0283, Acc: 99.08%\n",
      "  Epoch 8/10 - Loss: 0.0266, Acc: 99.16%\n",
      "  Epoch 9/10 - Loss: 0.0231, Acc: 99.24%\n",
      "  Epoch 10/10 - Loss: 0.0200, Acc: 99.33%\n",
      "\n",
      "Evaluando CNN...\n",
      "\n",
      "Resultados CNN:\n",
      "  Test Accuracy: 99.10%\n",
      "  Test Loss: 0.0300\n",
      "  Training Time: 559.3s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 2: CNN (Convolutional Neural Network)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 2: CNN (LeNet-like con BatchNorm)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nArquitectura:\")\n",
    "print(\"  Conv1: 1->32 (3x3) -> BN -> ReLU -> MaxPool(2x2)\")\n",
    "print(\"  Conv2: 32->64 (3x3) -> BN -> ReLU -> MaxPool(2x2)\")\n",
    "print(\"  FC: 1600 -> 128 -> 10\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Bloque 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bloque 2: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Clasificador denso\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bloque 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Bloque 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Clasificador\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Inicializar CNN\n",
    "cnn_model = CNN().to(DEVICE)\n",
    "cnn_params = count_parameters(cnn_model)\n",
    "\n",
    "print(f\"\\nPar√°metros entrenables: {cnn_params:,}\")\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\nEntrenando CNN...\")\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cnn_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        cnn_optimizer.zero_grad()\n",
    "        output = cnn_model(data)\n",
    "        loss = cnn_criterion(output, target)\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    train_acc = 100. * correct / total\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "\n",
    "cnn_train_time = time.time() - cnn_start_time\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(\"\\nEvaluando CNN...\")\n",
    "cnn_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = cnn_model(data)\n",
    "        test_loss += cnn_criterion(output, target).item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "cnn_test_acc = 100. * correct / total\n",
    "cnn_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nResultados CNN:\")\n",
    "print(f\"  Test Accuracy: {cnn_test_acc:.2f}%\")\n",
    "print(f\"  Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"  Training Time: {cnn_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a255ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO 3: PCA + Linear SVM (Cl√°sico)\n",
      "================================================================================\n",
      "\n",
      "Caracter√≠sticas:\n",
      "  Reducci√≥n dim: PCA (784 -> 150 componentes)\n",
      "  Clasificador: Linear SVM (C=1.0)\n",
      "\n",
      "Preparando datos...\n",
      "\n",
      "Aplicando PCA (784 -> 150 dimensiones)...\n",
      "Varianza explicada: 94.84%\n",
      "\n",
      "Entrenando Linear SVM...\n",
      "[LibLinear]\n",
      "Evaluando SVM...\n",
      "\n",
      "Resultados SVM:\n",
      "  Test Accuracy: 91.55%\n",
      "  Training Time: 11.7s\n",
      "  PCA componentes: 150\n",
      "  Varianza capturada: 94.8%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODELO 3: PCA + Linear SVM (Cl√°sico)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO 3: PCA + Linear SVM (Cl√°sico)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCaracter√≠sticas:\")\n",
    "print(\"  Reducci√≥n dim: PCA (784 -> 150 componentes)\")\n",
    "print(\"  Clasificador: Linear SVM (C=1.0)\")\n",
    "\n",
    "# Preparar datos para SVM (p√≠xeles crudos)\n",
    "print(\"\\nPreparando datos...\")\n",
    "\n",
    "# Usar los datos ya normalizados\n",
    "X_train_flat = X_train  # Ya est√° en forma (60000, 784)\n",
    "X_test_flat = X_test    # Ya est√° en forma (10000, 784)\n",
    "\n",
    "svm_start_time = time.time()\n",
    "\n",
    "# Aplicar PCA para reducir dimensionalidad\n",
    "print(\"\\nAplicando PCA (784 -> 150 dimensiones)...\")\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Varianza explicada: {explained_var*100:.2f}%\")\n",
    "\n",
    "# Escalar caracter√≠sticas\n",
    "scaler = StandardScaler()\n",
    "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "# Entrenar SVM\n",
    "print(\"\\nEntrenando Linear SVM...\")\n",
    "svm_model = LinearSVC(C=1.0, max_iter=1000, random_state=42, verbose=1)\n",
    "svm_model.fit(X_train_pca_scaled, y_train)\n",
    "\n",
    "svm_train_time = time.time() - svm_start_time\n",
    "\n",
    "# Evaluar\n",
    "print(\"\\nEvaluando SVM...\")\n",
    "svm_predictions = svm_model.predict(X_test_pca_scaled)\n",
    "svm_test_acc = 100. * np.mean(svm_predictions == y_test)\n",
    "\n",
    "print(f\"\\nResultados SVM:\")\n",
    "print(f\"  Test Accuracy: {svm_test_acc:.2f}%\")\n",
    "print(f\"  Training Time: {svm_train_time:.1f}s\")\n",
    "print(f\"  PCA componentes: {X_train_pca.shape[1]}\")\n",
    "print(f\"  Varianza capturada: {explained_var*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "764f646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN COMPARATIVO\n",
      "================================================================================\n",
      "\n",
      "Modelo          Par√°metros      Test Acc     Train Time     \n",
      "------------------------------------------------------------\n",
      "MLP             535,818         97.95       % 68.7           s\n",
      "CNN             421,834         99.10       % 559.3          s\n",
      "PCA+SVM         150             91.55       % 11.7           s\n",
      "\n",
      "================================================================================\n",
      "CALCULANDO CRITERIOS DE INFORMACI√ìN: AIC, BIC, FIC\n",
      "================================================================================\n",
      "\n",
      "Configuraci√≥n del FIC:\n",
      "  Œ± = 5.0   (peso de penalizaci√≥n de FLOPs - aumentado)\n",
      "  Œ≤ = 0.5   (peso de penalizaci√≥n de par√°metros - reducido)\n",
      "  Œª = 0.3   (30% log, 70% lineal - sensible a diferencias absolutas)\n",
      "\n",
      "  Objetivo: Priorizar eficiencia computacional cuando accuracy es similar\n",
      "\n",
      "Calculando m√©tricas para cada modelo...\n",
      "\n",
      "[1/3] MLP...\n",
      "  Contando FLOPs del MLP...\n",
      "  FLOPs por muestra: 1,885,440\n",
      "\n",
      "[2/3] CNN...\n",
      "  Contando FLOPs de la CNN...\n",
      "  FLOPs por muestra: 27,378,816\n",
      "\n",
      "[3/3] PCA+SVM...\n",
      "  FLOPs estimados por muestra: 119,100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RESUMEN COMPARATIVO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN COMPARATIVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Modelo':<15} {'Par√°metros':<15} {'Test Acc':<12} {'Train Time':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MLP':<15} {mlp_params:<15,} {mlp_test_acc:<12.2f}% {mlp_train_time:<15.1f}s\")\n",
    "print(f\"{'CNN':<15} {cnn_params:<15,} {cnn_test_acc:<12.2f}% {cnn_train_time:<15.1f}s\")\n",
    "print(f\"{'PCA+SVM':<15} {X_train_pca.shape[1]:<15} {svm_test_acc:<12.2f}% {svm_train_time:<15.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULAR AIC, BIC Y FIC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULANDO CRITERIOS DE INFORMACI√ìN: AIC, BIC, FIC\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nConfiguraci√≥n del FIC:\")\n",
    "print(\"  Œ± = 5.0   (peso de penalizaci√≥n de FLOPs - aumentado)\")\n",
    "print(\"  Œ≤ = 0.5   (peso de penalizaci√≥n de par√°metros - reducido)\")\n",
    "print(\"  Œª = 0.3   (30% log, 70% lineal - sensible a diferencias absolutas)\")\n",
    "print(\"\\n  Objetivo: Priorizar eficiencia computacional cuando accuracy es similar\")\n",
    "\n",
    "from flop_counter import FlopInformationCriterion, count_model_flops\n",
    "\n",
    "# Configurar FIC con m√°s peso en FLOPs\n",
    "# Œ±=5.0: Mayor penalizaci√≥n de FLOPs (vs Œ±=2.0 est√°ndar)\n",
    "# Œª=0.3: 30% logar√≠tmico, 70% lineal (m√°s sensible a diferencias absolutas)\n",
    "# Œ≤=0.5: Menor peso en par√°metros (para que FLOPs dominen m√°s)\n",
    "OPTIMAL_LAMBDA = 0.3\n",
    "ALPHA = 0.1\n",
    "BETA = 10\n",
    "fic_calculator = FlopInformationCriterion(\n",
    "    variant='custom',\n",
    "    alpha=ALPHA,  # Penaliza m√°s los FLOPs\n",
    "    beta=BETA,   # Menos peso a par√°metros\n",
    "    flops_scale=f'parametric_log_linear_{OPTIMAL_LAMBDA}'\n",
    ")\n",
    "\n",
    "def calculate_aic(log_likelihood, k):\n",
    "    \"\"\"AIC = -2*log(L) + 2*k\"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "def calculate_bic(log_likelihood, k, n):\n",
    "    \"\"\"BIC = -2*log(L) + k*log(n)\"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "def calculate_log_likelihood_pytorch(model, data_loader, device):\n",
    "    \"\"\"Calcula -2*log(L) para modelo PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            n_samples += len(target)\n",
    "    \n",
    "    # -2*log(L) = 2 * CrossEntropyLoss (sum)\n",
    "    return total_loss\n",
    "\n",
    "def calculate_log_likelihood_svm(model, X, y):\n",
    "    \"\"\"Calcula -2*log(L) para SVM (aproximaci√≥n usando hinge loss)\"\"\"\n",
    "    # Decision function da scores sin calibrar\n",
    "    decision_scores = model.decision_function(X)\n",
    "    \n",
    "    # Convertir a probabilidades usando softmax\n",
    "    exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # Cross-entropy\n",
    "    y_int = y.astype(int)\n",
    "    log_likelihood = -2 * np.sum(np.log(probs[np.arange(len(y)), y_int] + 1e-10))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "print(\"\\nCalculando m√©tricas para cada modelo...\")\n",
    "\n",
    "# ---------------------------\n",
    "# MLP\n",
    "# ---------------------------\n",
    "print(\"\\n[1/3] MLP...\")\n",
    "\n",
    "# Log-likelihood\n",
    "mlp_log_lik = calculate_log_likelihood_pytorch(mlp_model, test_loader, DEVICE)\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "# AIC y BIC\n",
    "mlp_aic = calculate_aic(mlp_log_lik, mlp_params)\n",
    "mlp_bic = calculate_bic(mlp_log_lik, mlp_params, n_test)\n",
    "\n",
    "# FIC - necesitamos contar FLOPs\n",
    "print(\"  Contando FLOPs del MLP...\")\n",
    "sample_input = X_test_torch[:1].to(DEVICE)  # Una muestra para profiling\n",
    "try:\n",
    "    mlp_flops_result = count_model_flops(\n",
    "        model=mlp_model,\n",
    "        input_data=sample_input,\n",
    "        framework='torch',\n",
    "        verbose=False\n",
    "    )\n",
    "    mlp_flops = mlp_flops_result['total_flops']\n",
    "    print(f\"  FLOPs por muestra: {mlp_flops:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Advertencia: No se pudieron contar FLOPs: {e}\")\n",
    "    mlp_flops = 0\n",
    "\n",
    "# Calcular FIC\n",
    "mlp_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=mlp_log_lik,\n",
    "    flops=mlp_flops,\n",
    "    n_params=mlp_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "mlp_fic = mlp_fic_result['fic']\n",
    "\n",
    "# ---------------------------\n",
    "# CNN\n",
    "# ---------------------------\n",
    "print(\"\\n[2/3] CNN...\")\n",
    "\n",
    "# Log-likelihood\n",
    "cnn_log_lik = calculate_log_likelihood_pytorch(cnn_model, test_loader, DEVICE)\n",
    "\n",
    "# AIC y BIC\n",
    "cnn_aic = calculate_aic(cnn_log_lik, cnn_params)\n",
    "cnn_bic = calculate_bic(cnn_log_lik, cnn_params, n_test)\n",
    "\n",
    "# FIC - contar FLOPs\n",
    "print(\"  Contando FLOPs de la CNN...\")\n",
    "try:\n",
    "    cnn_flops_result = count_model_flops(\n",
    "        model=cnn_model,\n",
    "        input_data=sample_input,\n",
    "        framework='torch',\n",
    "        verbose=False\n",
    "    )\n",
    "    cnn_flops = cnn_flops_result['total_flops']\n",
    "    print(f\"  FLOPs por muestra: {cnn_flops:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Advertencia: No se pudieron contar FLOPs: {e}\")\n",
    "    cnn_flops = 0\n",
    "\n",
    "# Calcular FIC\n",
    "cnn_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=cnn_log_lik,\n",
    "    flops=cnn_flops,\n",
    "    n_params=cnn_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "cnn_fic = cnn_fic_result['fic']\n",
    "\n",
    "# ---------------------------\n",
    "# PCA+SVM\n",
    "# ---------------------------\n",
    "print(\"\\n[3/3] PCA+SVM...\")\n",
    "\n",
    "# Log-likelihood (aproximaci√≥n)\n",
    "svm_log_lik = calculate_log_likelihood_svm(svm_model, X_test_pca_scaled, y_test)\n",
    "\n",
    "# \"Par√°metros efectivos\" del SVM: coeficientes de decisi√≥n\n",
    "# LinearSVC con 10 clases tiene 10 vectores de coeficientes\n",
    "svm_params = svm_model.coef_.size + svm_model.intercept_.size\n",
    "\n",
    "# AIC y BIC\n",
    "svm_aic = calculate_aic(svm_log_lik, svm_params)\n",
    "svm_bic = calculate_bic(svm_log_lik, svm_params, n_test)\n",
    "\n",
    "# FIC - SVM no tiene \"FLOPs\" en el sentido tradicional\n",
    "# Estimamos operaciones: PCA transform + dot products\n",
    "pca_ops = X_test_pca.shape[1] * X_test_flat.shape[1]  # 150 * 784\n",
    "svm_ops = X_test_pca.shape[1] * svm_model.coef_.shape[0]  # 150 * 10\n",
    "svm_flops = pca_ops + svm_ops  # Aproximaci√≥n conservadora\n",
    "\n",
    "print(f\"  FLOPs estimados por muestra: {svm_flops:,}\")\n",
    "\n",
    "# Calcular FIC\n",
    "svm_fic_result = fic_calculator.calculate_fic(\n",
    "    log_likelihood=svm_log_lik,\n",
    "    flops=svm_flops,\n",
    "    n_params=svm_params,\n",
    "    n_samples=n_test\n",
    ")\n",
    "svm_fic = svm_fic_result['fic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d8cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARACI√ìN DE CRITERIOS DE INFORMACI√ìN\n",
      "================================================================================\n",
      "\n",
      "Modelo          Params       FLOPs           AIC             BIC             FIC            \n",
      "---------------------------------------------------------------------------------------\n",
      "MLP             535,818      1,885,440       1072429.87      4935860.03      5358974.43     \n",
      "CNN             421,834      27,378,816      843971.16       3885537.88      4218645.59     \n",
      "PCA+SVM         1,510        119,100         14973.55        25861.17        27053.91       \n",
      "\n",
      "================================================================================\n",
      "MODELOS SELECCIONADOS POR CADA CRITERIO\n",
      "================================================================================\n",
      "\n",
      "Criterio        Modelo Seleccionado  Valor           Raz√≥n                                   \n",
      "------------------------------------------------------------------------------------------\n",
      "AIC             PCA+SVM              14973.55        Minimiza params + ajuste                \n",
      "BIC             PCA+SVM              25861.17        Penaliza m√°s los params                 \n",
      "FIC             PCA+SVM              27053.91        Considera params + FLOPs + ajuste       \n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: ¬øPor qu√© cada criterio prefiere diferente modelo?\n",
      "================================================================================\n",
      "\n",
      "AIC selecciona: PCA+SVM\n",
      "  ‚Üí Penaliza solo 2*k (# par√°metros)\n",
      "  ‚Üí Ignora costo computacional (FLOPs)\n",
      "  ‚Üí PCA+SVM tiene 91.55% accuracy\n",
      "  \n",
      "BIC selecciona: PCA+SVM\n",
      "  ‚Üí Penaliza k*log(n) - m√°s estricto que AIC\n",
      "  ‚Üí Favorece modelos m√°s simples\n",
      "  ‚Üí Tambi√©n ignora FLOPs completamente\n",
      "  ‚Üí PCA+SVM tiene 91.55% accuracy\n",
      "\n",
      "FIC selecciona: PCA+SVM (Œ±=5.0, Œ≤=0.5, Œª=0.3)\n",
      "  ‚Üí Penaliza FUERTEMENTE los FLOPs\n",
      "  ‚Üí Tambi√©n considera par√°metros (pero con menos peso)\n",
      "  ‚Üí Balance entre precisi√≥n y eficiencia computacional\n",
      "  ‚Üí PCA+SVM tiene 91.55% accuracy con 119,100 FLOPs\n",
      "  \n",
      "Trade-off detectado por FIC:\n",
      "  CNN: 99.10% acc, 27,378,816 FLOPs\n",
      "  MLP: 97.95% acc, 1,885,440 FLOPs\n",
      "  ‚Üí Diferencia accuracy: 1.15%\n",
      "  ‚Üí Diferencia FLOPs: 14.5x m√°s costoso\n",
      "  ‚Üí ¬øVale la pena 14.5x m√°s FLOPs por 1.15% mejor?\n",
      "  ‚Üí FIC dice: NO\n",
      "\n",
      "================================================================================\n",
      "RECOMENDACI√ìN SEG√öN CONTEXTO\n",
      "================================================================================\n",
      "\n",
      "üì± M√ìVIL/EDGE (recursos limitados):\n",
      "   ‚Üí Usar FIC para seleccionar modelo\n",
      "   ‚Üí FLOPs son cr√≠ticos para latencia y bater√≠a\n",
      "   ‚Üí Con Œ±=5.0, FIC penaliza fuertemente modelos costosos\n",
      "   \n",
      "üñ•Ô∏è  SERVIDOR (recursos abundantes):\n",
      "   ‚Üí AIC/BIC suficientes si solo importa precisi√≥n\n",
      "   ‚Üí FLOPs menos relevantes, priorizar accuracy\n",
      "   \n",
      "‚öñÔ∏è  PRODUCCI√ìN BALANCEADA (nuestra configuraci√≥n):\n",
      "   ‚Üí FIC con Œ±=5.0, Œ≤=0.5, Œª=0.3\n",
      "   ‚Üí Penaliza modelos ineficientes\n",
      "   ‚Üí Prefiere MLP si CNN no ofrece mejora sustancial\n",
      "   ‚Üí Ideal: CNN debe ser >2% mejor para justificar 14x FLOPs\n",
      "\n",
      "üí° AJUSTAR SENSIBILIDAD:\n",
      "   Œ± m√°s alto (7.0-10.0) ‚Üí Penaliza A√öN M√ÅS los FLOPs\n",
      "   Œª m√°s bajo (0.1-0.2)  ‚Üí M√°s sensible a diferencias absolutas\n",
      "   Œ≤ m√°s bajo (0.1-0.3)  ‚Üí Ignora casi completamente los par√°metros\n",
      "\n",
      "\n",
      "================================================================================\n",
      "VERIFICACI√ìN Y EVALUACI√ìN COMPLETADOS\n",
      "================================================================================\n",
      "\n",
      "Los 3 modelos han sido entrenados y evaluados con AIC, BIC y FIC\n",
      "\n",
      "üí° Observaciones clave:\n",
      "  - AIC/BIC ignoran FLOPs completamente\n",
      "  - FIC considera el costo computacional real\n",
      "  - Diferentes criterios pueden seleccionar diferentes modelos\n",
      "  - La elecci√≥n depende del contexto de deployment\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACI√ìN DE CRITERIOS DE INFORMACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Modelo':<15} {'Params':<12} {'FLOPs':<15} {'AIC':<15} {'BIC':<15} {'FIC':<15}\")\n",
    "print(\"-\" * 87)\n",
    "print(f\"{'MLP':<15} {mlp_params:<12,} {mlp_flops:<15,} {mlp_aic:<15.2f} {mlp_bic:<15.2f} {mlp_fic:<15.2f}\")\n",
    "print(f\"{'CNN':<15} {cnn_params:<12,} {cnn_flops:<15,} {cnn_aic:<15.2f} {cnn_bic:<15.2f} {cnn_fic:<15.2f}\")\n",
    "print(f\"{'PCA+SVM':<15} {svm_params:<12,} {svm_flops:<15,} {svm_aic:<15.2f} {svm_bic:<15.2f} {svm_fic:<15.2f}\")\n",
    "\n",
    "# Determinar ganadores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELOS SELECCIONADOS POR CADA CRITERIO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {'MLP': (mlp_aic, mlp_bic, mlp_fic, mlp_test_acc, mlp_flops),\n",
    "          'CNN': (cnn_aic, cnn_bic, cnn_fic, cnn_test_acc, cnn_flops),\n",
    "          'PCA+SVM': (svm_aic, svm_bic, svm_fic, svm_test_acc, svm_flops)}\n",
    "\n",
    "aic_winner = min(models.items(), key=lambda x: x[1][0])\n",
    "bic_winner = min(models.items(), key=lambda x: x[1][1])\n",
    "fic_winner = min(models.items(), key=lambda x: x[1][2])\n",
    "\n",
    "print(f\"\\n{'Criterio':<15} {'Modelo Seleccionado':<20} {'Valor':<15} {'Raz√≥n':<40}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'AIC':<15} {aic_winner[0]:<20} {aic_winner[1][0]:<15.2f} {'Minimiza params + ajuste':<40}\")\n",
    "print(f\"{'BIC':<15} {bic_winner[0]:<20} {bic_winner[1][1]:<15.2f} {'Penaliza m√°s los params':<40}\")\n",
    "print(f\"{'FIC':<15} {fic_winner[0]:<20} {fic_winner[1][2]:<15.2f} {'Considera params + FLOPs + ajuste':<40}\")\n",
    "\n",
    "# An√°lisis de diferencias\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS: ¬øPor qu√© cada criterio prefiere diferente modelo?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "AIC selecciona: {aic_winner[0]}\n",
    "  ‚Üí Penaliza solo 2*k (# par√°metros)\n",
    "  ‚Üí Ignora costo computacional (FLOPs)\n",
    "  ‚Üí {aic_winner[0]} tiene {aic_winner[1][3]:.2f}% accuracy\n",
    "  \n",
    "BIC selecciona: {bic_winner[0]}\n",
    "  ‚Üí Penaliza k*log(n) - m√°s estricto que AIC\n",
    "  ‚Üí Favorece modelos m√°s simples\n",
    "  ‚Üí Tambi√©n ignora FLOPs completamente\n",
    "  ‚Üí {bic_winner[0]} tiene {bic_winner[1][3]:.2f}% accuracy\n",
    "\n",
    "FIC selecciona: {fic_winner[0]} (Œ±=5.0, Œ≤=0.5, Œª=0.3)\n",
    "  ‚Üí Penaliza FUERTEMENTE los FLOPs\n",
    "  ‚Üí Tambi√©n considera par√°metros (pero con menos peso)\n",
    "  ‚Üí Balance entre precisi√≥n y eficiencia computacional\n",
    "  ‚Üí {fic_winner[0]} tiene {fic_winner[1][3]:.2f}% accuracy con {fic_winner[1][4]:,} FLOPs\n",
    "  \n",
    "Trade-off detectado por FIC:\n",
    "  CNN: {cnn_test_acc:.2f}% acc, {cnn_flops:,} FLOPs\n",
    "  MLP: {mlp_test_acc:.2f}% acc, {mlp_flops:,} FLOPs\n",
    "  ‚Üí Diferencia accuracy: {cnn_test_acc - mlp_test_acc:.2f}%\n",
    "  ‚Üí Diferencia FLOPs: {(cnn_flops / mlp_flops):.1f}x m√°s costoso\n",
    "  ‚Üí ¬øVale la pena {(cnn_flops / mlp_flops):.1f}x m√°s FLOPs por {cnn_test_acc - mlp_test_acc:.2f}% mejor?\n",
    "  ‚Üí FIC dice: {\"S√ç\" if fic_winner[0] == \"CNN\" else \"NO\"}\n",
    "\"\"\")\n",
    "\n",
    "# Recomendaci√≥n final\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMENDACI√ìN SEG√öN CONTEXTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üì± M√ìVIL/EDGE (recursos limitados):\n",
    "   ‚Üí Usar FIC para seleccionar modelo\n",
    "   ‚Üí FLOPs son cr√≠ticos para latencia y bater√≠a\n",
    "   ‚Üí Con Œ±=5.0, FIC penaliza fuertemente modelos costosos\n",
    "   \n",
    "üñ•Ô∏è  SERVIDOR (recursos abundantes):\n",
    "   ‚Üí AIC/BIC suficientes si solo importa precisi√≥n\n",
    "   ‚Üí FLOPs menos relevantes, priorizar accuracy\n",
    "   \n",
    "‚öñÔ∏è  PRODUCCI√ìN BALANCEADA (nuestra configuraci√≥n):\n",
    "   ‚Üí FIC con Œ±=5.0, Œ≤=0.5, Œª=0.3\n",
    "   ‚Üí Penaliza modelos ineficientes\n",
    "   ‚Üí Prefiere MLP si CNN no ofrece mejora sustancial\n",
    "   ‚Üí Ideal: CNN debe ser >2% mejor para justificar 14x FLOPs\n",
    "\n",
    "üí° AJUSTAR SENSIBILIDAD:\n",
    "   Œ± m√°s alto (7.0-10.0) ‚Üí Penaliza A√öN M√ÅS los FLOPs\n",
    "   Œª m√°s bajo (0.1-0.2)  ‚Üí M√°s sensible a diferencias absolutas\n",
    "   Œ≤ m√°s bajo (0.1-0.3)  ‚Üí Ignora casi completamente los par√°metros\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICACI√ìN Y EVALUACI√ìN COMPLETADOS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLos 3 modelos han sido entrenados y evaluados con AIC, BIC y FIC\")\n",
    "print(\"\\nüí° Observaciones clave:\")\n",
    "print(\"  - AIC/BIC ignoran FLOPs completamente\")\n",
    "print(\"  - FIC considera el costo computacional real\")\n",
    "print(\"  - Diferentes criterios pueden seleccionar diferentes modelos\")\n",
    "print(\"  - La elecci√≥n depende del contexto de deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
