{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2c0505",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{0. Libs}}$\n",
    "\n",
    "MNIST Classification: Comparación de 3 enfoques diferentes\n",
    "1. MLP (Multi-Layer Perceptron) - Baseline denso\n",
    "2. CNN (Convolutional Neural Network) - LeNet-like con BatchNorm\n",
    "3. HOG + SVM (Clásico) - Histograms of Oriented Gradients + Linear SVM\n",
    "\n",
    "Objetivo: Entrenar cada modelo y luego evaluar con AIC, BIC y FIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259b7b6",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{1. Configuración de parámetros generales}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MNIST: COMPARACIÓN DE 3 ENFOQUES DE CLASIFICACIÓN\n",
      "================================================================================\n",
      "\n",
      "Configuración:\n",
      "  Device: cpu\n",
      "  Batch size: 128\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "res=\"Resultados:\"\n",
    "\n",
    "print(f\"\\nConfiguración:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf66358",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{2. Carga de datos}}$\n",
    "\n",
    "Desde OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c945fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CARGANDO DATOS MNIST\n",
      "====================================================================================================\n",
      "Descargando MNIST desde OpenML...\n",
      "\n",
      "Datasets cargados:\n",
      "  Train: 60000 imágenes\n",
      "  Test:  10000 imágenes\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "X = mnist.data.to_numpy().astype(np.float32)\n",
    "y = mnist.target.to_numpy().astype(np.int64)\n",
    "\n",
    "# Normalizar\n",
    "X = X / 255.0\n",
    "\n",
    "spli=60000\n",
    "# Split train/test\n",
    "X_train = X[:spli]\n",
    "y_train = y[:spli]\n",
    "X_test = X[spli:]\n",
    "y_test = y[spli:]\n",
    "\n",
    "# Normalización estándar\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(f\"\\nDatasets cargados:\")\n",
    "print(f\"  Train: {X_train.shape[0]} imágenes\")\n",
    "print(f\"  Test:  {X_test.shape[0]} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a tensores PyTorch\n",
    "X_train_torch = torch.FloatTensor(X_train).view(-1, 1, 28, 28)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test).view(-1, 1, 28, 28)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed824f",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{3. Funciones auxiliares}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Cuenta parámetros entrenables\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_pytorch_model(model, train_loader, epochs, lr):\n",
    "    \"\"\"Entrena un modelo de PyTorch\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    \"\"\"Evalúa un modelo de PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e83ea",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{4. MNIST}}$\n",
    "\n",
    "Resuelto 10 veces, cada una por un método distinto para comparar cuál es el mejor de acuerdo a un FIC que parametrizamos para economizar en recursos de cómputo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cd4f1",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.1. Regresión Logística}}$\n",
    "\n",
    "Características: Lineal, sin capas ocultas, baseline clásico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 1: REGRESIÓN LOGÍSTICA\n",
      "====================================================================================================\n",
      "Características: Lineal, sin capas ocultas, baseline clásico\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 92.50%\n",
      "  Parámetros: 7,850\n",
      "  Training Time: 14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hecto\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_start = time.time()\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=100, random_state=42, verbose=0)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "logreg_train_time = time.time() - logreg_start\n",
    "logreg_predictions = logreg_model.predict(X_test)\n",
    "logreg_accuracy = 100. * np.mean(logreg_predictions == y_test)\n",
    "\n",
    "logreg_params = logreg_model.coef_.size + logreg_model.intercept_.size\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {logreg_accuracy:.2f}%\")\n",
    "print(f\"  Parámetros: {logreg_params:,}\")\n",
    "print(f\"  Training Time: {logreg_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05139c71",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.2. K-Nearest Neighbors}}$\n",
    "\n",
    "Características: No paramétrico, basado en distancia\n",
    "\n",
    "k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 2: K-NEAREST NEIGHBORS (k=5)\n",
      "====================================================================================================\n",
      "Características: No paramétrico, basado en distancia\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 94.42%\n",
      "  Parámetros: 0 (no paramétrico)\n",
      "  Training Time: 0.1s\n",
      "  Nota: Entrenado con subset de 10000 muestras\n"
     ]
    }
   ],
   "source": [
    "knn_start = time.time()\n",
    "\n",
    "# Usar subset para acelerar\n",
    "subset_size = 10000\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn_model.fit(X_train[:subset_size], y_train[:subset_size])\n",
    "\n",
    "knn_train_time = time.time() - knn_start\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = 100. * np.mean(knn_predictions == y_test)\n",
    "\n",
    "knn_params = 0  # KNN no tiene parámetros tradicionales\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {knn_accuracy:.2f}%\")\n",
    "print(f\"  Parámetros: {knn_params} (no paramétrico)\")\n",
    "print(f\"  Training Time: {knn_train_time:.1f}s\")\n",
    "print(f\"  Nota: Entrenado con subset de {subset_size} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a0ed1",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.3. Random Forest}}$\n",
    "\n",
    "Características: Ensemble, robusto, interpretable\n",
    "\n",
    "100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a255ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 3: RANDOM FOREST (100 árboles)\n",
      "====================================================================================================\n",
      "Características: Ensemble, robusto, interpretable\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 96.80%\n",
      "  Parámetros: 958,830 (nodos totales)\n",
      "  Training Time: 28.1s\n"
     ]
    }
   ],
   "source": [
    "rf_start = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_train_time = time.time() - rf_start\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_accuracy = 100. * np.mean(rf_predictions == y_test)\n",
    "\n",
    "# Estimación de parámetros: nodos × árboles\n",
    "rf_params = sum(tree.tree_.node_count for tree in rf_model.estimators_)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {rf_accuracy:.2f}%\")\n",
    "print(f\"  Parámetros: {rf_params:,} (nodos totales)\")\n",
    "print(f\"  Training Time: {rf_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ef933",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.4. PCA + SVM}}$\n",
    "\n",
    "Características: Reducción dimensionalidad + clasificador lineal\n",
    "\n",
    "PCA (150 comps) + LINEAR SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 4: PCA (150 componentes) + LINEAR SVM\n",
      "====================================================================================================\n",
      "Características: Reducción dimensionalidad + clasificador lineal\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 91.55%\n",
      "  Parámetros: 1,510\n",
      "  Training Time: 21.3s\n",
      "  Varianza explicada: 94.8%\n"
     ]
    }
   ],
   "source": [
    "pca_svm_start = time.time()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "svm_model = LinearSVC(C=1.0, max_iter=1000, random_state=42)\n",
    "svm_model.fit(X_train_pca_scaled, y_train)\n",
    "\n",
    "pca_svm_train_time = time.time() - pca_svm_start\n",
    "pca_svm_predictions = svm_model.predict(X_test_pca_scaled)\n",
    "pca_svm_accuracy = 100. * np.mean(pca_svm_predictions == y_test)\n",
    "\n",
    "pca_svm_params = svm_model.coef_.size + svm_model.intercept_.size\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {pca_svm_accuracy:.2f}%\")\n",
    "print(f\"  Parámetros: {pca_svm_params:,}\")\n",
    "print(f\"  Training Time: {pca_svm_train_time:.1f}s\")\n",
    "print(f\"  Varianza explicada: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e2d2a",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.5. MLP Tiny (muy muy pequeño :3)}}$\n",
    "\n",
    "Características: Red mínima, ultra eficiente\n",
    "\n",
    "MLP TINY (784 -> 64 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 5: MLP TINY (784 -> 64 -> 10)\n",
      "====================================================================================================\n",
      "Características: Red mínima, ultra eficiente\n",
      "\n",
      "Parámetros: 50,890\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.34%\n",
      "  Training Time: 64.7s\n"
     ]
    }
   ],
   "source": [
    "class MLPTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPTiny, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "mlp_tiny_model = MLPTiny().to(DEVICE)\n",
    "mlp_tiny_params = count_parameters(mlp_tiny_model)\n",
    "\n",
    "print(f\"\\nParámetros: {mlp_tiny_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_tiny_start = time.time()\n",
    "train_pytorch_model(mlp_tiny_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_tiny_train_time = time.time() - mlp_tiny_start\n",
    "\n",
    "mlp_tiny_accuracy = evaluate_pytorch_model(mlp_tiny_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_tiny_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_tiny_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d0de3",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.6. MLP Medium (balanceado)}}$\n",
    "\n",
    "Características: Balance entre tamaño y capacidad\n",
    "\n",
    "MLP MEDIUM (784 -> 256 -> 128 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 6: MLP MEDIUM (784 -> 256 -> 128 -> 10)\n",
      "====================================================================================================\n",
      "Características: Balance entre tamaño y capacidad\n",
      "\n",
      "Parámetros: 235,146\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 98.28%\n",
      "  Training Time: 73.1s\n"
     ]
    }
   ],
   "source": [
    "class MLPMedium(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPMedium, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp_medium_model = MLPMedium().to(DEVICE)\n",
    "mlp_medium_params = count_parameters(mlp_medium_model)\n",
    "\n",
    "print(f\"\\nParámetros: {mlp_medium_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_medium_start = time.time()\n",
    "train_pytorch_model(mlp_medium_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_medium_train_time = time.time() - mlp_medium_start\n",
    "\n",
    "mlp_medium_accuracy = evaluate_pytorch_model(mlp_medium_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_medium_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_medium_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9815c",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.7. MLP Large (sobredimensionado)}}$\n",
    "\n",
    "Características: Muy grande, posible overfitting\n",
    "\n",
    "MLP LARGE (784 -> 512 -> 512 -> 256 -> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cacac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 7: MLP LARGE (784 -> 512 -> 512 -> 256 -> 10)\n",
      "====================================================================================================\n",
      "Características: Muy grande, posible overfitting\n",
      "\n",
      "Parámetros: 798,474\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.93%\n",
      "  Training Time: 128.5s\n"
     ]
    }
   ],
   "source": [
    "class MLPLarge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPLarge, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "mlp_large_model = MLPLarge().to(DEVICE)\n",
    "mlp_large_params = count_parameters(mlp_large_model)\n",
    "\n",
    "print(f\"\\nParámetros: {mlp_large_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "mlp_large_start = time.time()\n",
    "train_pytorch_model(mlp_large_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "mlp_large_train_time = time.time() - mlp_large_start\n",
    "\n",
    "mlp_large_accuracy = evaluate_pytorch_model(mlp_large_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {mlp_large_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {mlp_large_train_time:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd91e44",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.8. CNN Tiny (mínima convolucional)}}$\n",
    "\n",
    "Características: Convolucional mínima, muy eficiente\n",
    "\n",
    "CNN TINY (SOLO UNA capa conv :0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 8: CNN TINY (1 capa conv)\n",
      "====================================================================================================\n",
      "Características: Convolucional mínima, muy eficiente\n",
      "\n",
      "Parámetros: 31,530\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 97.98%\n",
      "  Training Time: 155.8s\n"
     ]
    }
   ],
   "source": [
    "class CNNTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTiny, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_tiny_model = CNNTiny().to(DEVICE)\n",
    "cnn_tiny_params = count_parameters(cnn_tiny_model)\n",
    "\n",
    "print(f\"\\nParámetros: {cnn_tiny_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_tiny_start = time.time()\n",
    "train_pytorch_model(cnn_tiny_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_tiny_train_time = time.time() - cnn_tiny_start\n",
    "\n",
    "cnn_tiny_accuracy = evaluate_pytorch_model(cnn_tiny_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_tiny_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_tiny_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6931bc3",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.9. CNN Medium (LeNet-like)}}$\n",
    "\n",
    "Características: LeNet-like moderno, buen balance\n",
    "\n",
    "CNN MEDIUM (2 capas conv con BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 9: CNN MEDIUM (2 capas conv con BatchNorm)\n",
      "====================================================================================================\n",
      "Características: LeNet-like moderno, buen balance\n",
      "\n",
      "Parámetros: 421,834\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 99.19%\n",
      "  Training Time: 693.7s\n"
     ]
    }
   ],
   "source": [
    "class CNNMedium(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMedium, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_medium_model = CNNMedium().to(DEVICE)\n",
    "cnn_medium_params = count_parameters(cnn_medium_model)\n",
    "\n",
    "print(f\"\\nParámetros: {cnn_medium_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_medium_start = time.time()\n",
    "train_pytorch_model(cnn_medium_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_medium_train_time = time.time() - cnn_medium_start\n",
    "\n",
    "cnn_medium_accuracy = evaluate_pytorch_model(cnn_medium_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_medium_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_medium_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322de3a0",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{4.10. CNN Deep (muy profunda)}}$\n",
    "\n",
    "Características: Máxima capacidad, posible overkill para MNIST\n",
    "\n",
    "CNN DEEP (4 capas conv, muy profunda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODELO 10: CNN DEEP (4 capas conv, muy profunda)\n",
      "====================================================================================================\n",
      "Características: Máxima capacidad, posible overkill para MNIST\n",
      "\n",
      "Parámetros: 276,554\n",
      "Entrenando...\n",
      "\n",
      "Resultados:\n",
      "  Test Accuracy: 99.18%\n",
      "  Training Time: 1066.6s\n"
     ]
    }
   ],
   "source": [
    "class CNNDeep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDeep, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 1 * 1, 256)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_deep_model = CNNDeep().to(DEVICE)\n",
    "cnn_deep_params = count_parameters(cnn_deep_model)\n",
    "\n",
    "print(f\"\\nParámetros: {cnn_deep_params:,}\")\n",
    "print(\"Entrenando...\")\n",
    "\n",
    "cnn_deep_start = time.time()\n",
    "train_pytorch_model(cnn_deep_model, train_loader, EPOCHS, LEARNING_RATE)\n",
    "cnn_deep_train_time = time.time() - cnn_deep_start\n",
    "\n",
    "cnn_deep_accuracy = evaluate_pytorch_model(cnn_deep_model, test_loader)\n",
    "\n",
    "print(f\"\\n{res}\")\n",
    "print(f\"  Test Accuracy: {cnn_deep_accuracy:.2f}%\")\n",
    "print(f\"  Training Time: {cnn_deep_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317860b9",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{5. AIC, BIC y FIC pa' todos los modelos}}$\n",
    "\n",
    "Favor de leer los comentarios en el código (sobre todo en los hiperparámetros de la fórmula) para entender mejor qué pasa y por qué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parámetros FIC configurados:\n",
      "  Variant:        custom\n",
      "  Alpha (FLOPs):  1000\n",
      "  Beta (Params):  0.0\n",
      "  Lambda (Balance): 0.1\n",
      "\n",
      "💡 Guía de ajuste:\n",
      "  • ALPHA alto → Penaliza fuertemente modelos con muchos FLOPs\n",
      "  • BETA alto → Penaliza fuertemente modelos con muchos parámetros\n",
      "  • LAMBDA cercano a 0 → Prioriza eficiencia en FLOPs\n",
      "  • LAMBDA cercano a 1 → Prioriza parsimonia en parámetros\n",
      "\n",
      "Ejemplos de configuraciones:\n",
      "  Deployment (edge):   alpha=10.0, beta=1.0, lambda=0.2\n",
      "  Research:            alpha=1.0, beta=2.0, lambda=0.7\n",
      "  Balanced:            alpha=5.0, beta=0.5, lambda=0.3\n"
     ]
    }
   ],
   "source": [
    "from flop_counter import FlopInformationCriterion, count_model_flops\n",
    "\n",
    "# Parámetros ajustables del FIC\n",
    "\n",
    "wonder_flop_priority = 3 # Va de la mano con alpha, en este caso sería la potencia de 10 que prioriza los pocos flops.\n",
    "# No sé si debería colocarla como una variable propia del modelo pero funciona para al menos este ejemplo que lo amerita.\n",
    "# En este caso le di prioridad absoluta a los FLOPs, sin tener en cuenta los parámetros\n",
    "# Incluso si se tiene una beta en cero, alpha debería aumentarse o reducirse de acuerdo a qué tanto nos importa la log-verosimilitud\n",
    "# Teniendo eso en cuenta, unos 10000 de alpha indican que la prioridad que le doy a los FLOPs es mucho mayor que la que le doy a la verosimilitud.\n",
    "# En este caso es extremo porque quiero probar mi punto, pero este notebook ejemplifica que el FIC balanceará diferente de acuerdo a las prioridades que le pongamos.\n",
    "# Si bajamos wonder_flop_priority a, por ejemplo, 2 (10^2 = 100), entonces decimos que quizá nos importan los FLOPs pero la verosimilitud sigue importando bastantito.\n",
    "\n",
    "FIC_VARIANT = 'custom'  # Opciones: 'deployment', 'research', 'balanced', 'custom'\n",
    "FIC_ALPHA = 1000         # Peso del término de FLOPs (mayor = penaliza más FLOPs)\n",
    "FIC_BETA = 0.0         # Peso del término de parámetros (mayor = penaliza más params)\n",
    "FIC_LAMBDA = 0.1        # Balance entre términos (0=solo FLOPs, 1=solo params) Este parámetro depende mucho de la cantidad de FLOPs que se esperan\n",
    "\n",
    "print(f\"\\nParámetros FIC configurados:\")\n",
    "print(f\"  Variant:        {FIC_VARIANT}\")\n",
    "print(f\"  Alpha (FLOPs):  {FIC_ALPHA}\")\n",
    "print(f\"  Beta (Params):  {FIC_BETA}\")\n",
    "print(f\"  Lambda (Balance): {FIC_LAMBDA}\")\n",
    "\n",
    "print(\"\\n💡 Guía de ajuste:\")\n",
    "print(\"  • ALPHA alto → Penaliza fuertemente modelos con muchos FLOPs\")\n",
    "print(\"  • BETA alto → Penaliza fuertemente modelos con muchos parámetros\")\n",
    "print(\"  • LAMBDA cercano a 0 → Prioriza eficiencia en FLOPs\")\n",
    "print(\"  • LAMBDA cercano a 1 → Prioriza parsimonia en parámetros\")\n",
    "print(\"\\nEjemplos de configuraciones:\")\n",
    "print(\"  Deployment (edge):   alpha=10.0, beta=1.0, lambda=0.2\")\n",
    "print(\"  Research:            alpha=1.0, beta=2.0, lambda=0.7\")\n",
    "print(\"  Balanced:            alpha=5.0, beta=0.5, lambda=0.3\")\n",
    "\n",
    "# Configurar FIC con parámetros personalizados\n",
    "fic_calculator = FlopInformationCriterion(\n",
    "    variant=FIC_VARIANT,\n",
    "    alpha=FIC_ALPHA,\n",
    "    beta=FIC_BETA,\n",
    "    lambda_balance=FIC_LAMBDA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7f736",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.1. Funciones de cálculo}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "74fb598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aic(log_likelihood, k):\n",
    "    \"\"\"AIC = 2k - 2ln(L) = log_likelihood + 2k\"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "def calculate_bic(log_likelihood, k, n):\n",
    "    \"\"\"BIC = k*ln(n) - 2ln(L) = log_likelihood + k*ln(n)\"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "def calculate_log_likelihood_pytorch(model, data_loader, device):\n",
    "    \"\"\"Calcula log-likelihood negativa para modelos PyTorch\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def calculate_log_likelihood_sklearn(predictions, y_true, n_classes=10):\n",
    "    \"\"\"Aproximación de log-likelihood para modelos sklearn\"\"\"\n",
    "    correct = (predictions == y_true).astype(float)\n",
    "    # Probabilidades: alta si correcto, baja si incorrecto\n",
    "    probs = np.where(correct == 1, 0.99, 0.01/(n_classes-1))\n",
    "    return -2 * np.sum(np.log(probs + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847c24b",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.2. SKLearn eval}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "99aa5d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[sklearn] Logistic Regression...\n",
      "  Log-likelihood: 10,390\n",
      "  Params: 7,850\n",
      "  FLOPs: 15,680\n",
      "  AIC: 26,090\n",
      "  BIC: 82,691\n",
      "  FIC: 11,370\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[sklearn] KNN (k=5)...\n",
      "  Log-likelihood: 7,781\n",
      "  Params: 100\n",
      "  FLOPs: 39,200,000\n",
      "  AIC: 7,981\n",
      "  BIC: 8,702\n",
      "  FIC: 44,810\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[sklearn] Random Forest...\n",
      "  Log-likelihood: 4,548\n",
      "  Params: 958,830\n",
      "  FLOPs: 9,588,300\n",
      "  AIC: 1,922,208\n",
      "  BIC: 8,835,699\n",
      "  FIC: 14,785\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[sklearn] PCA+SVM...\n",
      "  Log-likelihood: 11,680\n",
      "  Params: 1,510\n",
      "  FLOPs: 120,600\n",
      "  AIC: 14,700\n",
      "  BIC: 25,588\n",
      "  FIC: 12,959\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "sklearn_models = [\n",
    "    (\"Logistic Regression\", logreg_predictions, logreg_params, logreg_accuracy, logreg_train_time),\n",
    "    (\"KNN (k=5)\", knn_predictions, knn_params if knn_params > 0 else 100, knn_accuracy, knn_train_time),\n",
    "    (\"Random Forest\", rf_predictions, rf_params, rf_accuracy, rf_train_time),\n",
    "    (\"PCA+SVM\", pca_svm_predictions, pca_svm_params, pca_svm_accuracy, pca_svm_train_time),\n",
    "]\n",
    "\n",
    "for name, predictions, n_params, accuracy, train_time in sklearn_models:\n",
    "    print(f\"\\n[sklearn] {name}...\")\n",
    "    \n",
    "    # Calcular log-likelihood\n",
    "    log_lik = calculate_log_likelihood_sklearn(predictions, y_test)\n",
    "    \n",
    "    # Calcular AIC y BIC\n",
    "    aic = calculate_aic(log_lik, n_params)\n",
    "    bic = calculate_bic(log_lik, n_params, len(y_test))\n",
    "    \n",
    "    # Estimar FLOPs según el tipo de modelo\n",
    "    if \"Logistic\" in name:\n",
    "        flops = 784 * 10 * 2  # matmul input->output\n",
    "    elif \"KNN\" in name:\n",
    "        flops = 784 * len(y_test) * 5  # distance calculations\n",
    "    elif \"Forest\" in name:\n",
    "        flops = rf_params * 10  # traversals aproximados\n",
    "    elif \"SVM\" in name:\n",
    "        flops = 150 * 10 * 2 + 784 * 150  # PCA transform + SVM\n",
    "    else:\n",
    "        flops = 0\n",
    "    \n",
    "    # Calcular FIC con parámetros configurados\n",
    "    fic_result = fic_calculator.calculate_fic(\n",
    "        log_likelihood=log_lik,\n",
    "        flops=flops,\n",
    "        n_params=n_params,\n",
    "        n_samples=len(y_test)\n",
    "    )\n",
    "    \n",
    "    # Mostrar desglose del FIC\n",
    "    print(f\"  Log-likelihood: {log_lik:,.0f}\")\n",
    "    print(f\"  Params: {n_params:,}\")\n",
    "    print(f\"  FLOPs: {flops:,}\")\n",
    "    print(f\"  AIC: {aic:,.0f}\")\n",
    "    print(f\"  BIC: {bic:,.0f}\")\n",
    "    print(f\"  FIC: {fic_result['fic']:,.0f}\")\n",
    "    print(f\"    └─ FLOPs term: {fic_result.get('flops_term', 0):,.0f}\")\n",
    "    print(f\"    └─ Params term: {fic_result.get('params_term', 0):,.0f}\")\n",
    "    \n",
    "    all_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'params': n_params,\n",
    "        'flops': flops,\n",
    "        'train_time': train_time,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'fic': fic_result['fic'],\n",
    "        'log_likelihood': log_lik,\n",
    "        'flops_term': fic_result.get('flops_term', 0),\n",
    "        'params_term': fic_result.get('params_term', 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158313d",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{5.3. PyTorch eval}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b06801b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PyTorch] MLP Tiny...\n",
      "  Log-likelihood: 925\n",
      "  Params: 50,890\n",
      "  FLOPs: 1,237,568\n",
      "  AIC: 102,705\n",
      "  BIC: 469,640\n",
      "  FIC: 3,442\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[PyTorch] MLP Medium...\n",
      "  Log-likelihood: 609\n",
      "  Params: 235,146\n",
      "  FLOPs: 1,393,536\n",
      "  AIC: 470,901\n",
      "  BIC: 2,166,384\n",
      "  FIC: 3,278\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[PyTorch] MLP Large...\n",
      "  Log-likelihood: 702\n",
      "  Params: 798,474\n",
      "  FLOPs: 2,410,240\n",
      "  AIC: 1,597,650\n",
      "  BIC: 7,354,920\n",
      "  FIC: 4,341\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Tiny...\n",
      "  Log-likelihood: 644\n",
      "  Params: 31,530\n",
      "  FLOPs: 19,894,784\n",
      "  AIC: 63,704\n",
      "  BIC: 291,046\n",
      "  FIC: 20,229\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Medium...\n",
      "  Log-likelihood: 306\n",
      "  Params: 421,834\n",
      "  FLOPs: 27,378,816\n",
      "  AIC: 843,974\n",
      "  BIC: 3,885,541\n",
      "  FIC: 26,660\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n",
      "\n",
      "[PyTorch] CNN Deep...\n",
      "  Log-likelihood: 319\n",
      "  Params: 276,554\n",
      "  FLOPs: 17,720,576\n",
      "  AIC: 553,427\n",
      "  BIC: 2,547,476\n",
      "  FIC: 17,937\n",
      "    └─ FLOPs term: 0\n",
      "    └─ Params term: 0\n"
     ]
    }
   ],
   "source": [
    "pytorch_models = [\n",
    "    (\"MLP Tiny\", mlp_tiny_model, mlp_tiny_params, mlp_tiny_accuracy, mlp_tiny_train_time),\n",
    "    (\"MLP Medium\", mlp_medium_model, mlp_medium_params, mlp_medium_accuracy, mlp_medium_train_time),\n",
    "    (\"MLP Large\", mlp_large_model, mlp_large_params, mlp_large_accuracy, mlp_large_train_time),\n",
    "    (\"CNN Tiny\", cnn_tiny_model, cnn_tiny_params, cnn_tiny_accuracy, cnn_tiny_train_time),\n",
    "    (\"CNN Medium\", cnn_medium_model, cnn_medium_params, cnn_medium_accuracy, cnn_medium_train_time),\n",
    "    (\"CNN Deep\", cnn_deep_model, cnn_deep_params, cnn_deep_accuracy, cnn_deep_train_time),\n",
    "]\n",
    "\n",
    "for name, model, n_params, accuracy, train_time in pytorch_models:\n",
    "    print(f\"\\n[PyTorch] {name}...\")\n",
    "    \n",
    "    # Calcular log-likelihood\n",
    "    log_lik = calculate_log_likelihood_pytorch(model, test_loader, DEVICE)\n",
    "    \n",
    "    # Calcular AIC y BIC\n",
    "    aic = calculate_aic(log_lik, n_params)\n",
    "    bic = calculate_bic(log_lik, n_params, len(y_test))\n",
    "    \n",
    "    # Contar FLOPs reales\n",
    "    sample_input = X_test_torch[:1].to(DEVICE)\n",
    "    try:\n",
    "        flops_result = count_model_flops(\n",
    "            model=model,\n",
    "            input_data=sample_input,\n",
    "            framework='torch',\n",
    "            verbose=False\n",
    "        )\n",
    "        flops = flops_result['total_flops']\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Warning: No se pudieron contar FLOPs: {e}\")\n",
    "        flops = 0\n",
    "    \n",
    "    # Calcular FIC con parámetros configurados\n",
    "    fic_result = fic_calculator.calculate_fic(\n",
    "        log_likelihood=log_lik,\n",
    "        flops=flops,\n",
    "        n_params=n_params,\n",
    "        n_samples=len(y_test)\n",
    "    )\n",
    "    \n",
    "    # Mostrar desglose del FIC\n",
    "    print(f\"  Log-likelihood: {log_lik:,.0f}\")\n",
    "    print(f\"  Params: {n_params:,}\")\n",
    "    print(f\"  FLOPs: {flops:,}\")\n",
    "    print(f\"  AIC: {aic:,.0f}\")\n",
    "    print(f\"  BIC: {bic:,.0f}\")\n",
    "    print(f\"  FIC: {fic_result['fic']:,.0f}\")\n",
    "    print(f\"    └─ FLOPs term: {fic_result.get('flops_term', 0):,.0f}\")\n",
    "    print(f\"    └─ Params term: {fic_result.get('params_term', 0):,.0f}\")\n",
    "    \n",
    "    all_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'params': n_params,\n",
    "        'flops': flops,\n",
    "        'train_time': train_time,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'fic': fic_result['fic'],\n",
    "        'log_likelihood': log_lik,\n",
    "        'flops_term': fic_result.get('flops_term', 0),\n",
    "        'params_term': fic_result.get('params_term', 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf08d3",
   "metadata": {},
   "source": [
    "## $\\color{#dda}{\\text{6. Resultados}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "764f646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLA COMPARATIVA COMPLETA: 10 ENFOQUES\n",
      "====================================================================================================\n",
      "\n",
      "Modelo                    Acc%     Params       FLOPs           Time(s)    AIC          BIC          FIC         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLP Medium                98.28    235,146      1,393,536       73.1       470901       2166384      3278        \n",
      "MLP Tiny                  97.34    50,890       1,237,568       64.7       102705       469640       3442        \n",
      "MLP Large                 97.93    798,474      2,410,240       128.5      1597650      7354920      4341        \n",
      "Logistic Regression       92.50    7,850        15,680          14.2       26090        82691        11370       \n",
      "PCA+SVM                   91.55    1,510        120,600         21.3       14700        25588        12959       \n",
      "Random Forest             96.80    958,830      9,588,300       28.1       1922208      8835699      14785       \n",
      "CNN Deep                  99.18    276,554      17,720,576      1066.6     553427       2547476      17937       \n",
      "CNN Tiny                  97.98    31,530       19,894,784      155.8      63704        291046       20229       \n",
      "CNN Medium                99.19    421,834      27,378,816      693.7      843974       3885541      26660       \n",
      "KNN (k=5)                 94.42    100          39,200,000      0.1        7981         8702         44810       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLA COMPARATIVA COMPLETA: 10 ENFOQUES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n{'Modelo':<25} {'Acc%':<8} {'Params':<12} {'FLOPs':<15} {'Time(s)':<10} {'AIC':<12} {'BIC':<12} {'FIC':<12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for name in sorted(all_results.keys(), key=lambda x: all_results[x]['fic']):\n",
    "    r = all_results[name]\n",
    "    print(f\"{name:<25} {r['accuracy']:<8.2f} {r['params']:<12,} {r['flops']:<15,} \"\n",
    "          f\"{r['train_time']:<10.1f} {r['aic']:<12.0f} {r['bic']:<12.0f} {r['fic']:<12.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8358ae",
   "metadata": {},
   "source": [
    "\n",
    "## $\\color{#dda}{\\text{7. Conclusiones}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba24f7",
   "metadata": {},
   "source": [
    "La variable que dice wonder es la potencia de 10 que aumentará a alpha lo suficiente para dar prioridad a los FLOPs pero sin opacar la verosimilitud. Con estos parámetros se priorizan los FLOPs pero sin dejar de lado la eficiencia del modelo y su log-likelihood. Para este caso de demostración, digamos que dejamos la beta en cero y la lambda en 0.1 porque priorizamos el balance entre FLOPs y verosimilitud.\n",
    "\n",
    "```python\n",
    "wonder_flop_priority = 3\n",
    "\n",
    "FIC_VARIANT = 'custom' \n",
    "FIC_ALPHA = 1000\n",
    "FIC_BETA = 0.0\n",
    "FIC_LAMBDA = 0.1 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
