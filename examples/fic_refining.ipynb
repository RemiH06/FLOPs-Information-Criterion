{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec9b878",
   "metadata": {},
   "source": [
    "FIC Parameter Optimization\n",
    "==========================\n",
    "Experimentación sistemática para encontrar los valores óptimos de:\n",
    "- alpha: peso de penalización de FLOPs\n",
    "- beta: peso de penalización de parámetros  \n",
    "- lambda: balance entre término logarítmico y lineal\n",
    "\n",
    "Este notebook ejecuta 3 experimentos con diferentes tipos de modelos\n",
    "para determinar empíricamente la mejor configuración del FIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc90180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flop_counter.flop_information_criterion import FlopInformationCriterion\n",
    "\n",
    "# Configuración de reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69757f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# CONFIGURACIÓN DE EXPERIMENTOS\n",
    "# ===========================================================================\n",
    "\n",
    "# Rango de valores a explorar para lambda (balance log vs linear)\n",
    "# lambda=0.0: 100% lineal (diferencias absolutas en FLOPs)\n",
    "# lambda=1.0: 100% logarítmico (diferencias relativas en FLOPs)\n",
    "LAMBDA_VALUES = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "\n",
    "# Valores de alpha a probar (penalización de FLOPs)\n",
    "# Valores más altos penalizan más fuertemente los FLOPs\n",
    "ALPHA_VALUES = [0.5, 1.0, 2.0, 5.0, 7.0, 10.0]\n",
    "\n",
    "# Valores de beta a probar (penalización de parámetros)\n",
    "# Valores más altos penalizan más fuertemente el número de parámetros\n",
    "BETA_VALUES = [0.0, 0.3, 0.5, 1.0, 2.0]\n",
    "\n",
    "print(f\"N valores de lambda: {len(LAMBDA_VALUES)}\")\n",
    "print(f\"N valores de alpha: {len(ALPHA_VALUES)}\")\n",
    "print(f\"N valores de beta: {len(BETA_VALUES)}\")\n",
    "print(f\"Total configuraciones por experimento: {len(LAMBDA_VALUES) * len(ALPHA_VALUES) * len(BETA_VALUES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ===========================================================================\n",
    "\n",
    "def calculate_aic(log_likelihood: float, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el criterio AIC (Akaike Information Criterion).\n",
    "    AIC = -2*log(L) + 2*k\n",
    "    \"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "\n",
    "def calculate_bic(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el criterio BIC (Bayesian Information Criterion).\n",
    "    BIC = -2*log(L) + k*log(n)\n",
    "    \"\"\"\n",
    "    return log_likelihood + k * np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(results_dict):\n",
    "    \"\"\"\n",
    "    Convierte el diccionario de resultados a un DataFrame de pandas\n",
    "    para facilitar el análisis.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Diccionario con estructura {(lambda, alpha, beta, model): result}\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con todas las métricas organizadas\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for key, result in results_dict.items():\n",
    "        lambda_val, alpha, beta, model = key\n",
    "        \n",
    "        data.append({\n",
    "            'Lambda': lambda_val,\n",
    "            'Alpha': alpha,\n",
    "            'Beta': beta,\n",
    "            'Model': model,\n",
    "            'FIC': result['fic'],\n",
    "            'LogLik': result['log_likelihood_term'],\n",
    "            'FLOPs_Penalty': result['flops_penalty'],\n",
    "            'Params_Penalty': result['params_penalty'],\n",
    "            'FLOPs': result['flops'],\n",
    "            'Params': result['n_params'],\n",
    "            'Accuracy': result.get('accuracy', 0)\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def analyze_sensitivity(df, experiment_name):\n",
    "    \"\"\"\n",
    "    Analiza cómo cada combinación de parámetros afecta la capacidad\n",
    "    del FIC para distinguir entre modelos.\n",
    "    \n",
    "    La sensibilidad se mide como la diferencia promedio en FIC entre\n",
    "    el modelo de referencia y los demás modelos.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con resultados del experimento\n",
    "        experiment_name: Nombre del experimento para el reporte\n",
    "    \n",
    "    Returns:\n",
    "        Lista de estadísticas por cada valor de lambda\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"ANÁLISIS DE SENSIBILIDAD: {experiment_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Determinar modelo de referencia (el primero alfabéticamente)\n",
    "    all_models = df['Model'].unique()\n",
    "    reference_model = sorted(all_models)[0]\n",
    "    \n",
    "    print(f\"\\nModelo de referencia: {reference_model}\")\n",
    "    print(f\"\\n{'Lambda':<10} {'Alpha':<10} {'Beta':<10} {'Avg DELTA_FIC':<20} {'Interpretation':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    best_configs = []\n",
    "    \n",
    "    # Analizar cada combinación de lambda, alpha, beta\n",
    "    for lambda_val in sorted(df['Lambda'].unique()):\n",
    "        for alpha in sorted(df['Alpha'].unique()):\n",
    "            for beta in sorted(df['Beta'].unique()):\n",
    "                # Filtrar configuración específica\n",
    "                config_df = df[\n",
    "                    (df['Lambda'] == lambda_val) & \n",
    "                    (df['Alpha'] == alpha) & \n",
    "                    (df['Beta'] == beta)\n",
    "                ]\n",
    "                \n",
    "                if len(config_df) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Calcular diferencia promedio respecto al modelo de referencia\n",
    "                ref_fic = config_df[config_df['Model'] == reference_model]['FIC'].values\n",
    "                other_fics = config_df[config_df['Model'] != reference_model]['FIC'].values\n",
    "                \n",
    "                if len(ref_fic) > 0 and len(other_fics) > 0:\n",
    "                    avg_delta = np.mean(np.abs(other_fics - ref_fic[0]))\n",
    "                    \n",
    "                    # Interpretación de la sensibilidad\n",
    "                    if avg_delta < 1:\n",
    "                        interp = \"Muy baja - no distingue\"\n",
    "                    elif avg_delta < 10:\n",
    "                        interp = \"Baja\"\n",
    "                    elif avg_delta < 50:\n",
    "                        interp = \"Media\"\n",
    "                    elif avg_delta < 100:\n",
    "                        interp = \"Alta\"\n",
    "                    else:\n",
    "                        interp = \"Muy alta - buena distinción\"\n",
    "                    \n",
    "                    best_configs.append({\n",
    "                        'lambda': lambda_val,\n",
    "                        'alpha': alpha,\n",
    "                        'beta': beta,\n",
    "                        'avg_delta': avg_delta,\n",
    "                        'interpretation': interp\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"{lambda_val:<10.1f} {alpha:<10.1f} {beta:<10.1f} {avg_delta:<20.2f} {interp:<30}\")\n",
    "    \n",
    "    # Encontrar mejor configuración\n",
    "    if best_configs:\n",
    "        best = max(best_configs, key=lambda x: x['avg_delta'])\n",
    "        print(f\"\\n--- MEJOR CONFIGURACIÓN PARA ESTE EXPERIMENTO ---\")\n",
    "        print(f\"Lambda = {best['lambda']:.1f}, Alpha = {best['alpha']:.1f}, Beta = {best['beta']:.1f}\")\n",
    "        print(f\"Sensibilidad promedio: {best['avg_delta']:.2f}\")\n",
    "        print(f\"Interpretación: {best['interpretation']}\")\n",
    "    \n",
    "    return best_configs\n",
    "\n",
    "\n",
    "def print_experiment_header(title, models_info):\n",
    "    \"\"\"\n",
    "    Imprime el encabezado informativo de cada experimento.\n",
    "    \n",
    "    Args:\n",
    "        title: Título del experimento\n",
    "        models_info: Dict con información de cada modelo\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(\"\\nModelos a comparar:\")\n",
    "    for model_name, info in models_info.items():\n",
    "        print(f\"   {model_name}: {info['params']} params, {info['expected_flops']} FLOPs esperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# EXPERIMENTO 1: REGRESIÓN LINEAL\n",
    "# ===========================================================================\n",
    "# Objetivo: Detectar modelos con igual número de parámetros pero diferentes FLOPs\n",
    "# Caso: Modelo simple vs modelo con operaciones innecesarias\n",
    "\n",
    "print_experiment_header(\n",
    "    \"EXPERIMENTO 1: REGRESIÓN LINEAL - EFICIENCIA COMPUTACIONAL\",\n",
    "    {\n",
    "        'Simple': {'params': 2, 'expected_flops': '~500'},\n",
    "        'Ineficiente': {'params': 2, 'expected_flops': '~5000 (10x más)'}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generar datos de regresión lineal\n",
    "n_samples = 100\n",
    "X_train = np.random.randn(n_samples, 1)\n",
    "true_slope = 2.5\n",
    "true_intercept = 1.0\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y_train = true_slope * X_train.squeeze() + true_intercept + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f304a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_simple(X):\n",
    "    \"\"\"Modelo lineal eficiente: y = ax + b\"\"\"\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    return X_extended @ W\n",
    "\n",
    "\n",
    "def linear_model_inefficient(X):\n",
    "    \"\"\"\n",
    "    Modelo lineal con operaciones costosas innecesarias.\n",
    "    Produce el mismo resultado que el modelo simple pero con ~10x más FLOPs.\n",
    "    \"\"\"\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    temp = X_extended @ W\n",
    "    \n",
    "    # Operaciones costosas que no aportan valor\n",
    "    for _ in range(5):\n",
    "        temp = np.exp(np.log(np.abs(temp) + 1e-10)) * np.sign(temp)\n",
    "        temp = temp @ np.eye(1)\n",
    "        temp = np.sin(np.arcsin(np.clip(\n",
    "            temp / (np.max(np.abs(temp)) + 1e-10), -0.99, 0.99\n",
    "        ))) * (np.max(np.abs(temp)) + 1e-10)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9007372",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp1 = {}\n",
    "\n",
    "for lambda_val in LAMBDA_VALUES:\n",
    "    print(f\"   Procesando lambda = {lambda_val:.1f}\")\n",
    "    \n",
    "    for alpha in ALPHA_VALUES:\n",
    "        for beta in BETA_VALUES:\n",
    "            # Crear calculador de FIC con esta configuración\n",
    "            fic_calc = FlopInformationCriterion(\n",
    "                variant='custom',\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                lambda_balance=lambda_val\n",
    "            )\n",
    "            \n",
    "            # Evaluar ambos modelos\n",
    "            try:\n",
    "                result_simple = fic_calc.evaluate_model(\n",
    "                    model=linear_model_simple,\n",
    "                    X=X_train,\n",
    "                    y_true=y_train,\n",
    "                    task='regression',\n",
    "                    n_params=2,\n",
    "                    framework='numpy'\n",
    "                )\n",
    "                \n",
    "                result_inefficient = fic_calc.evaluate_model(\n",
    "                    model=linear_model_inefficient,\n",
    "                    X=X_train,\n",
    "                    y_true=y_train,\n",
    "                    task='regression',\n",
    "                    n_params=2,\n",
    "                    framework='numpy'\n",
    "                )\n",
    "                \n",
    "                results_exp1[(lambda_val, alpha, beta, 'Simple')] = result_simple\n",
    "                results_exp1[(lambda_val, alpha, beta, 'Ineficiente')] = result_inefficient\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      Error en lambda={lambda_val}, alpha={alpha}, beta={beta}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar resultados\n",
    "df_exp1 = create_results_dataframe(results_exp1)\n",
    "configs_exp1 = analyze_sensitivity(df_exp1, \"Experimento 1: Regresión Lineal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# EXPERIMENTO 2: REDES NEURONALES\n",
    "# ===========================================================================\n",
    "# Objetivo: Comparar arquitecturas con diferente profundidad y ancho\n",
    "# Caso: Wide vs Deep vs Balanced\n",
    "\n",
    "print_experiment_header(\n",
    "    \"EXPERIMENTO 2: REDES NEURONALES - ARQUITECTURAS\",\n",
    "    {\n",
    "        'Wide-Shallow': {'params': 2303, 'expected_flops': 'bajo'},\n",
    "        'Deep-Narrow': {'params': 2523, 'expected_flops': 'alto'},\n",
    "        'Balanced': {'params': 2353, 'expected_flops': 'medio'}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generar datos de clasificación\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "X_train = np.random.randn(n_samples, n_features)\n",
    "y_train = np.random.randint(0, n_classes, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ca146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Función softmax estable numéricamente\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def wide_shallow_net(X):\n",
    "    \"\"\"Red ancha y poco profunda: 20 -> 100 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 100) * 0.01\n",
    "    b1 = np.zeros(100)\n",
    "    W2 = np.random.randn(100, 3) * 0.01\n",
    "    b2 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    logits = h1 @ W2 + b2\n",
    "    return softmax(logits)\n",
    "\n",
    "\n",
    "def deep_narrow_net(X):\n",
    "    \"\"\"Red profunda y estrecha: 20 -> 30 -> 30 -> 30 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 30) * 0.01\n",
    "    b1 = np.zeros(30)\n",
    "    W2 = np.random.randn(30, 30) * 0.01\n",
    "    b2 = np.zeros(30)\n",
    "    W3 = np.random.randn(30, 30) * 0.01\n",
    "    b3 = np.zeros(30)\n",
    "    W4 = np.random.randn(30, 3) * 0.01\n",
    "    b4 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    h3 = np.maximum(0, h2 @ W3 + b3)\n",
    "    logits = h3 @ W4 + b4\n",
    "    return softmax(logits)\n",
    "\n",
    "\n",
    "def balanced_net(X):\n",
    "    \"\"\"Red balanceada: 20 -> 50 -> 25 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 50) * 0.01\n",
    "    b1 = np.zeros(50)\n",
    "    W2 = np.random.randn(50, 25) * 0.01\n",
    "    b2 = np.zeros(25)\n",
    "    W3 = np.random.randn(25, 3) * 0.01\n",
    "    b3 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    logits = h2 @ W3 + b3\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular número de parámetros\n",
    "n_params_wide = (20*100 + 100) + (100*3 + 3)\n",
    "n_params_deep = (20*30 + 30) + (30*30 + 30) + (30*30 + 30) + (30*3 + 3)\n",
    "n_params_balanced = (20*50 + 50) + (50*25 + 25) + (25*3 + 3)\n",
    "\n",
    "results_exp2 = {}\n",
    "\n",
    "for lambda_val in LAMBDA_VALUES:\n",
    "    print(f\"   Procesando lambda = {lambda_val:.1f}\")\n",
    "    \n",
    "    for alpha in ALPHA_VALUES:\n",
    "        for beta in BETA_VALUES:\n",
    "            fic_calc = FlopInformationCriterion(\n",
    "                variant='custom',\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                lambda_balance=lambda_val\n",
    "            )\n",
    "            \n",
    "            for name, model, n_params in [\n",
    "                (\"Wide-Shallow\", wide_shallow_net, n_params_wide),\n",
    "                (\"Deep-Narrow\", deep_narrow_net, n_params_deep),\n",
    "                (\"Balanced\", balanced_net, n_params_balanced)\n",
    "            ]:\n",
    "                try:\n",
    "                    result = fic_calc.evaluate_model(\n",
    "                        model=model,\n",
    "                        X=X_train,\n",
    "                        y_true=y_train,\n",
    "                        task='classification',\n",
    "                        n_params=n_params,\n",
    "                        framework='numpy'\n",
    "                    )\n",
    "                    \n",
    "                    results_exp2[(lambda_val, alpha, beta, name)] = result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      Error en {name}: {e}\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar resultados\n",
    "df_exp2 = create_results_dataframe(results_exp2)\n",
    "configs_exp2 = analyze_sensitivity(df_exp2, \"Experimento 2: Redes Neuronales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14980c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# EXPERIMENTO 3: REGRESIÓN POLINOMIAL\n",
    "# ===========================================================================\n",
    "# Objetivo: Detectar overfitting y complejidad innecesaria\n",
    "# Caso: Polinomios de diferentes grados (el óptimo es grado 2)\n",
    "\n",
    "print_experiment_header(\n",
    "    \"EXPERIMENTO 3: REGRESIÓN POLINOMIAL - OVERFITTING\",\n",
    "    {\n",
    "        'Linear': {'params': 2, 'expected_flops': 'muy bajo'},\n",
    "        'Quadratic': {'params': 3, 'expected_flops': 'bajo (óptimo)'},\n",
    "        'Cubic': {'params': 4, 'expected_flops': 'medio'},\n",
    "        'Poly5': {'params': 6, 'expected_flops': 'alto'},\n",
    "        'Poly10': {'params': 11, 'expected_flops': 'muy alto'}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generar datos con relación cuadrática verdadera\n",
    "n_samples = 150\n",
    "X_poly = np.random.randn(n_samples, 1)\n",
    "y_poly = 2.0 * X_poly.squeeze()**2 + 1.5 * X_poly.squeeze() + 1.0\n",
    "y_poly += np.random.randn(n_samples) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poly_model(degree):\n",
    "    \"\"\"\n",
    "    Crea un modelo polinomial de grado especificado.\n",
    "    \n",
    "    Args:\n",
    "        degree: Grado del polinomio\n",
    "    \n",
    "    Returns:\n",
    "        Función que implementa el modelo polinomial\n",
    "    \"\"\"\n",
    "    def model(X_input):\n",
    "        # Crear features polinomiales\n",
    "        X_features = np.column_stack([X_input**i for i in range(degree + 1)])\n",
    "        \n",
    "        # Coeficientes con valores verdaderos para términos relevantes\n",
    "        coeffs = np.random.randn(degree + 1) * 0.1\n",
    "        coeffs[0] = 1.0      # intercept\n",
    "        if degree >= 1:\n",
    "            coeffs[1] = 1.5  # término lineal\n",
    "        if degree >= 2:\n",
    "            coeffs[2] = 2.0  # término cuadrático (correcto)\n",
    "        \n",
    "        return X_features @ coeffs\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp3 = {}\n",
    "\n",
    "for lambda_val in LAMBDA_VALUES:\n",
    "    print(f\"   Procesando lambda = {lambda_val:.1f}\")\n",
    "    \n",
    "    for alpha in ALPHA_VALUES:\n",
    "        for beta in BETA_VALUES:\n",
    "            fic_calc = FlopInformationCriterion(\n",
    "                variant='custom',\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                lambda_balance=lambda_val\n",
    "            )\n",
    "            \n",
    "            for degree, name in [(1, 'Linear'), (2, 'Quadratic'), (3, 'Cubic'),\n",
    "                                  (5, 'Poly5'), (10, 'Poly10')]:\n",
    "                try:\n",
    "                    model = create_poly_model(degree)\n",
    "                    \n",
    "                    result = fic_calc.evaluate_model(\n",
    "                        model=model,\n",
    "                        X=X_poly,\n",
    "                        y_true=y_poly,\n",
    "                        task='regression',\n",
    "                        n_params=degree + 1,\n",
    "                        framework='numpy'\n",
    "                    )\n",
    "                    \n",
    "                    results_exp3[(lambda_val, alpha, beta, name)] = result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      Error en {name}: {e}\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar resultados\n",
    "df_exp3 = create_results_dataframe(results_exp3)\n",
    "configs_exp3 = analyze_sensitivity(df_exp3, \"Experimento 3: Regresión Polinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e024a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ANÁLISIS GLOBAL Y RECOMENDACIONES\n",
    "# ===========================================================================\n",
    "\n",
    "print(f\"\\n\\n{'='*100}\")\n",
    "print(\"ANÁLISIS GLOBAL: MEJORES CONFIGURACIONES\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la mejor configuración en cada experimento\n",
    "if configs_exp1 and configs_exp2 and configs_exp3:\n",
    "    best_exp1 = max(configs_exp1, key=lambda x: x['avg_delta'])\n",
    "    best_exp2 = max(configs_exp2, key=lambda x: x['avg_delta'])\n",
    "    best_exp3 = max(configs_exp3, key=lambda x: x['avg_delta'])\n",
    "    \n",
    "    print(\"\\nMejor configuración por experimento:\")\n",
    "    print(f\"\\nExperimento 1 (Regresión):\")\n",
    "    print(f\"   Lambda = {best_exp1['lambda']:.1f}, Alpha = {best_exp1['alpha']:.1f}, Beta = {best_exp1['beta']:.1f}\")\n",
    "    print(f\"   Sensibilidad: {best_exp1['avg_delta']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nExperimento 2 (Redes Neuronales):\")\n",
    "    print(f\"   Lambda = {best_exp2['lambda']:.1f}, Alpha = {best_exp2['alpha']:.1f}, Beta = {best_exp2['beta']:.1f}\")\n",
    "    print(f\"   Sensibilidad: {best_exp2['avg_delta']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nExperimento 3 (Polinomios):\")\n",
    "    print(f\"   Lambda = {best_exp3['lambda']:.1f}, Alpha = {best_exp3['alpha']:.1f}, Beta = {best_exp3['beta']:.1f}\")\n",
    "    print(f\"   Sensibilidad: {best_exp3['avg_delta']:.2f}\")\n",
    "    \n",
    "    # Calcular configuración promedio recomendada\n",
    "    avg_lambda = np.mean([best_exp1['lambda'], best_exp2['lambda'], best_exp3['lambda']])\n",
    "    avg_alpha = np.mean([best_exp1['alpha'], best_exp2['alpha'], best_exp3['alpha']])\n",
    "    avg_beta = np.mean([best_exp1['beta'], best_exp2['beta'], best_exp3['beta']])\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"CONFIGURACIÓN RECOMENDADA (PROMEDIO)\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"\\nLambda = {avg_lambda:.2f}\")\n",
    "    print(f\"Alpha  = {avg_alpha:.2f}\")\n",
    "    print(f\"Beta   = {avg_beta:.2f}\")\n",
    "    \n",
    "    print(f\"\\nFórmula del FIC con estos valores:\")\n",
    "    print(f\"FIC = -2*log(L) + {avg_alpha:.1f} * [{avg_lambda:.1f}*log(FLOPs) + {1-avg_lambda:.1f}*FLOPs/1e6] + {avg_beta:.1f}*k\")\n",
    "    \n",
    "    print(f\"\\nInterpretación:\")\n",
    "    if avg_lambda < 0.3:\n",
    "        print(\"   Lambda bajo: El término lineal domina (sensible a diferencias absolutas)\")\n",
    "    elif avg_lambda > 0.7:\n",
    "        print(\"   Lambda alto: El término logarítmico domina (sensible a diferencias relativas)\")\n",
    "    else:\n",
    "        print(\"   Lambda balanceado: Combina sensibilidad relativa y absoluta\")\n",
    "    \n",
    "    if avg_alpha > 5:\n",
    "        print(\"   Alpha alto: Penalización fuerte de FLOPs (prioriza eficiencia)\")\n",
    "    else:\n",
    "        print(\"   Alpha moderado: Balance entre precisión y eficiencia\")\n",
    "    \n",
    "    if avg_beta < 0.5:\n",
    "        print(\"   Beta bajo: Poco peso a parámetros (FLOPs dominan)\")\n",
    "    elif avg_beta > 1:\n",
    "        print(\"   Beta alto: Peso significativo a parámetros\")\n",
    "    else:\n",
    "        print(\"   Beta moderado: Balance entre FLOPs y parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b56329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"OPTIMIZACIÓN COMPLETADA\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"\\nDataFrames disponibles para análisis adicional:\")\n",
    "print(f\"   - df_exp1: Experimento de regresión lineal\")\n",
    "print(f\"   - df_exp2: Experimento de redes neuronales\")\n",
    "print(f\"   - df_exp3: Experimento de regresión polinomial\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
