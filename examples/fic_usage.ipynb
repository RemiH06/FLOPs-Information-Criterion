{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e01a3",
   "metadata": {},
   "source": [
    "### $\\color{#ddd}{\\text{Ejemplos de uso del FIC (FLOPs Information Criterion)}}$\n",
    "### $\\color{#ddd}{\\text{Comparación con criterios tradicionales: AIC, BIC, y otros}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08527b",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{0. Libs import}}$\n",
    "\n",
    "Para la fecha en la que hago este jupyter no he hecho un paquete instalable universalmente así que se debe especificar el contexto de dónde está la librería de flop_counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0dd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import flop_counter\n",
    "from flop_counter.flop_information_criterion import FlopInformationCriterion\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc59e34",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{1. Declaración de funciones para criterios}}$\n",
    "\n",
    "Para comparación. Estos criterios son clásicos y se usan sobre todo en metolodogía Box-Jenkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033e9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aic(log_likelihood: float, k: int) -> float:\n",
    "    \"\"\"\n",
    "    AIC: Akaike Information Criterion\n",
    "    AIC = -2*log(L) + 2*k\n",
    "    Penaliza solo por número de parámetros\n",
    "    \"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "\n",
    "def calculate_bic(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    BIC: Bayesian Information Criterion  \n",
    "    BIC = -2*log(L) + k*log(n)\n",
    "    Penaliza más que AIC, favorece modelos más simples\n",
    "    \"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "\n",
    "def calculate_hqic(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    HQIC: Hannan-Quinn Information Criterion\n",
    "    HQIC = -2*log(L) + 2*k*log(log(n))\n",
    "    Penalización intermedia entre AIC y BIC\n",
    "    \"\"\"\n",
    "    return log_likelihood + 2 * k * np.log(np.log(n))\n",
    "\n",
    "\n",
    "def calculate_mdl(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    MDL: Minimum Description Length\n",
    "    MDL = -log(L) + (k/2)*log(n)\n",
    "    Similar a BIC pero con constantes diferentes\n",
    "    \"\"\"\n",
    "    return log_likelihood / 2 + (k / 2) * np.log(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400a61d",
   "metadata": {},
   "source": [
    "Las dos funciones siguientes son únicamente de comparación y para mostrar los resultados de manera más estética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3607a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_criteria_comparison(name: str, results: dict, show_all: bool = False):\n",
    "    \"\"\"\n",
    "    Imprime comparación detallada de criterios para un modelo.\n",
    "    Muestra información del modelo y valores de AIC, BIC, FIC.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODELO: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Información básica del modelo\n",
    "    print(f\"\\nInformación del Modelo:\")\n",
    "    print(f\"  Parámetros:       {results.get('n_params', 'N/A'):,}\")\n",
    "    print(f\"  FLOPs:            {results.get('flops', 'N/A'):,}\")\n",
    "    print(f\"  Muestras:         {results.get('n_samples', 'N/A')}\")\n",
    "    print(f\"  Log-Likelihood:   {results.get('log_likelihood_term', 'N/A'):.2f}\")\n",
    "    \n",
    "    if 'accuracy' in results:\n",
    "        acc_metric = 'R²' if results.get('accuracy', 0) <= 1 else 'Accuracy'\n",
    "        print(f\"  {acc_metric}:            {results['accuracy']:.4f}\")\n",
    "    \n",
    "    # Valores de criterios de información\n",
    "    print(f\"\\nCriterios de Información:\")\n",
    "    print(f\"  {'Criterio':<15} {'Valor':<15} {'Penalización':<20}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    \n",
    "    if 'aic' in results:\n",
    "        print(f\"  {'AIC':<15} {results['aic']:<15.2f} {'2k':<20}\")\n",
    "    \n",
    "    if 'bic' in results:\n",
    "        print(f\"  {'BIC':<15} {results['bic']:<15.2f} {'k*log(n)':<20}\")\n",
    "    \n",
    "    # FIC destacado\n",
    "    lambda_val = results.get('lambda', 'N/A')\n",
    "    print(f\"  {'FIC':<15} {results['fic']:<15.2f} {'α*f(FLOPs) + β*k':<20} <- Incluye FLOPs\")\n",
    "    \n",
    "    # Criterios adicionales opcionales\n",
    "    if show_all:\n",
    "        if 'hqic' in results:\n",
    "            print(f\"  {'HQIC':<15} {results['hqic']:<15.2f} {'2k*log(log(n))':<20}\")\n",
    "        \n",
    "        if 'mdl' in results:\n",
    "            print(f\"  {'MDL':<15} {results['mdl']:<15.2f} {'(k/2)*log(n)':<20}\")\n",
    "    \n",
    "    # Desglose detallado del FIC\n",
    "    print(f\"\\nDesglose del FIC:\")\n",
    "    print(f\"  Ajuste (likelihood):       {results['log_likelihood_term']:.2f}\")\n",
    "    print(f\"  Penalización FLOPs:        {results['flops_penalty']:.2f}\")\n",
    "    print(f\"  Penalización parámetros:   {results['params_penalty']:.2f}\")\n",
    "    print(f\"  Coeficientes: α={results['alpha']:.2f}, β={results['beta']:.2f}, λ={lambda_val}\")\n",
    "\n",
    "\n",
    "def compare_all_criteria(models_results: dict, show_all: bool = False):\n",
    "    \"\"\"\n",
    "    Compara todos los modelos lado a lado según diferentes criterios.\n",
    "    Identifica el mejor modelo según cada criterio y calcula diferencias ΔFIC.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARACIÓN DE MODELOS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    model_names = list(models_results.keys())\n",
    "    criteria = ['AIC', 'BIC', 'FIC']\n",
    "    \n",
    "    if show_all:\n",
    "        criteria.extend(['HQIC', 'MDL'])\n",
    "    \n",
    "    # Tabla comparativa\n",
    "    print(f\"\\n{'Modelo':<20}\", end=\"\")\n",
    "    for criterion in criteria:\n",
    "        print(f\"{criterion:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (20 + 12 * len(criteria)))\n",
    "    \n",
    "    for name in model_names:\n",
    "        res = models_results[name]\n",
    "        print(f\"{name:<20}\", end=\"\")\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            key = criterion.lower()\n",
    "            value = res.get(key, float('nan'))\n",
    "            print(f\"{value:>12.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Identificar mejor modelo según cada criterio\n",
    "    print(f\"\\n{'Criterio':<15} {'Mejor Modelo':<20} {'Valor':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        key = criterion.lower()\n",
    "        best_name = min(model_names, key=lambda x: models_results[x].get(key, float('inf')))\n",
    "        best_value = models_results[best_name][key]\n",
    "        print(f\"{criterion:<15} {best_name:<20} {best_value:<15.2f}\")\n",
    "    \n",
    "    # Análisis de diferencias ΔFIC\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ΔFIC: Diferencias respecto al mejor modelo según FIC\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    fic_values = {name: res['fic'] for name, res in models_results.items()}\n",
    "    best_fic = min(fic_values.values())\n",
    "    \n",
    "    print(f\"\\n{'Modelo':<20} {'FIC':<15} {'ΔFIC':<15} {'Interpretación':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name in sorted(model_names, key=lambda x: fic_values[x]):\n",
    "        fic = fic_values[name]\n",
    "        delta_fic = fic - best_fic\n",
    "        \n",
    "        # Interpretación de la magnitud de diferencia\n",
    "        if delta_fic < 2:\n",
    "            interpretation = \"Equivalente al mejor\"\n",
    "        elif delta_fic < 10:\n",
    "            interpretation = \"Evidencia sustancial contra\"\n",
    "        else:\n",
    "            interpretation = \"Evidencia fuerte contra\"\n",
    "        \n",
    "        marker = \"* MEJOR\" if delta_fic < 0.01 else \"\"\n",
    "        \n",
    "        print(f\"{name:<20} {fic:<15.2f} {delta_fic:<15.2f} {interpretation:<30} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14722f6d",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{2. Ejemplos}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bb64e",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 1: Regresión Lineal Simple vs Compleja}}$\n",
    "\n",
    "##### Demostración: Dos modelos con igual número de parámetros pero diferentes FLOPs\n",
    "##### Objetivo: Mostrar que AIC/BIC son ciegos a eficiencia computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290e037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos de regresión lineal\n",
    "n_samples = 100\n",
    "X_train = np.random.randn(n_samples, 1)\n",
    "true_slope = 2.5\n",
    "true_intercept = 1.0\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y_train = true_slope * X_train.squeeze() + true_intercept + noise\n",
    "\n",
    "def linear_model_simple(X):\n",
    "    \"\"\"\n",
    "    Modelo lineal eficiente: y = ax + b\n",
    "    Implementación directa sin operaciones innecesarias\n",
    "    \"\"\"\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    return X_extended @ W\n",
    "\n",
    "def linear_model_complex(X):\n",
    "    \"\"\"\n",
    "    Modelo lineal ineficiente: y = ax + b\n",
    "    Mismo resultado pero con operaciones costosas innecesarias\n",
    "    Simula código mal optimizado o arquitectura redundante\n",
    "    \"\"\"\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    \n",
    "    # Operaciones adicionales que no aportan valor\n",
    "    temp = X_extended @ W\n",
    "    temp = np.exp(np.log(temp))  # Identidad costosa: exp(log(x)) = x\n",
    "    temp = temp @ np.eye(1)      # Multiplicación innecesaria por identidad\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccbb9d",
   "metadata": {},
   "source": [
    "##### Crear calculador de FIC y evaluar ambos modelos\n",
    "##### Configuración: α=5.0 (penaliza FLOPs), β=0.5 (menos peso a params), λ=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6602dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion\\flop_counter\\flop_information_criterion.py:199: UserWarning: FLOPs = 0, penalización por FLOPs es 0\n",
      "  warnings.warn(\"FLOPs = 0, penalización por FLOPs es 0\", UserWarning)\n",
      "C:\\Users\\hecto\\AppData\\Local\\Temp\\ipykernel_15812\\1260066972.py:29: RuntimeWarning: invalid value encountered in log\n",
      "  temp = np.exp(np.log(temp))  # Identidad costosa: exp(log(x)) = x\n"
     ]
    }
   ],
   "source": [
    "# Inicializar calculador con configuración balanceada\n",
    "fic_calc = FlopInformationCriterion(\n",
    "    variant='custom',\n",
    "    alpha=5.0,      # Penalización fuerte de FLOPs\n",
    "    beta=0.5,       # Menos peso a parámetros\n",
    "    lambda_balance=0.3  # 30% log, 70% lineal\n",
    ")\n",
    "\n",
    "result_simple = fic_calc.evaluate_model(\n",
    "    model=linear_model_simple,\n",
    "    X=X_train,\n",
    "    y_true=y_train,\n",
    "    task='regression',\n",
    "    n_params=2,\n",
    "    framework='numpy'\n",
    ")\n",
    "\n",
    "result_complex = fic_calc.evaluate_model(\n",
    "    model=linear_model_complex,\n",
    "    X=X_train,\n",
    "    y_true=y_train,\n",
    "    task='regression',\n",
    "    n_params=2,\n",
    "    framework='numpy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d877dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO: Modelo Simple\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2\n",
      "  FLOPs:            0\n",
      "  Muestras:         100\n",
      "  Log-Likelihood:   975.95\n",
      "  R²:            0.0000\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             979.95          2k                  \n",
      "  BIC             985.16          k*log(n)            \n",
      "  FIC             976.95          α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "  HQIC            982.06          2k*log(log(n))      \n",
      "  MDL             492.58          (k/2)*log(n)        \n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       975.95\n",
      "  Penalización FLOPs:        0.00\n",
      "  Penalización parámetros:   1.00\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "================================================================================\n",
      "MODELO: Modelo Complejo\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2\n",
      "  FLOPs:            0\n",
      "  Muestras:         100\n",
      "  Log-Likelihood:   nan\n",
      "  R²:            0.0000\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             nan             2k                  \n",
      "  BIC             nan             k*log(n)            \n",
      "  FIC             nan             α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "  HQIC            nan             2k*log(log(n))      \n",
      "  MDL             nan             (k/2)*log(n)        \n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       nan\n",
      "  Penalización FLOPs:        0.00\n",
      "  Penalización parámetros:   1.00\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC        HQIC         MDL\n",
      "--------------------------------------------------------------------------------\n",
      "Simple                    979.95      985.16      976.95      982.06      492.58\n",
      "Complejo                     nan         nan         nan         nan         nan\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             Simple               979.95         \n",
      "BIC             Simple               985.16         \n",
      "FIC             Simple               976.95         \n",
      "HQIC            Simple               982.06         \n",
      "MDL             Simple               492.58         \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "Simple               976.95          0.00            Equivalente al mejor           * MEJOR\n",
      "Complejo             nan             nan             Evidencia fuerte contra        \n"
     ]
    }
   ],
   "source": [
    "# Calcular criterios tradicionales para ambos modelos\n",
    "for name, result in [(\"Simple\", result_simple), (\"Complejo\", result_complex)]:\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], result['n_params'])\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "    result['hqic'] = calculate_hqic(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "    result['mdl'] = calculate_mdl(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "\n",
    "# Mostrar resultados individuales\n",
    "print_criteria_comparison(\"Modelo Simple\", result_simple, show_all=True)\n",
    "print_criteria_comparison(\"Modelo Complejo\", result_complex, show_all=True)\n",
    "\n",
    "# Comparación lado a lado\n",
    "models_linear = {\n",
    "    'Simple': result_simple,\n",
    "    'Complejo': result_complex\n",
    "}\n",
    "compare_all_criteria(models_linear, show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a25687",
   "metadata": {},
   "source": [
    "### __OBSERVACIONES CLAVE__\n",
    "> **AIC y BIC:** Idénticos (mismo número de parámetros, mismo ajuste)  \n",
    "> **FIC:** Detecta y penaliza el modelo complejo por sus FLOPs innecesarios  \n",
    "> **Conclusión:** Solo **FIC** captura la ineficiencia computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef4c36",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 2: Redes Neuronales de Diferentes Arquitecturas}}$\n",
    "\n",
    "##### Demostración: Comparar arquitecturas Wide vs Deep vs Balanced\n",
    "##### Objetivo: Mostrar que profundidad y ancho afectan FLOPs diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22415873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos de clasificación\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "\n",
    "X_train = np.random.randn(n_samples, n_features)\n",
    "y_train = np.random.randint(0, n_classes, n_samples)\n",
    "y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax estable numéricamente para evitar overflow\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f45bd7",
   "metadata": {},
   "source": [
    "Tres diseños diferentes: ancha-poco profunda, profunda-estrecha, balanceada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb455d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_shallow_net(X):\n",
    "    \"\"\"\n",
    "    Red ancha y poco profunda: 20 -> 100 -> 3\n",
    "    Características:\n",
    "      - Pocas capas (2)\n",
    "      - Muchas neuronas por capa (100 hidden)\n",
    "      - ~2,303 parámetros\n",
    "      - FLOPs relativamente bajos (pocas operaciones secuenciales)\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(20, 100) * 0.01\n",
    "    b1 = np.zeros(100)\n",
    "    W2 = np.random.randn(100, 3) * 0.01\n",
    "    b2 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)  # ReLU\n",
    "    logits = h1 @ W2 + b2\n",
    "    return softmax(logits)\n",
    "\n",
    "def deep_narrow_net(X):\n",
    "    \"\"\"\n",
    "    Red profunda y estrecha: 20 -> 30 -> 30 -> 30 -> 3\n",
    "    Características:\n",
    "      - Muchas capas (4)\n",
    "      - Pocas neuronas por capa (30 hidden)\n",
    "      - ~2,523 parámetros\n",
    "      - FLOPs altos (muchas operaciones secuenciales)\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(20, 30) * 0.01\n",
    "    b1 = np.zeros(30)\n",
    "    W2 = np.random.randn(30, 30) * 0.01\n",
    "    b2 = np.zeros(30)\n",
    "    W3 = np.random.randn(30, 30) * 0.01\n",
    "    b3 = np.zeros(30)\n",
    "    W4 = np.random.randn(30, 3) * 0.01\n",
    "    b4 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    h3 = np.maximum(0, h2 @ W3 + b3)\n",
    "    logits = h3 @ W4 + b4\n",
    "    return softmax(logits)\n",
    "\n",
    "def balanced_net(X):\n",
    "    \"\"\"\n",
    "    Red balanceada: 20 -> 50 -> 25 -> 3\n",
    "    Características:\n",
    "      - Capas intermedias (3)\n",
    "      - Neuronas intermedias (50, 25)\n",
    "      - ~2,353 parámetros\n",
    "      - FLOPs medios (balance profundidad/ancho)\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(20, 50) * 0.01\n",
    "    b1 = np.zeros(50)\n",
    "    W2 = np.random.randn(50, 25) * 0.01\n",
    "    b2 = np.zeros(25)\n",
    "    W3 = np.random.randn(25, 3) * 0.01\n",
    "    b3 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    logits = h2 @ W3 + b3\n",
    "    return softmax(logits)\n",
    "\n",
    "# Calcular número exacto de parámetros\n",
    "n_params_wide = (20*100 + 100) + (100*3 + 3)  # 2,303\n",
    "n_params_deep = (20*30 + 30) + (30*30 + 30) + (30*30 + 30) + (30*3 + 3)  # 2,523\n",
    "n_params_balanced = (20*50 + 50) + (50*25 + 25) + (25*3 + 3)  # 2,353"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e86aa",
   "metadata": {},
   "source": [
    "Evaluar las tres arquitecturas y compararlas con todos los criterios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb92f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando: Wide-Shallow...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Wide-Shallow\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,403\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.48\n",
      "  R²:            0.3350\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5245.48         2k                  \n",
      "  BIC             13171.33        k*log(n)            \n",
      "  FIC             1651.62         α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.48\n",
      "  Penalización FLOPs:        10.64\n",
      "  Penalización parámetros:   1201.50\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "Evaluando: Deep-Narrow...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Deep-Narrow\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,583\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.44\n",
      "  R²:            0.3900\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5605.44         2k                  \n",
      "  BIC             14125.00        k*log(n)            \n",
      "  FIC             1741.58         α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.44\n",
      "  Penalización FLOPs:        10.64\n",
      "  Penalización parámetros:   1291.50\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "Evaluando: Balanced...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Balanced\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,403\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.45\n",
      "  R²:            0.2850\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5245.45         2k                  \n",
      "  BIC             13171.31        k*log(n)            \n",
      "  FIC             1651.59         α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.45\n",
      "  Penalización FLOPs:        10.64\n",
      "  Penalización parámetros:   1201.50\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC\n",
      "--------------------------------------------------------\n",
      "Wide-Shallow             5245.48    13171.33     1651.62\n",
      "Deep-Narrow              5605.44    14125.00     1741.58\n",
      "Balanced                 5245.45    13171.31     1651.59\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             Balanced             5245.45        \n",
      "BIC             Balanced             13171.31       \n",
      "FIC             Balanced             1651.59        \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "Balanced             1651.59         0.00            Equivalente al mejor           * MEJOR\n",
      "Wide-Shallow         1651.62         0.03            Equivalente al mejor           \n",
      "Deep-Narrow          1741.58         90.00           Evidencia fuerte contra        \n"
     ]
    }
   ],
   "source": [
    "results_nn = {}\n",
    "\n",
    "for name, model, n_params in [\n",
    "    (\"Wide-Shallow\", wide_shallow_net, n_params_wide),\n",
    "    (\"Deep-Narrow\", deep_narrow_net, n_params_deep),\n",
    "    (\"Balanced\", balanced_net, n_params_balanced)\n",
    "]:\n",
    "    print(f\"\\nEvaluando: {name}...\")\n",
    "    \n",
    "    result = fic_calc.evaluate_model(\n",
    "        model=model,\n",
    "        X=X_train,\n",
    "        y_true=y_train,\n",
    "        task='classification',\n",
    "        n_params=n_params,\n",
    "        framework='numpy'\n",
    "    )\n",
    "    \n",
    "    # Agregar criterios tradicionales\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], n_params)\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], n_params, n_samples)\n",
    "    result['hqic'] = calculate_hqic(result['log_likelihood_term'], n_params, n_samples)\n",
    "    result['mdl'] = calculate_mdl(result['log_likelihood_term'], n_params, n_samples)\n",
    "    \n",
    "    results_nn[name] = result\n",
    "    \n",
    "    print_criteria_comparison(name, result, show_all=False)\n",
    "\n",
    "# Comparación completa\n",
    "compare_all_criteria(results_nn, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83022cc9",
   "metadata": {},
   "source": [
    "### __OBSERVACIONES CLAVE 2__\n",
    "> **AIC/BIC:** Prefieren el modelo con menos parámetros  \n",
    "> **FIC:** Captura el *trade-off* entre parámetros **y** FLOPs  \n",
    "> **Conclusión:** *Deep-Narrow* puede tener FLOPs similares a *Wide-Shallow*, pero más parámetros. **FIC** detecta estas diferencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e3d66",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 3: CNNs en PyTorch}}$\n",
    "\n",
    "##### Demostración: Redes convolucionales simples vs profundas\n",
    "##### Objetivo: Mostrar FIC en modelos de PyTorch reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e31507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1530e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN simple: 2 capas convolucionales + clasificador\n",
    "    Arquitectura: Conv(3->16) -> Pool -> Conv(16->32) -> Pool -> FC(2048->10)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN profunda: 4 capas convolucionales + clasificador\n",
    "    Arquitectura: Conv(3->16) -> Pool -> Conv(16->32) -> Pool -> \n",
    "                  Conv(32->64) -> Pool -> Conv(64->64) -> Pool -> FC(256->10)\n",
    "    Más profunda = Más FLOPs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.fc = nn.Linear(64 * 2 * 2, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d0754",
   "metadata": {},
   "source": [
    "Evaluar ambas CNNs con FIC y mostrar diferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ea6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos dummy para evaluación\n",
    "X_torch = torch.randn(16, 3, 32, 32)  # 16 imágenes 32x32 RGB\n",
    "y_torch = torch.randint(0, 10, (16,))  # 10 clases\n",
    "\n",
    "simple_cnn = SimpleCNN()\n",
    "deep_cnn = DeepCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f03a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando: SimpleCNN...\n",
      "\n",
      "================================================================================\n",
      "MODELO: SimpleCNN\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       25,578\n",
      "  FLOPs:            186,122,240\n",
      "  Muestras:         16\n",
      "  Log-Likelihood:   73.73\n",
      "  R²:            0.1250\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             51229.73        2k                  \n",
      "  BIC             70991.00        k*log(n)            \n",
      "  FIC             13542.72        α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       73.73\n",
      "  Penalización FLOPs:        679.99\n",
      "  Penalización parámetros:   12789.00\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "Evaluando: DeepCNN...\n",
      "\n",
      "================================================================================\n",
      "MODELO: DeepCNN\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       63,082\n",
      "  FLOPs:            110,624,768\n",
      "  Muestras:         16\n",
      "  Log-Likelihood:   75.89\n",
      "  R²:            0.0625\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             126239.89       2k                  \n",
      "  BIC             174976.33       k*log(n)            \n",
      "  FIC             32031.86        α*f(FLOPs) + β*k     <- Incluye FLOPs\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       75.89\n",
      "  Penalización FLOPs:        414.97\n",
      "  Penalización parámetros:   31541.00\n",
      "  Coeficientes: α=5.00, β=0.50, λ=0.3\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC\n",
      "--------------------------------------------------------\n",
      "SimpleCNN               51229.73    70991.00    13542.72\n",
      "DeepCNN                126239.89   174976.33    32031.86\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             SimpleCNN            51229.73       \n",
      "BIC             SimpleCNN            70991.00       \n",
      "FIC             SimpleCNN            13542.72       \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "SimpleCNN            13542.72        0.00            Equivalente al mejor           * MEJOR\n",
      "DeepCNN              32031.86        18489.14        Evidencia fuerte contra        \n"
     ]
    }
   ],
   "source": [
    "results_torch = {}\n",
    "\n",
    "for name, model in [(\"SimpleCNN\", simple_cnn), (\"DeepCNN\", deep_cnn)]:\n",
    "    print(f\"\\nEvaluando: {name}...\")\n",
    "    \n",
    "    result = fic_calc.evaluate_model(\n",
    "        model=model,\n",
    "        X=X_torch,\n",
    "        y_true=y_torch.numpy(),\n",
    "        task='classification',\n",
    "        framework='torch'\n",
    "    )\n",
    "    \n",
    "    # Agregar criterios tradicionales\n",
    "    n_params = result['n_params']\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], n_params)\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], n_params, 16)\n",
    "    \n",
    "    results_torch[name] = result\n",
    "    print_criteria_comparison(name, result, show_all=False)\n",
    "\n",
    "compare_all_criteria(results_torch, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48d0ec",
   "metadata": {},
   "source": [
    "### __OBSERVACIONES CLAVE 3__\n",
    "> **FIC** refleja la diferencia en profundidad de la red  \n",
    "> **DeepCNN** tiene significativamente más FLOPs que **SimpleCNN**  \n",
    "> **Conclusión:** Para imágenes pequeñas, **SimpleCNN** puede ser suficiente.  \n",
    "> **FIC** ayuda a tomar esta decisión cuantitativamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634ee9",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{3. Conclusión}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5aa82",
   "metadata": {},
   "source": [
    "### Ventajas del FIC (FLOPs Information Criterion)\n",
    "\n",
    "**1. Captura complejidad computacional real**\n",
    "- AIC/BIC solo consideran número de parámetros\n",
    "- FIC considera FLOPs (costo de ejecución real)\n",
    "- Detecta operaciones innecesarias que AIC/BIC ignoran\n",
    "\n",
    "**2. Distingue modelos con igual número de parámetros**\n",
    "- Dos redes con 1000 parámetros pueden tener FLOPs muy diferentes\n",
    "- FIC penaliza apropiadamente el modelo más costoso\n",
    "- Útil para identificar arquitecturas ineficientes\n",
    "\n",
    "**3. Refleja la arquitectura del modelo**\n",
    "- Profundidad: más capas = más FLOPs secuenciales\n",
    "- Ancho: más neuronas = más FLOPs por capa\n",
    "- Operaciones especiales (convoluciones, attention) se capturan\n",
    "\n",
    "**4. Útil para deployment en producción**\n",
    "- Selecciona modelos eficientes para recursos limitados\n",
    "- Balance cuantitativo entre precisión y costo computacional\n",
    "- Crítico para móviles, edge devices, y alta escala\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5b98f",
   "metadata": {},
   "source": [
    "### Fórmula del FIC\n",
    "\n",
    "$$\\text{FIC} = -2\\log(L) + \\alpha \\left[\\lambda \\log(\\text{FLOPs}) + (1-\\lambda)\\frac{\\text{FLOPs}}{10^6}\\right] + \\beta k$$\n",
    "\n",
    "Donde:\n",
    "- $L$ = verosimilitud del modelo (ajuste a los datos)\n",
    "- $\\alpha$ = peso de penalización de FLOPs (recomendado: 5.0)\n",
    "- $\\lambda$ = balance log/lineal (recomendado: 0.3)\n",
    "- $\\beta$ = peso de penalización de parámetros (recomendado: 0.5)\n",
    "- $k$ = número de parámetros del modelo\n",
    "\n",
    "---\n",
    "\n",
    "### Cuándo usar cada criterio\n",
    "\n",
    "| Criterio | Cuándo usar | Ventaja | Limitación |\n",
    "|----------|-------------|---------|------------|\n",
    "| **AIC** | Muestras pequeñas, selección clásica | Simple, bien establecido | Ignora FLOPs |\n",
    "| **BIC** | Preferencia por modelos simples | Penaliza más que AIC | Ignora FLOPs |\n",
    "| **FIC** | Deployment, recursos limitados, optimización | Considera costo real | Requiere contar FLOPs |\n",
    "\n",
    "---\n",
    "\n",
    "### Configuraciones recomendadas del FIC\n",
    "\n",
    "**Para móviles/edge devices** (FLOPs críticos):\n",
    "- $\\alpha = 7.0$ - 10.0 (penaliza fuerte)\n",
    "- $\\beta = 0.3$ (menos peso a parámetros)\n",
    "- $\\lambda = 0.2$ (80% lineal, muy sensible a FLOPs)\n",
    "\n",
    "**Para producción balanceada** (default):\n",
    "- $\\alpha = 5.0$ (penalización moderada-alta)\n",
    "- $\\beta = 0.5$ (balance entre FLOPs y params)\n",
    "- $\\lambda = 0.3$ (30% log, 70% lineal)\n",
    "\n",
    "**Para servidores con recursos** (menos restrictivo):\n",
    "- $\\alpha = 2.0$ (penalización suave)\n",
    "- $\\beta = 1.0$ (peso igual FLOPs y params)\n",
    "- $\\lambda = 0.5$ (balance 50/50)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b230186",
   "metadata": {},
   "source": [
    "### Interpretación de diferencias ΔFIC\n",
    "\n",
    "Al comparar modelos, la diferencia en FIC se interpreta como:\n",
    "\n",
    "- **ΔFIC < 2**: Modelos prácticamente equivalentes\n",
    "  - No hay evidencia fuerte para preferir uno sobre otro\n",
    "  - Usar otros criterios (latencia real, memoria)\n",
    "\n",
    "- **2 ≤ ΔFIC < 10**: Evidencia considerable contra el modelo con mayor FIC\n",
    "  - Preferir el modelo con menor FIC si no hay razones específicas\n",
    "  - Diferencia notable pero no definitiva\n",
    "\n",
    "- **ΔFIC ≥ 10**: Evidencia fuerte contra el modelo con mayor FIC\n",
    "  - Claramente preferir el modelo con menor FIC\n",
    "  - Diferencia sustancial en eficiencia o ajuste\n",
    "\n",
    "---\n",
    "\n",
    "### Casos de uso reales\n",
    "\n",
    "**1. Selección de arquitectura de red neuronal**\n",
    "```python\n",
    "# Comparar MLP vs CNN para un problema\n",
    "# Si ΔFIC(CNN - MLP) > 10 pero accuracy solo mejora 1%\n",
    "# → MLP es mejor opción para deployment\n",
    "```\n",
    "\n",
    "**2. Detección de código ineficiente**\n",
    "```python\n",
    "# Dos implementaciones con igual accuracy\n",
    "# FIC detecta operaciones redundantes que AIC/BIC no ven\n",
    "# → Optimizar implementación con mayor FLOPs\n",
    "```\n",
    "\n",
    "**3. Trade-off precisión vs eficiencia**\n",
    "```python\n",
    "# Modelo A: 98% acc, 100M FLOPs\n",
    "# Modelo B: 99% acc, 1000M FLOPs\n",
    "# FIC cuantifica si 1% extra vale 10x más FLOPs\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f7b6f",
   "metadata": {},
   "source": [
    "### __Limitaciones del FIC__\n",
    "> **Requiere contar FLOPs:** No siempre trivial en todos los frameworks  \n",
    "> **No captura todo:** Memoria, ancho de banda, latencia real pueden diferir  \n",
    "> **FLOPs ≠ tiempo real:** Optimizaciones de hardware pueden cambiar el orden  \n",
    "> **Sensible a configuración:** Valores de α, β, λ afectan resultados  \n",
    "\n",
    "\n",
    "**Recomendación:** __Usar FIC junto con AIC/BIC para decisión informada completa__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
