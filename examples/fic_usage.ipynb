{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e01a3",
   "metadata": {},
   "source": [
    "### $\\color{#ddd}{\\text{Ejemplos de uso del FIC (FLOPs Information Criterion)}}$\n",
    "### $\\color{#ddd}{\\text{Comparación con criterios tradicionales: AIC, BIC, y otros}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08527b",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{0. Libs import}}$\n",
    "\n",
    "Para la fecha en la que hago este jupyter no he hecho un paquete instalable universalmente así que se debe especificar el contexto de dónde está la librería de flop_counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0dd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r'C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Fix OpenMP\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import flop_counter\n",
    "from flop_counter.flop_information_criterion import FlopInformationCriterion\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc59e34",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{1. Declaración de funciones para criterios}}$\n",
    "\n",
    "Para comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033e9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterios tradicionales\n",
    "def calculate_aic(log_likelihood: float, k: int) -> float:\n",
    "    \"\"\"\n",
    "    AIC: Akaike Information Criterion\n",
    "    AIC = -2*log(L) + 2*k\n",
    "    \"\"\"\n",
    "    return log_likelihood + 2 * k\n",
    "\n",
    "\n",
    "def calculate_bic(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    BIC: Bayesian Information Criterion\n",
    "    BIC = -2*log(L) + k*log(n)\n",
    "    \"\"\"\n",
    "    return log_likelihood + k * np.log(n)\n",
    "\n",
    "\n",
    "def calculate_hqic(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    HQIC: Hannan-Quinn Information Criterion\n",
    "    HQIC = -2*log(L) + 2*k*log(log(n))\n",
    "    \"\"\"\n",
    "    return log_likelihood + 2 * k * np.log(np.log(n))\n",
    "\n",
    "\n",
    "def calculate_mdl(log_likelihood: float, k: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    MDL: Minimum Description Length\n",
    "    MDL = -log(L) + (k/2)*log(n)\n",
    "    Nota: Similar a BIC pero con diferentes constantes\n",
    "    \"\"\"\n",
    "    return log_likelihood / 2 + (k / 2) * np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3607a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaciones\n",
    "def print_criteria_comparison(name: str, results: dict, show_all: bool = False):\n",
    "    \"\"\"Imprime comparación de criterios de información.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODELO: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Información básica\n",
    "    print(f\"\\nInformación del Modelo:\")\n",
    "    print(f\"  Parámetros:       {results.get('n_params', 'N/A'):,}\")\n",
    "    print(f\"  FLOPs:            {results.get('flops', 'N/A'):,}\")\n",
    "    print(f\"  Muestras:         {results.get('n_samples', 'N/A')}\")\n",
    "    print(f\"  Log-Likelihood:   {results.get('log_likelihood_term', 'N/A'):.2f}\")\n",
    "    \n",
    "    if 'accuracy' in results:\n",
    "        acc_metric = 'R²' if results.get('accuracy', 0) <= 1 else 'Accuracy'\n",
    "        print(f\"  {acc_metric}:            {results['accuracy']:.4f}\")\n",
    "    \n",
    "    # Criterios de información\n",
    "    print(f\"\\nCriterios de Información:\")\n",
    "    print(f\"  {'Criterio':<15} {'Valor':<15} {'Penalización':<20}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    \n",
    "    # AIC\n",
    "    if 'aic' in results:\n",
    "        print(f\"  {'AIC':<15} {results['aic']:<15.2f} {'2k':<20}\")\n",
    "    \n",
    "    # BIC\n",
    "    if 'bic' in results:\n",
    "        print(f\"  {'BIC':<15} {results['bic']:<15.2f} {'k*log(n)':<20}\")\n",
    "    \n",
    "    # FIC (destacado)\n",
    "    print(f\"  {'FIC (RIC)':<15} {results['fic']:<15.2f} {'2*log(FLOPs) + k':<20} *\")\n",
    "    \n",
    "    # Criterios adicionales (solo si show_all=True)\n",
    "    if show_all:\n",
    "        if 'hqic' in results:\n",
    "            print(f\"  {'HQIC':<15} {results['hqic']:<15.2f} {'2k*log(log(n))':<20}\")\n",
    "        \n",
    "        if 'mdl' in results:\n",
    "            print(f\"  {'MDL':<15} {results['mdl']:<15.2f} {'(k/2)*log(n)':<20}\")\n",
    "    \n",
    "    # Desglose del FIC\n",
    "    print(f\"\\nDesglose del FIC:\")\n",
    "    print(f\"  Ajuste (likelihood):       {results['log_likelihood_term']:.2f}\")\n",
    "    print(f\"  Penalización FLOPs:        {results['flops_penalty']:.2f}\")\n",
    "    print(f\"  Penalización parámetros:   {results['params_penalty']:.2f}\")\n",
    "    print(f\"  Coeficientes: α={results['alpha']:.2f}, β={results['beta']:.2f}\")\n",
    "\n",
    "\n",
    "def compare_all_criteria(models_results: dict, show_all: bool = False):\n",
    "    \"\"\"Compara todos los modelos según diferentes criterios.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARACIÓN DE MODELOS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    model_names = list(models_results.keys())\n",
    "    criteria = ['AIC', 'BIC', 'FIC']\n",
    "    \n",
    "    if show_all:\n",
    "        criteria.extend(['HQIC', 'MDL'])\n",
    "    \n",
    "    # Tabla comparativa\n",
    "    print(f\"\\n{'Modelo':<20}\", end=\"\")\n",
    "    for criterion in criteria:\n",
    "        print(f\"{criterion:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (20 + 12 * len(criteria)))\n",
    "    \n",
    "    for name in model_names:\n",
    "        res = models_results[name]\n",
    "        print(f\"{name:<20}\", end=\"\")\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            key = criterion.lower()\n",
    "            value = res.get(key, float('nan'))\n",
    "            print(f\"{value:>12.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Encontrar mejores modelos según cada criterio\n",
    "    print(f\"\\n{'Criterio':<15} {'Mejor Modelo':<20} {'Valor':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        key = criterion.lower()\n",
    "        best_name = min(model_names, key=lambda x: models_results[x].get(key, float('inf')))\n",
    "        best_value = models_results[best_name][key]\n",
    "        print(f\"{criterion:<15} {best_name:<20} {best_value:<15.2f}\")\n",
    "    \n",
    "    # Calcular diferencias respecto al mejor FIC\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ΔFIC: Diferencias respecto al mejor modelo según FIC\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    fic_values = {name: res['fic'] for name, res in models_results.items()}\n",
    "    best_fic = min(fic_values.values())\n",
    "    \n",
    "    print(f\"\\n{'Modelo':<20} {'FIC':<15} {'ΔFIC':<15} {'Interpretación':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name in sorted(model_names, key=lambda x: fic_values[x]):\n",
    "        fic = fic_values[name]\n",
    "        delta_fic = fic - best_fic\n",
    "        \n",
    "        # Interpretación según diferencia\n",
    "        if delta_fic < 2:\n",
    "            interpretation = \"Equivalente al mejor\"\n",
    "        elif delta_fic < 10:\n",
    "            interpretation = \"Evidencia sustancial contra\"\n",
    "        else:\n",
    "            interpretation = \"Evidencia fuerte contra\"\n",
    "        \n",
    "        marker = \"* MEJOR\" if delta_fic < 0.01 else \"\"\n",
    "        \n",
    "        print(f\"{name:<20} {fic:<15.2f} {delta_fic:<15.2f} {interpretation:<30} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14722f6d",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{2. Ejemplos}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bb64e",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 1: Regresión Lineal Simple}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290e037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "n_samples = 100\n",
    "X_train = np.random.randn(n_samples, 1)\n",
    "true_slope = 2.5\n",
    "true_intercept = 1.0\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y_train = true_slope * X_train.squeeze() + true_intercept + noise\n",
    "\n",
    "# Definir modelos de diferentes complejidades\n",
    "def linear_model_simple(X):\n",
    "    \"\"\"Modelo lineal simple: y = ax + b\"\"\"\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    return X_extended @ W\n",
    "\n",
    "def linear_model_complex(X):\n",
    "    \"\"\"Modelo lineal con más operaciones innecesarias\"\"\"\n",
    "    # Mismo resultado pero con más FLOPs\n",
    "    W = np.array([[true_slope], [true_intercept]])\n",
    "    X_extended = np.column_stack([X, np.ones(len(X))])\n",
    "    \n",
    "    # Operaciones adicionales innecesarias (más FLOPs)\n",
    "    temp = X_extended @ W\n",
    "    temp = np.exp(np.log(temp))  # identidad costosa\n",
    "    temp = temp @ np.eye(1)      # multiplicación innecesaria\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6602dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hecto\\OneDrive\\Escritorio\\Personal\\iroFactory\\31.FLOPs-Information-Criterion\\flop_counter\\flop_information_criterion.py:234: UserWarning: FLOPs = 0, penalización por FLOPs es 0\n",
      "  warnings.warn(\"FLOPs = 0, penalización por FLOPs es 0\", UserWarning)\n",
      "C:\\Users\\hecto\\AppData\\Local\\Temp\\ipykernel_18284\\482758071.py:24: RuntimeWarning: invalid value encountered in log\n",
      "  temp = np.exp(np.log(temp))  # identidad costosa\n"
     ]
    }
   ],
   "source": [
    "# Evaluar modelos\n",
    "fic_calc = FlopInformationCriterion(variant='hybrid')\n",
    "\n",
    "result_simple = fic_calc.evaluate_model(\n",
    "    model=linear_model_simple,\n",
    "    X=X_train,\n",
    "    y_true=y_train,\n",
    "    task='regression',\n",
    "    n_params=2,  # pendiente + intercept\n",
    "    framework='numpy'\n",
    ")\n",
    "\n",
    "result_complex = fic_calc.evaluate_model(\n",
    "    model=linear_model_complex,\n",
    "    X=X_train,\n",
    "    y_true=y_train,\n",
    "    task='regression',\n",
    "    n_params=2,  # mismo número de parámetros\n",
    "    framework='numpy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d877dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO: Modelo Simple\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2\n",
      "  FLOPs:            0\n",
      "  Muestras:         100\n",
      "  Log-Likelihood:   975.95\n",
      "  R²:            0.0000\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             979.95          2k                  \n",
      "  BIC             985.16          k*log(n)            \n",
      "  FIC (RIC)       977.95          2*log(FLOPs) + k     *\n",
      "  HQIC            982.06          2k*log(log(n))      \n",
      "  MDL             492.58          (k/2)*log(n)        \n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       975.95\n",
      "  Penalización FLOPs:        0.00\n",
      "  Penalización parámetros:   2.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "\n",
      "================================================================================\n",
      "MODELO: Modelo Complejo\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2\n",
      "  FLOPs:            0\n",
      "  Muestras:         100\n",
      "  Log-Likelihood:   nan\n",
      "  R²:            0.0000\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             nan             2k                  \n",
      "  BIC             nan             k*log(n)            \n",
      "  FIC (RIC)       nan             2*log(FLOPs) + k     *\n",
      "  HQIC            nan             2k*log(log(n))      \n",
      "  MDL             nan             (k/2)*log(n)        \n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       nan\n",
      "  Penalización FLOPs:        0.00\n",
      "  Penalización parámetros:   2.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC        HQIC         MDL\n",
      "--------------------------------------------------------------------------------\n",
      "Simple                    979.95      985.16      977.95      982.06      492.58\n",
      "Complejo                     nan         nan         nan         nan         nan\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             Simple               979.95         \n",
      "BIC             Simple               985.16         \n",
      "FIC             Simple               977.95         \n",
      "HQIC            Simple               982.06         \n",
      "MDL             Simple               492.58         \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "Simple               977.95          0.00            Equivalente al mejor           * MEJOR\n",
      "Complejo             nan             nan             Evidencia fuerte contra        \n",
      "\n",
      " Observación:\n",
      "   AIC y BIC son idénticos (mismo # de parámetros)\n",
      "   FIC correctamente penaliza el modelo complejo (más FLOPs)\n"
     ]
    }
   ],
   "source": [
    "# Calcular criterios tradicionales para ambos modelos\n",
    "for name, result in [(\"Simple\", result_simple), (\"Complejo\", result_complex)]:\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], result['n_params'])\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "    result['hqic'] = calculate_hqic(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "    result['mdl'] = calculate_mdl(result['log_likelihood_term'], result['n_params'], result['n_samples'])\n",
    "\n",
    "# Mostrar resultados\n",
    "print_criteria_comparison(\"Modelo Simple\", result_simple, show_all=True)\n",
    "print_criteria_comparison(\"Modelo Complejo\", result_complex, show_all=True)\n",
    "\n",
    "# Comparación\n",
    "models_linear = {\n",
    "    'Simple': result_simple,\n",
    "    'Complejo': result_complex\n",
    "}\n",
    "compare_all_criteria(models_linear, show_all=True)\n",
    "\n",
    "print(\"\\n Observación:\")\n",
    "print(\"   AIC y BIC son idénticos (mismo # de parámetros)\")\n",
    "print(\"   FIC correctamente penaliza el modelo complejo (más FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef4c36",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 2: Redes Neuronales de Diferentes Arquitecturas}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22415873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos de clasificación\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "\n",
    "X_train = np.random.randn(n_samples, n_features)\n",
    "y_train = np.random.randint(0, n_classes, n_samples)\n",
    "y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax estable numéricamente\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb455d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: Red poco profunda y ancha\n",
    "def wide_shallow_net(X):\n",
    "    \"\"\"Red ancha y poco profunda: 20 -> 100 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 100) * 0.01\n",
    "    b1 = np.zeros(100)\n",
    "    W2 = np.random.randn(100, 3) * 0.01\n",
    "    b2 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)  # ReLU\n",
    "    logits = h1 @ W2 + b2\n",
    "    return softmax(logits)\n",
    "\n",
    "# Modelo 2: Red profunda y estrecha\n",
    "def deep_narrow_net(X):\n",
    "    \"\"\"Red profunda y estrecha: 20 -> 30 -> 30 -> 30 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 30) * 0.01\n",
    "    b1 = np.zeros(30)\n",
    "    W2 = np.random.randn(30, 30) * 0.01\n",
    "    b2 = np.zeros(30)\n",
    "    W3 = np.random.randn(30, 30) * 0.01\n",
    "    b3 = np.zeros(30)\n",
    "    W4 = np.random.randn(30, 3) * 0.01\n",
    "    b4 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    h3 = np.maximum(0, h2 @ W3 + b3)\n",
    "    logits = h3 @ W4 + b4\n",
    "    return softmax(logits)\n",
    "\n",
    "# Modelo 3: Red balanceada\n",
    "def balanced_net(X):\n",
    "    \"\"\"Red balanceada: 20 -> 50 -> 25 -> 3\"\"\"\n",
    "    W1 = np.random.randn(20, 50) * 0.01\n",
    "    b1 = np.zeros(50)\n",
    "    W2 = np.random.randn(50, 25) * 0.01\n",
    "    b2 = np.zeros(25)\n",
    "    W3 = np.random.randn(25, 3) * 0.01\n",
    "    b3 = np.zeros(3)\n",
    "    \n",
    "    h1 = np.maximum(0, X @ W1 + b1)\n",
    "    h2 = np.maximum(0, h1 @ W2 + b2)\n",
    "    logits = h2 @ W3 + b3\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb92f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluando: Wide-Shallow...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Wide-Shallow\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,403\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.48\n",
      "  R²:            0.3350\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5245.48         2k                  \n",
      "  BIC             13171.33        k*log(n)            \n",
      "  FIC (RIC)       2856.66         2*log(FLOPs) + k     *\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.48\n",
      "  Penalización FLOPs:        14.18\n",
      "  Penalización parámetros:   2403.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "  Evaluando: Deep-Narrow...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Deep-Narrow\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,583\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.44\n",
      "  R²:            0.3900\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5605.44         2k                  \n",
      "  BIC             14125.00        k*log(n)            \n",
      "  FIC (RIC)       3036.63         2*log(FLOPs) + k     *\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.44\n",
      "  Penalización FLOPs:        14.18\n",
      "  Penalización parámetros:   2583.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "  Evaluando: Balanced...\n",
      "\n",
      "================================================================================\n",
      "MODELO: Balanced\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       2,403\n",
      "  FLOPs:            1,200\n",
      "  Muestras:         200\n",
      "  Log-Likelihood:   439.45\n",
      "  R²:            0.2850\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             5245.45         2k                  \n",
      "  BIC             13171.31        k*log(n)            \n",
      "  FIC (RIC)       2856.63         2*log(FLOPs) + k     *\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       439.45\n",
      "  Penalización FLOPs:        14.18\n",
      "  Penalización parámetros:   2403.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC\n",
      "--------------------------------------------------------\n",
      "Wide-Shallow             5245.48    13171.33     2856.66\n",
      "Deep-Narrow              5605.44    14125.00     3036.63\n",
      "Balanced                 5245.45    13171.31     2856.63\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             Balanced             5245.45        \n",
      "BIC             Balanced             13171.31       \n",
      "FIC             Balanced             2856.63        \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "Balanced             2856.63         0.00            Equivalente al mejor           * MEJOR\n",
      "Wide-Shallow         2856.66         0.03            Equivalente al mejor           \n",
      "Deep-Narrow          3036.63         180.00          Evidencia fuerte contra        \n",
      "\n",
      " Observación:\n",
      "   FIC captura el trade-off entre parámetros y FLOPs\n",
      "   Modelos con más parámetros no siempre son más costosos computacionalmente\n"
     ]
    }
   ],
   "source": [
    "# Calcular parámetros para cada modelo\n",
    "n_params_wide = (20*100 + 100) + (100*3 + 3)  # 2303\n",
    "n_params_deep = (20*30 + 30) + (30*30 + 30) + (30*30 + 30) + (30*3 + 3)  # 2523\n",
    "n_params_balanced = (20*50 + 50) + (50*25 + 25) + (25*3 + 3)  # 2353\n",
    "\n",
    "results_nn = {}\n",
    "\n",
    "for name, model, n_params in [\n",
    "    (\"Wide-Shallow\", wide_shallow_net, n_params_wide),\n",
    "    (\"Deep-Narrow\", deep_narrow_net, n_params_deep),\n",
    "    (\"Balanced\", balanced_net, n_params_balanced)\n",
    "]:\n",
    "    print(f\"  Evaluando: {name}...\")\n",
    "    \n",
    "    result = fic_calc.evaluate_model(\n",
    "        model=model,\n",
    "        X=X_train,\n",
    "        y_true=y_train,\n",
    "        task='classification',\n",
    "        n_params=n_params,\n",
    "        framework='numpy'\n",
    "    )\n",
    "    \n",
    "    # Agregar criterios tradicionales\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], n_params)\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], n_params, n_samples)\n",
    "    result['hqic'] = calculate_hqic(result['log_likelihood_term'], n_params, n_samples)\n",
    "    result['mdl'] = calculate_mdl(result['log_likelihood_term'], n_params, n_samples)\n",
    "    \n",
    "    results_nn[name] = result\n",
    "    \n",
    "    print_criteria_comparison(name, result, show_all=False)\n",
    "\n",
    "# Comparación completa\n",
    "compare_all_criteria(results_nn, show_all=False)\n",
    "\n",
    "print(\"\\n Observación:\")\n",
    "print(\"   FIC captura el trade-off entre parámetros y FLOPs\")\n",
    "print(\"   Modelos con más parámetros no siempre son más costosos computacionalmente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e3d66",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ejemplo 3: Comparación con PyTorch}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e31507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1530e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.fc = nn.Linear(64 * 2 * 2, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ea6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos dummy\n",
    "X_torch = torch.randn(16, 3, 32, 32)\n",
    "y_torch = torch.randint(0, 10, (16,))\n",
    "\n",
    "# Evaluar modelos\n",
    "simple_cnn = SimpleCNN()\n",
    "deep_cnn = DeepCNN()\n",
    "\n",
    "results_torch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f03a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando: SimpleCNN...\n",
      "\n",
      "================================================================================\n",
      "MODELO: SimpleCNN\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       25,578\n",
      "  FLOPs:            186,122,240\n",
      "  Muestras:         16\n",
      "  Log-Likelihood:   69.79\n",
      "  R²:            0.2500\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             51225.79        2k                  \n",
      "  BIC             70987.07        k*log(n)            \n",
      "  FIC (RIC)       26058.12        2*log(FLOPs) + k     *\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       69.79\n",
      "  Penalización FLOPs:        410.33\n",
      "  Penalización parámetros:   25578.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "\n",
      "Evaluando: DeepCNN...\n",
      "\n",
      "================================================================================\n",
      "MODELO: DeepCNN\n",
      "================================================================================\n",
      "\n",
      "Información del Modelo:\n",
      "  Parámetros:       63,082\n",
      "  FLOPs:            110,624,768\n",
      "  Muestras:         16\n",
      "  Log-Likelihood:   73.66\n",
      "  R²:            0.1875\n",
      "\n",
      "Criterios de Información:\n",
      "  Criterio        Valor           Penalización        \n",
      "  --------------------------------------------------\n",
      "  AIC             126237.66       2k                  \n",
      "  BIC             174974.10       k*log(n)            \n",
      "  FIC (RIC)       63413.95        2*log(FLOPs) + k     *\n",
      "\n",
      "Desglose del FIC:\n",
      "  Ajuste (likelihood):       73.66\n",
      "  Penalización FLOPs:        258.29\n",
      "  Penalización parámetros:   63082.00\n",
      "  Coeficientes: α=2.00, β=1.00\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Modelo                       AIC         BIC         FIC\n",
      "--------------------------------------------------------\n",
      "SimpleCNN               51225.79    70987.07    26058.12\n",
      "DeepCNN                126237.66   174974.10    63413.95\n",
      "\n",
      "Criterio        Mejor Modelo         Valor          \n",
      "--------------------------------------------------\n",
      "AIC             SimpleCNN            51225.79       \n",
      "BIC             SimpleCNN            70987.07       \n",
      "FIC             SimpleCNN            26058.12       \n",
      "\n",
      "================================================================================\n",
      "ΔFIC: Diferencias respecto al mejor modelo según FIC\n",
      "================================================================================\n",
      "\n",
      "Modelo               FIC             ΔFIC            Interpretación                \n",
      "--------------------------------------------------------------------------------\n",
      "SimpleCNN            26058.12        0.00            Equivalente al mejor           * MEJOR\n",
      "DeepCNN              63413.95        37355.83        Evidencia fuerte contra        \n",
      "\n",
      " Observación:\n",
      "   FIC refleja la diferencia en profundidad de la red\n",
      "   DeepCNN tiene más FLOPs a pesar de entrada pequeña\n"
     ]
    }
   ],
   "source": [
    "for name, model in [(\"SimpleCNN\", simple_cnn), (\"DeepCNN\", deep_cnn)]:\n",
    "    print(f\"\\nEvaluando: {name}...\")\n",
    "    \n",
    "    result = fic_calc.evaluate_model(\n",
    "        model=model,\n",
    "        X=X_torch,\n",
    "        y_true=y_torch.numpy(),\n",
    "        task='classification',\n",
    "        framework='torch'\n",
    "    )\n",
    "    \n",
    "    # Agregar criterios tradicionales\n",
    "    n_params = result['n_params']\n",
    "    result['aic'] = calculate_aic(result['log_likelihood_term'], n_params)\n",
    "    result['bic'] = calculate_bic(result['log_likelihood_term'], n_params, 16)\n",
    "    \n",
    "    results_torch[name] = result\n",
    "    print_criteria_comparison(name, result, show_all=False)\n",
    "\n",
    "compare_all_criteria(results_torch, show_all=False)\n",
    "\n",
    "print(\"\\n Observación:\")\n",
    "print(\"   FIC refleja la diferencia en profundidad de la red\")\n",
    "print(\"   DeepCNN tiene más FLOPs a pesar de entrada pequeña\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634ee9",
   "metadata": {},
   "source": [
    "# $\\color{#dda}{\\text{3. Conclusión}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5b98f",
   "metadata": {},
   "source": [
    "### $\\color{#dda}{\\text{Ventajas del FIC:}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{1. Captura complejidad computacional real}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - AIC/BIC solo consideran número de parámetros}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - FIC considera FLOPs (costo de ejecución)}}$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\color{#ddd}{\\text{2. Distingue modelos con igual número de parámetros}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Dos redes con 1000 parámetros pueden tener FLOPs muy diferentes}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - FIC penaliza apropiadamente el modelo más costoso}}$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\color{#ddd}{\\text{3. Refleja la arquitectura del modelo}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Profundidad, ancho, conexiones skip}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Todo se traduce en FLOPs}}$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\color{#ddd}{\\text{4. Útil para deployment}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Selecciona modelos eficientes para producción}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Balance entre precisión y costo computacional}}$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\color{#ddd}{\\text{Cuándo usar:}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    AIC:  Selección de modelos cuando muestra es pequeña}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    BIC:  Cuando se prefiere modelos más simples (penaliza más)}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    FIC:  Cuando el costo computacional es importante (deployment, móvil, edge)}}$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\color{#ddd}{\\text{RECOMENDACIÓN:}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{Use FIC-Hybrid (α=2, β=1) para balance entre:}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Ajuste a los datos}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Complejidad computacional (FLOPs)}}$\n",
    "\n",
    "$\\color{#ddd}{\\text{    - Complejidad paramétrica (parámetros)}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
